===> ENV Variables ...
ALLOW_UNSIGNED=false
COMPONENT=kafka
CONFLUENT_DEB_VERSION=1
CONFLUENT_MAJOR_VERSION=5
CONFLUENT_METRICS_ENABLE=true
CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS=kafka:9092
CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS=1
CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT=zookeeper:2181
CONFLUENT_MINOR_VERSION=0
CONFLUENT_MVN_LABEL=
CONFLUENT_PATCH_VERSION=0
CONFLUENT_PLATFORM_LABEL=
CONFLUENT_SUPPORT_CUSTOMER_ID=anonymous
CONFLUENT_VERSION=5.0.0
CUB_CLASSPATH=/etc/confluent/docker/docker-utils.jar
HOME=/root
HOSTNAME=cdeeb0e55b86
KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
KAFKA_BROKER_ID=1
KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
KAFKA_METRIC_REPORTERS=io.confluent.metrics.reporter.ConfluentMetricsReporter
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
KAFKA_VERSION=2.0.0
KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
LANG=C.UTF-8
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
PYTHON_PIP_VERSION=8.1.2
PYTHON_VERSION=2.7.9-1
SCALA_VERSION=2.11
SHLVL=1
ZULU_OPENJDK_VERSION=8=8.30.0.1
_=/usr/bin/env
===> User
uid=0(root) gid=0(root) groups=0(root)
===> Configuring ...
===> Running preflight checks ... 
===> Check if /var/lib/kafka/data is writable ...
===> Check if Zookeeper is healthy ...
===> Launching ... 
===> Launching kafka ... 
[2019-04-19 12:46:00,020] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-04-19 12:46:00,781] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.0-IV1
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = zookeeper:2181
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-04-19 12:46:01,019] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2019-04-19 12:46:01,025] INFO starting (kafka.server.KafkaServer)
[2019-04-19 12:46:01,028] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[2019-04-19 12:46:01,067] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-04-19 12:46:01,081] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,081] INFO Client environment:host.name=cdeeb0e55b86 (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,082] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,082] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,082] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,083] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/kafka/avro-1.8.1.jar:/usr/bin/../share/java/kafka/commons-compress-1.8.1.jar:/usr/bin/../share/java/kafka/httpcore-4.4.4.jar:/usr/bin/../share/java/kafka/jersey-common-2.27.jar:/usr/bin/../share/java/kafka/commons-codec-1.9.jar:/usr/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/connect-json-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/kafka-clients-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.27.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.6.jar:/usr/bin/../share/java/kafka/commons-validator-1.4.1.jar:/usr/bin/../share/java/kafka/support-metrics-common-5.0.0.jar:/usr/bin/../share/java/kafka/lz4-java-1.4.1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.11-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/usr/bin/../share/java/kafka/guava-20.0.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/usr/bin/../share/java/kafka/connect-file-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b42.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.9.6.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/usr/bin/../share/java/kafka/scala-logging_2.11-3.9.0.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.27.jar:/usr/bin/../share/java/kafka/maven-artifact-3.5.3.jar:/usr/bin/../share/java/kafka/httpclient-4.5.2.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/zookeeper-3.4.13.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/javax.inject-1.jar:/usr/bin/../share/java/kafka/netty-3.10.6.Final.jar:/usr/bin/../share/java/kafka/zkclient-0.10.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.27.jar:/usr/bin/../share/java/kafka/connect-transforms-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/kafka-streams-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.0-cp1-test-sources.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/jackson-databind-2.9.6.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.jar:/usr/bin/../share/java/kafka/jersey-server-2.27.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.6.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.9.6.jar:/usr/bin/../share/java/kafka/xz-1.5.jar:/usr/bin/../share/java/kafka/javax.inject-2.5.0-b42.jar:/usr/bin/../share/java/kafka/javassist-3.22.0-CR2.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.0-cp1-test.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.27.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0-b42.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/jersey-client-2.27.jar:/usr/bin/../share/java/kafka/commons-lang3-3.5.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jackson-core-2.9.6.jar:/usr/bin/../share/java/kafka/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.0-cp1-sources.jar:/usr/bin/../share/java/kafka/connect-runtime-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.0-cp1-scaladoc.jar:/usr/bin/../share/java/kafka/commons-collections-3.2.1.jar:/usr/bin/../share/java/kafka/commons-lang3-3.1.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/scala-library-2.11.12.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.0-cp1-javadoc.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0-b42.jar:/usr/bin/../share/java/kafka/jline-0.9.94.jar:/usr/bin/../share/java/kafka/reflections-0.9.11.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/connect-api-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/common-utils-5.0.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/usr/bin/../share/java/kafka/support-metrics-client-5.0.0.jar:/usr/bin/../share/java/kafka/paranamer-2.7.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0-b42.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/usr/bin/../share/java/kafka/httpmime-4.5.2.jar:/usr/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/kafka/kafka-tools-2.0.0-cp1.jar:/usr/bin/../share/java/kafka/confluent-metrics-5.0.0.jar:/usr/bin/../share/java/confluent-support-metrics/log4j-1.2.17.jar:/usr/bin/../share/java/confluent-support-metrics/support-metrics-fullcollector-5.0.0.jar:/usr/bin/../share/java/confluent-support-metrics/zookeeper-3.4.13.jar:/usr/bin/../share/java/confluent-support-metrics/netty-3.10.6.Final.jar:/usr/bin/../share/java/confluent-support-metrics/zkclient-0.10.jar:/usr/bin/../share/java/confluent-support-metrics/audience-annotations-0.5.0.jar:/usr/bin/../share/java/confluent-support-metrics/jline-0.9.94.jar:/usr/bin/../share/java/confluent-support-metrics/common-utils-5.0.0.jar:/usr/bin/../share/java/confluent-support-metrics/slf4j-api-1.7.25.jar:/usr/share/java/confluent-support-metrics/log4j-1.2.17.jar:/usr/share/java/confluent-support-metrics/support-metrics-fullcollector-5.0.0.jar:/usr/share/java/confluent-support-metrics/zookeeper-3.4.13.jar:/usr/share/java/confluent-support-metrics/netty-3.10.6.Final.jar:/usr/share/java/confluent-support-metrics/zkclient-0.10.jar:/usr/share/java/confluent-support-metrics/audience-annotations-0.5.0.jar:/usr/share/java/confluent-support-metrics/jline-0.9.94.jar:/usr/share/java/confluent-support-metrics/common-utils-5.0.0.jar:/usr/share/java/confluent-support-metrics/slf4j-api-1.7.25.jar (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,084] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,085] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,085] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,086] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,087] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,087] INFO Client environment:os.version=4.9.125-linuxkit (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,088] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,088] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,089] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,093] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3b69e7d1 (org.apache.zookeeper.ZooKeeper)
[2019-04-19 12:46:01,131] INFO Opening socket connection to server zookeeper/172.24.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-04-19 12:46:01,139] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-04-19 12:46:01,153] INFO Socket connection established to zookeeper/172.24.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-04-19 12:46:01,173] INFO Session establishment complete on server zookeeper/172.24.0.3:2181, sessionid = 0x1000580959f0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-04-19 12:46:01,182] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-04-19 12:46:01,952] INFO Cluster ID = t8fF06H2TrSdRZJb7qBbkA (kafka.server.KafkaServer)
[2019-04-19 12:46:01,975] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-04-19 12:46:02,128] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.0-IV1
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = zookeeper:2181
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-04-19 12:46:02,143] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.0-IV1
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = zookeeper:2181
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-04-19 12:46:02,210] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-04-19 12:46:02,214] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-04-19 12:46:02,215] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-04-19 12:46:02,289] INFO Loading logs. (kafka.log.LogManager)
[2019-04-19 12:46:02,309] INFO Logs loading complete in 19 ms. (kafka.log.LogManager)
[2019-04-19 12:46:02,332] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-04-19 12:46:02,338] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-04-19 12:46:02,344] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2019-04-19 12:46:02,412] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2019-04-19 12:46:02,938] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-04-19 12:46:03,000] INFO Awaiting socket connections on 0.0.0.0:29092. (kafka.network.Acceptor)
[2019-04-19 12:46:03,025] INFO [SocketServer brokerId=1] Started 2 acceptor threads (kafka.network.SocketServer)
[2019-04-19 12:46:03,071] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-04-19 12:46:03,073] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-04-19 12:46:03,095] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-04-19 12:46:03,127] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-04-19 12:46:03,210] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-04-19 12:46:03,221] INFO Result of znode creation at /brokers/ids/1 is: OK (kafka.zk.KafkaZkClient)
[2019-04-19 12:46:03,225] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(kafka,9092,ListenerName(PLAINTEXT),PLAINTEXT), EndPoint(localhost,29092,ListenerName(PLAINTEXT_HOST),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-04-19 12:46:03,229] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-04-19 12:46:03,380] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[2019-04-19 12:46:03,397] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-04-19 12:46:03,406] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-04-19 12:46:03,409] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-04-19 12:46:03,424] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-04-19 12:46:03,427] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[2019-04-19 12:46:03,429] INFO [Controller id=1] 1 successfully elected as the controller (kafka.controller.KafkaController)
[2019-04-19 12:46:03,431] INFO [Controller id=1] Reading controller epoch from ZooKeeper (kafka.controller.KafkaController)
[2019-04-19 12:46:03,442] INFO [Controller id=1] Incrementing controller epoch in ZooKeeper (kafka.controller.KafkaController)
[2019-04-19 12:46:03,464] INFO [Controller id=1] Epoch incremented to 1 (kafka.controller.KafkaController)
[2019-04-19 12:46:03,466] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
[2019-04-19 12:46:03,466] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:46:03,477] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
[2019-04-19 12:46:03,481] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:46:03,483] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
[2019-04-19 12:46:03,492] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
[2019-04-19 12:46:03,496] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 24 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:03,538] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-04-19 12:46:03,575] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-04-19 12:46:03,578] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-04-19 12:46:03,586] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-04-19 12:46:03,609] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
[2019-04-19 12:46:03,626] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
[2019-04-19 12:46:03,643] INFO [Controller id=1] Partitions being reassigned: Map() (kafka.controller.KafkaController)
[2019-04-19 12:46:03,646] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
[2019-04-19 12:46:03,648] INFO [Controller id=1] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)
[2019-04-19 12:46:03,649] INFO [Controller id=1] Current list of topics in the cluster: Set() (kafka.controller.KafkaController)
[2019-04-19 12:46:03,650] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
[2019-04-19 12:46:03,652] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
[2019-04-19 12:46:03,659] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,662] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,663] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
[2019-04-19 12:46:03,665] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
[2019-04-19 12:46:03,698] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ReplicaStateMachine)
[2019-04-19 12:46:03,704] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ReplicaStateMachine)
[2019-04-19 12:46:03,713] INFO [RequestSendThread controllerId=1] Controller 1 connected to kafka:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[2019-04-19 12:46:03,727] INFO [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map() (kafka.controller.ReplicaStateMachine)
[2019-04-19 12:46:03,738] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.PartitionStateMachine)
[2019-04-19 12:46:03,741] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.PartitionStateMachine)
[2019-04-19 12:46:03,745] INFO ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka:9092
	confluent.metrics.reporter.publish.ms = 15000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 1
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|OfflinePartitionsCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TotalFetchRequestsPerSec|TotalProduceRequestsPerSec|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec).*
 (io.confluent.metrics.reporter.ConfluentMetricsReporterConfig)
[2019-04-19 12:46:03,750] INFO [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map() (kafka.controller.PartitionStateMachine)
[2019-04-19 12:46:03,752] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
[2019-04-19 12:46:03,756] INFO [Controller id=1] Removing partitions Set() from the list of reassigned partitions in zookeeper (kafka.controller.KafkaController)
[2019-04-19 12:46:03,758] INFO [Controller id=1] No more partitions need to be reassigned. Deleting zk path /admin/reassign_partitions (kafka.controller.KafkaController)
[2019-04-19 12:46:03,778] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,781] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,786] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,791] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,793] INFO [Controller id=1] Starting preferred replica leader election for partitions  (kafka.controller.KafkaController)
[2019-04-19 12:46:03,802] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
[2019-04-19 12:46:03,842] INFO ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.id = confluent-metrics-reporter
	compression.type = lz4
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2019-04-19 12:46:03,908] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2019-04-19 12:46:03,908] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2019-04-19 12:46:03,914] INFO Kafka version : 2.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:03,914] INFO Kafka commitId : 887f2c180e0a7130 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:03,926] INFO Starting Confluent metrics reporter for cluster id t8fF06H2TrSdRZJb7qBbkA with an interval of 15000 ms (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[2019-04-19 12:46:04,040] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-04-19 12:46:04,105] INFO [SocketServer brokerId=1] Started processors for 2 acceptors (kafka.network.SocketServer)
[2019-04-19 12:46:04,115] INFO Kafka version : 2.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:04,116] INFO Kafka commitId : 887f2c180e0a7130 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:04,127] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[2019-04-19 12:46:04,131] INFO Waiting 10029 ms for the monitored service to finish starting up... (io.confluent.support.metrics.BaseMetricsReporter)
[2019-04-19 12:46:04,173] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 0 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:08,818] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 12:46:08,899] DEBUG [Controller id=1] Preferred replicas by broker Map() (kafka.controller.KafkaController)
[2019-04-19 12:46:09,508] INFO [Controller id=1] New topics: [Set(emails)], deleted topics: [Set()], new partition replica assignment [Map(emails-0 -> Vector(1), emails-3 -> Vector(1), emails-10 -> Vector(1), emails-1 -> Vector(1), emails-7 -> Vector(1), emails-11 -> Vector(1), emails-4 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails-8 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails-5 -> Vector(1), emails-9 -> Vector(1), emails-12 -> Vector(1))] (kafka.controller.KafkaController)
[2019-04-19 12:46:09,512] INFO [Controller id=1] New partition creation callback for emails-0,emails-3,emails-10,emails-1,emails-7,emails-11,emails-4,emails-14,emails-6,emails-8,emails-13,emails-2,emails-5,emails-9,emails-12 (kafka.controller.KafkaController)
[2019-04-19 12:46:09,531] TRACE [Controller id=1 epoch=1] Changed partition emails-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,532] TRACE [Controller id=1 epoch=1] Changed partition emails-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,533] TRACE [Controller id=1 epoch=1] Changed partition emails-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,534] TRACE [Controller id=1 epoch=1] Changed partition emails-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,535] TRACE [Controller id=1 epoch=1] Changed partition emails-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,536] TRACE [Controller id=1 epoch=1] Changed partition emails-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,537] TRACE [Controller id=1 epoch=1] Changed partition emails-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,538] TRACE [Controller id=1 epoch=1] Changed partition emails-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,539] TRACE [Controller id=1 epoch=1] Changed partition emails-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,540] TRACE [Controller id=1 epoch=1] Changed partition emails-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,542] TRACE [Controller id=1 epoch=1] Changed partition emails-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,542] TRACE [Controller id=1 epoch=1] Changed partition emails-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,543] TRACE [Controller id=1 epoch=1] Changed partition emails-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,544] TRACE [Controller id=1 epoch=1] Changed partition emails-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,545] TRACE [Controller id=1 epoch=1] Changed partition emails-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:09,611] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-6 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,616] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-2 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-3 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,620] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-8 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,627] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-7 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,627] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-10 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,627] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-14 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,627] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-12 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,628] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-11 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,628] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-9 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,628] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-13 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,628] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-0 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,628] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-1 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,629] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-5 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:09,629] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-4 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:10,078] TRACE [Controller id=1 epoch=1] Changed partition emails-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,079] TRACE [Controller id=1 epoch=1] Changed partition emails-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,079] TRACE [Controller id=1 epoch=1] Changed partition emails-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,079] TRACE [Controller id=1 epoch=1] Changed partition emails-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,080] TRACE [Controller id=1 epoch=1] Changed partition emails-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,080] TRACE [Controller id=1 epoch=1] Changed partition emails-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,080] TRACE [Controller id=1 epoch=1] Changed partition emails-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,080] TRACE [Controller id=1 epoch=1] Changed partition emails-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,080] TRACE [Controller id=1 epoch=1] Changed partition emails-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,081] TRACE [Controller id=1 epoch=1] Changed partition emails-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,081] TRACE [Controller id=1 epoch=1] Changed partition emails-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,081] TRACE [Controller id=1 epoch=1] Changed partition emails-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,081] TRACE [Controller id=1 epoch=1] Changed partition emails-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,081] TRACE [Controller id=1 epoch=1] Changed partition emails-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,082] TRACE [Controller id=1 epoch=1] Changed partition emails-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:10,085] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-0 (state.change.logger)
[2019-04-19 12:46:10,085] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-11 (state.change.logger)
[2019-04-19 12:46:10,085] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-8 (state.change.logger)
[2019-04-19 12:46:10,085] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-13 (state.change.logger)
[2019-04-19 12:46:10,086] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-2 (state.change.logger)
[2019-04-19 12:46:10,086] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-5 (state.change.logger)
[2019-04-19 12:46:10,087] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-10 (state.change.logger)
[2019-04-19 12:46:10,088] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-7 (state.change.logger)
[2019-04-19 12:46:10,089] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-4 (state.change.logger)
[2019-04-19 12:46:10,089] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-9 (state.change.logger)
[2019-04-19 12:46:10,090] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-12 (state.change.logger)
[2019-04-19 12:46:10,096] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-1 (state.change.logger)
[2019-04-19 12:46:10,097] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-6 (state.change.logger)
[2019-04-19 12:46:10,097] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-3 (state.change.logger)
[2019-04-19 12:46:10,097] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails-14 (state.change.logger)
[2019-04-19 12:46:10,122] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-0 (state.change.logger)
[2019-04-19 12:46:10,127] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-11 (state.change.logger)
[2019-04-19 12:46:10,127] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-8 (state.change.logger)
[2019-04-19 12:46:10,127] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-13 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-2 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-5 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-10 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-7 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-4 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-9 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-12 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-1 (state.change.logger)
[2019-04-19 12:46:10,128] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-6 (state.change.logger)
[2019-04-19 12:46:10,129] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-3 (state.change.logger)
[2019-04-19 12:46:10,129] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails-14 (state.change.logger)
[2019-04-19 12:46:10,131] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-9 (state.change.logger)
[2019-04-19 12:46:10,243] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-6 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,276] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-2 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,280] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-3 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,282] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-8 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,283] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-7 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,279] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-10 (state.change.logger)
[2019-04-19 12:46:10,288] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-11 (state.change.logger)
[2019-04-19 12:46:10,288] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-12 (state.change.logger)
[2019-04-19 12:46:10,288] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-13 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-14 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-1 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-2 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-3 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-4 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-5 (state.change.logger)
[2019-04-19 12:46:10,289] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-6 (state.change.logger)
[2019-04-19 12:46:10,290] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-7 (state.change.logger)
[2019-04-19 12:46:10,290] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-8 (state.change.logger)
[2019-04-19 12:46:10,290] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 1 from controller 1 epoch 1 for partition emails-0 (state.change.logger)
[2019-04-19 12:46:10,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-10 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-14 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-12 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-11 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,320] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-9 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,349] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-13 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,357] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-0 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,362] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-1 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,363] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-5 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,370] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails-4 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:10,502] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-6 (state.change.logger)
[2019-04-19 12:46:10,510] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-13 (state.change.logger)
[2019-04-19 12:46:10,513] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-3 (state.change.logger)
[2019-04-19 12:46:10,514] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-0 (state.change.logger)
[2019-04-19 12:46:10,514] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-10 (state.change.logger)
[2019-04-19 12:46:10,515] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-7 (state.change.logger)
[2019-04-19 12:46:10,515] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-4 (state.change.logger)
[2019-04-19 12:46:10,515] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-1 (state.change.logger)
[2019-04-19 12:46:10,515] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-14 (state.change.logger)
[2019-04-19 12:46:10,517] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-11 (state.change.logger)
[2019-04-19 12:46:10,517] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-8 (state.change.logger)
[2019-04-19 12:46:10,517] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-5 (state.change.logger)
[2019-04-19 12:46:10,517] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-2 (state.change.logger)
[2019-04-19 12:46:10,517] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-12 (state.change.logger)
[2019-04-19 12:46:10,520] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition emails-9 (state.change.logger)
[2019-04-19 12:46:10,528] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions emails-0,emails-3,emails-10,emails-1,emails-7,emails-11,emails-4,emails-14,emails-6,emails-8,emails-13,emails-2,emails-5,emails-9,emails-12 (kafka.server.ReplicaFetcherManager)
[2019-04-19 12:46:10,736] INFO [Log partition=emails-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:10,771] INFO [Log partition=emails-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 133 ms (kafka.log.Log)
[2019-04-19 12:46:10,777] INFO Created log for partition emails-6 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:10,780] INFO [Partition emails-6 broker=1] No checkpointed highwatermark is found for partition emails-6 (kafka.cluster.Partition)
[2019-04-19 12:46:10,789] INFO Replica loaded for partition emails-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:10,802] INFO [Partition emails-6 broker=1] emails-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:10,846] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-6 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:10,877] INFO [Log partition=emails-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:10,879] INFO [Log partition=emails-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-04-19 12:46:10,882] INFO Created log for partition emails-13 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:10,884] INFO [Partition emails-13 broker=1] No checkpointed highwatermark is found for partition emails-13 (kafka.cluster.Partition)
[2019-04-19 12:46:10,885] INFO Replica loaded for partition emails-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:10,886] INFO [Partition emails-13 broker=1] emails-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:10,887] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-13 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:10,903] INFO [Log partition=emails-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:10,915] INFO [Log partition=emails-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-04-19 12:46:10,923] INFO Created log for partition emails-3 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:10,935] INFO [Partition emails-3 broker=1] No checkpointed highwatermark is found for partition emails-3 (kafka.cluster.Partition)
[2019-04-19 12:46:10,935] INFO Replica loaded for partition emails-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:10,935] INFO [Partition emails-3 broker=1] emails-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:10,936] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-3 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:10,942] INFO [Log partition=emails-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:10,943] INFO [Log partition=emails-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:10,947] INFO Created log for partition emails-0 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:10,952] INFO [Partition emails-0 broker=1] No checkpointed highwatermark is found for partition emails-0 (kafka.cluster.Partition)
[2019-04-19 12:46:10,956] INFO Replica loaded for partition emails-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:10,965] INFO [Partition emails-0 broker=1] emails-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:10,969] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-0 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:10,975] INFO [Log partition=emails-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:10,977] INFO [Log partition=emails-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:10,981] INFO Created log for partition emails-10 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:10,983] INFO [Partition emails-10 broker=1] No checkpointed highwatermark is found for partition emails-10 (kafka.cluster.Partition)
[2019-04-19 12:46:10,983] INFO Replica loaded for partition emails-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:10,985] INFO [Partition emails-10 broker=1] emails-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:10,988] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-10 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,005] INFO [Log partition=emails-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,016] INFO [Log partition=emails-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-04-19 12:46:11,020] INFO Created log for partition emails-7 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,020] INFO [Partition emails-7 broker=1] No checkpointed highwatermark is found for partition emails-7 (kafka.cluster.Partition)
[2019-04-19 12:46:11,020] INFO Replica loaded for partition emails-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,021] INFO [Partition emails-7 broker=1] emails-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,022] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-7 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,036] INFO [Log partition=emails-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,039] INFO [Log partition=emails-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-04-19 12:46:11,043] INFO Created log for partition emails-4 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,043] INFO [Partition emails-4 broker=1] No checkpointed highwatermark is found for partition emails-4 (kafka.cluster.Partition)
[2019-04-19 12:46:11,043] INFO Replica loaded for partition emails-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,044] INFO [Partition emails-4 broker=1] emails-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,044] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-4 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,073] INFO [Log partition=emails-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,078] INFO [Log partition=emails-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-04-19 12:46:11,080] INFO Created log for partition emails-1 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,083] INFO [Partition emails-1 broker=1] No checkpointed highwatermark is found for partition emails-1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,085] INFO Replica loaded for partition emails-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,086] INFO [Partition emails-1 broker=1] emails-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,089] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-1 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,096] INFO [Log partition=emails-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,098] INFO [Log partition=emails-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:11,110] INFO Created log for partition emails-14 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,113] INFO [Partition emails-14 broker=1] No checkpointed highwatermark is found for partition emails-14 (kafka.cluster.Partition)
[2019-04-19 12:46:11,115] INFO Replica loaded for partition emails-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,117] INFO [Partition emails-14 broker=1] emails-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,118] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-14 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,126] INFO [Log partition=emails-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,126] INFO [Log partition=emails-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:11,129] INFO Created log for partition emails-11 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,130] INFO [Partition emails-11 broker=1] No checkpointed highwatermark is found for partition emails-11 (kafka.cluster.Partition)
[2019-04-19 12:46:11,131] INFO Replica loaded for partition emails-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,132] INFO [Partition emails-11 broker=1] emails-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,133] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-11 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,139] INFO [Log partition=emails-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,141] INFO [Log partition=emails-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:11,144] INFO Created log for partition emails-8 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,145] INFO [Partition emails-8 broker=1] No checkpointed highwatermark is found for partition emails-8 (kafka.cluster.Partition)
[2019-04-19 12:46:11,146] INFO Replica loaded for partition emails-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,149] INFO [Partition emails-8 broker=1] emails-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,151] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-8 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,166] INFO [Log partition=emails-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,169] INFO [Log partition=emails-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:11,171] INFO Created log for partition emails-5 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,179] INFO [Partition emails-5 broker=1] No checkpointed highwatermark is found for partition emails-5 (kafka.cluster.Partition)
[2019-04-19 12:46:11,179] INFO Replica loaded for partition emails-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,179] INFO [Partition emails-5 broker=1] emails-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,180] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-5 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,186] INFO [Log partition=emails-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,187] INFO [Log partition=emails-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:11,190] INFO Created log for partition emails-2 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,191] INFO [Partition emails-2 broker=1] No checkpointed highwatermark is found for partition emails-2 (kafka.cluster.Partition)
[2019-04-19 12:46:11,191] INFO Replica loaded for partition emails-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,192] INFO [Partition emails-2 broker=1] emails-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,193] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-2 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,200] INFO [Log partition=emails-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,202] INFO [Log partition=emails-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:11,205] INFO Created log for partition emails-12 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,206] INFO [Partition emails-12 broker=1] No checkpointed highwatermark is found for partition emails-12 (kafka.cluster.Partition)
[2019-04-19 12:46:11,207] INFO Replica loaded for partition emails-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,209] INFO [Partition emails-12 broker=1] emails-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,216] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-12 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,223] INFO [Log partition=emails-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:11,225] INFO [Log partition=emails-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:11,232] INFO Created log for partition emails-9 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:11,233] INFO [Partition emails-9 broker=1] No checkpointed highwatermark is found for partition emails-9 (kafka.cluster.Partition)
[2019-04-19 12:46:11,233] INFO Replica loaded for partition emails-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:11,234] INFO [Partition emails-9 broker=1] emails-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:11,236] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 1 for partition emails-9 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:11,240] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-6 (state.change.logger)
[2019-04-19 12:46:11,240] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-13 (state.change.logger)
[2019-04-19 12:46:11,241] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-3 (state.change.logger)
[2019-04-19 12:46:11,242] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-0 (state.change.logger)
[2019-04-19 12:46:11,243] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-10 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-7 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-4 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-1 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-14 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-11 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-8 (state.change.logger)
[2019-04-19 12:46:11,249] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-5 (state.change.logger)
[2019-04-19 12:46:11,250] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-2 (state.change.logger)
[2019-04-19 12:46:11,250] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-12 (state.change.logger)
[2019-04-19 12:46:11,250] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition emails-9 (state.change.logger)
[2019-04-19 12:46:11,303] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2019-04-19 12:46:11,312] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partitions=[{topic=emails,partition=9,error_code=0},{topic=emails,partition=10,error_code=0},{topic=emails,partition=11,error_code=0},{topic=emails,partition=12,error_code=0},{topic=emails,partition=13,error_code=0},{topic=emails,partition=14,error_code=0},{topic=emails,partition=1,error_code=0},{topic=emails,partition=2,error_code=0},{topic=emails,partition=3,error_code=0},{topic=emails,partition=4,error_code=0},{topic=emails,partition=5,error_code=0},{topic=emails,partition=6,error_code=0},{topic=emails,partition=7,error_code=0},{topic=emails,partition=8,error_code=0},{topic=emails,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 1 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:11,334] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,336] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,336] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,337] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,343] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,363] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,365] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,365] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,365] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,365] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,365] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,369] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,382] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,382] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,382] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[2019-04-19 12:46:11,394] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 2 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:14,130] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2019-04-19 12:46:14,131] INFO Starting metrics collection from monitored component... (io.confluent.support.metrics.BaseMetricsReporter)
[2019-04-19 12:46:14,987] WARN The replication factor of topic __confluent.support.metrics will be set to 1, which is less than the desired replication factor of 3 (reason: this cluster contains only 1 brokers).  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2019-04-19 12:46:14,987] INFO Attempting to create topic __confluent.support.metrics with 1 replicas, assuming 1 total brokers (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2019-04-19 12:46:15,050] INFO Topic creation Map(__confluent.support.metrics-0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[2019-04-19 12:46:15,063] INFO [Controller id=1] New topics: [Set(__confluent.support.metrics)], deleted topics: [Set()], new partition replica assignment [Map(__confluent.support.metrics-0 -> Vector(1))] (kafka.controller.KafkaController)
[2019-04-19 12:46:15,064] INFO [Controller id=1] New partition creation callback for __confluent.support.metrics-0 (kafka.controller.KafkaController)
[2019-04-19 12:46:15,066] TRACE [Controller id=1 epoch=1] Changed partition __confluent.support.metrics-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:15,070] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __confluent.support.metrics-0 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:15,086] TRACE [Controller id=1 epoch=1] Changed partition __confluent.support.metrics-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:15,087] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __confluent.support.metrics-0 (state.change.logger)
[2019-04-19 12:46:15,088] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __confluent.support.metrics-0 (state.change.logger)
[2019-04-19 12:46:15,090] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 3 from controller 1 epoch 1 for partition __confluent.support.metrics-0 (state.change.logger)
[2019-04-19 12:46:15,092] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __confluent.support.metrics-0 (state.change.logger)
[2019-04-19 12:46:15,093] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __confluent.support.metrics-0 (kafka.server.ReplicaFetcherManager)
[2019-04-19 12:46:15,089] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __confluent.support.metrics-0 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:15,111] INFO [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:15,116] INFO [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-04-19 12:46:15,121] INFO Created log for partition __confluent.support.metrics-0 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 31536000000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:15,133] INFO [Partition __confluent.support.metrics-0 broker=1] No checkpointed highwatermark is found for partition __confluent.support.metrics-0 (kafka.cluster.Partition)
[2019-04-19 12:46:15,145] INFO Replica loaded for partition __confluent.support.metrics-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:15,146] INFO [Partition __confluent.support.metrics-0 broker=1] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:15,133] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://kafka:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2019-04-19 12:46:15,150] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 3 for partition __confluent.support.metrics-0 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:15,155] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __confluent.support.metrics-0 (state.change.logger)
[2019-04-19 12:46:15,156] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2019-04-19 12:46:15,161] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partitions=[{topic=__confluent.support.metrics,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 3 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:15,174] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __confluent.support.metrics-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[2019-04-19 12:46:15,177] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 4 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:15,196] INFO Kafka version : 2.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:15,197] INFO Kafka commitId : 887f2c180e0a7130 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:15,289] INFO Cluster ID: t8fF06H2TrSdRZJb7qBbkA (org.apache.kafka.clients.Metadata)
[2019-04-19 12:46:15,450] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __confluent.support.metrics-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:15,532] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2019-04-19 12:46:15,539] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2019-04-19 12:46:16,575] INFO [Controller id=1] New topics: [Set(emails_two)], deleted topics: [Set()], new partition replica assignment [Map(emails_two-4 -> Vector(1), emails_two-10 -> Vector(1), emails_two-0 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), emails_two-3 -> Vector(1), emails_two-9 -> Vector(1), emails_two-14 -> Vector(1), emails_two-6 -> Vector(1), emails_two-12 -> Vector(1), emails_two-2 -> Vector(1), emails_two-7 -> Vector(1), emails_two-8 -> Vector(1), emails_two-11 -> Vector(1), emails_two-1 -> Vector(1))] (kafka.controller.KafkaController)
[2019-04-19 12:46:16,576] INFO [Controller id=1] New partition creation callback for emails_two-4,emails_two-10,emails_two-0,emails_two-5,emails_two-13,emails_two-3,emails_two-9,emails_two-14,emails_two-6,emails_two-12,emails_two-2,emails_two-7,emails_two-8,emails_two-11,emails_two-1 (kafka.controller.KafkaController)
[2019-04-19 12:46:16,577] TRACE [Controller id=1 epoch=1] Changed partition emails_two-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,578] TRACE [Controller id=1 epoch=1] Changed partition emails_two-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,578] TRACE [Controller id=1 epoch=1] Changed partition emails_two-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,580] TRACE [Controller id=1 epoch=1] Changed partition emails_two-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,583] TRACE [Controller id=1 epoch=1] Changed partition emails_two-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,584] TRACE [Controller id=1 epoch=1] Changed partition emails_two-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,585] TRACE [Controller id=1 epoch=1] Changed partition emails_two-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,586] TRACE [Controller id=1 epoch=1] Changed partition emails_two-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,586] TRACE [Controller id=1 epoch=1] Changed partition emails_two-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,588] TRACE [Controller id=1 epoch=1] Changed partition emails_two-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,589] TRACE [Controller id=1 epoch=1] Changed partition emails_two-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,590] TRACE [Controller id=1 epoch=1] Changed partition emails_two-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,590] TRACE [Controller id=1 epoch=1] Changed partition emails_two-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,591] TRACE [Controller id=1 epoch=1] Changed partition emails_two-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,592] TRACE [Controller id=1 epoch=1] Changed partition emails_two-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:16,597] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-4 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-5 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-6 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-11 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-8 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-12 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-13 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-10 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-1 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-14 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-2 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-0 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-3 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-9 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,598] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-7 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:16,711] TRACE [Controller id=1 epoch=1] Changed partition emails_two-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,711] TRACE [Controller id=1 epoch=1] Changed partition emails_two-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,711] TRACE [Controller id=1 epoch=1] Changed partition emails_two-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Changed partition emails_two-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-9 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-6 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-11 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-3 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-14 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-8 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-0 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-5 (state.change.logger)
[2019-04-19 12:46:16,712] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-13 (state.change.logger)
[2019-04-19 12:46:16,713] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-2 (state.change.logger)
[2019-04-19 12:46:16,713] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-10 (state.change.logger)
[2019-04-19 12:46:16,713] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-4 (state.change.logger)
[2019-04-19 12:46:16,713] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-7 (state.change.logger)
[2019-04-19 12:46:16,713] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-1 (state.change.logger)
[2019-04-19 12:46:16,713] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_two-12 (state.change.logger)
[2019-04-19 12:46:16,715] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-9 (state.change.logger)
[2019-04-19 12:46:16,717] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-3 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-2 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-5 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-4 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-1 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-0 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-11 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-10 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-13 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-12 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-7 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-6 (state.change.logger)
[2019-04-19 12:46:16,719] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-9 (state.change.logger)
[2019-04-19 12:46:16,720] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-8 (state.change.logger)
[2019-04-19 12:46:16,720] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 5 from controller 1 epoch 1 for partition emails_two-14 (state.change.logger)
[2019-04-19 12:46:16,718] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-6 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-11 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-14 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-3 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-8 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-0 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-5 (state.change.logger)
[2019-04-19 12:46:16,726] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-2 (state.change.logger)
[2019-04-19 12:46:16,727] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-13 (state.change.logger)
[2019-04-19 12:46:16,727] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-10 (state.change.logger)
[2019-04-19 12:46:16,727] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-4 (state.change.logger)
[2019-04-19 12:46:16,727] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-7 (state.change.logger)
[2019-04-19 12:46:16,727] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-1 (state.change.logger)
[2019-04-19 12:46:16,727] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_two-12 (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-4 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-5 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-6 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-11 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-8 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-12 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-13 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,732] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-10 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,733] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-1 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,733] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-14 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,749] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-2 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,750] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-0 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,752] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-3 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,753] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-9 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,755] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_two-7 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:16,790] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-2 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-12 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-9 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-6 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-3 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-0 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-13 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-10 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-7 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-4 (state.change.logger)
[2019-04-19 12:46:16,797] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-1 (state.change.logger)
[2019-04-19 12:46:16,798] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-14 (state.change.logger)
[2019-04-19 12:46:16,798] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-11 (state.change.logger)
[2019-04-19 12:46:16,798] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-8 (state.change.logger)
[2019-04-19 12:46:16,798] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition emails_two-5 (state.change.logger)
[2019-04-19 12:46:16,798] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions emails_two-4,emails_two-10,emails_two-0,emails_two-5,emails_two-13,emails_two-3,emails_two-9,emails_two-14,emails_two-6,emails_two-12,emails_two-2,emails_two-7,emails_two-8,emails_two-11,emails_two-1 (kafka.server.ReplicaFetcherManager)
[2019-04-19 12:46:16,809] INFO [Log partition=emails_two-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:16,819] INFO [Log partition=emails_two-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-04-19 12:46:16,822] INFO Created log for partition emails_two-2 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:16,824] INFO [Partition emails_two-2 broker=1] No checkpointed highwatermark is found for partition emails_two-2 (kafka.cluster.Partition)
[2019-04-19 12:46:16,825] INFO Replica loaded for partition emails_two-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:16,827] INFO [Partition emails_two-2 broker=1] emails_two-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:16,829] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-2 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:16,879] INFO [Log partition=emails_two-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:16,881] INFO [Log partition=emails_two-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:16,883] INFO Created log for partition emails_two-12 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:16,886] INFO [Partition emails_two-12 broker=1] No checkpointed highwatermark is found for partition emails_two-12 (kafka.cluster.Partition)
[2019-04-19 12:46:16,886] INFO Replica loaded for partition emails_two-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:16,887] INFO [Partition emails_two-12 broker=1] emails_two-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:16,890] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-12 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:16,918] INFO [Log partition=emails_two-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:16,920] INFO [Log partition=emails_two-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:16,924] INFO Created log for partition emails_two-9 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:16,933] INFO [Partition emails_two-9 broker=1] No checkpointed highwatermark is found for partition emails_two-9 (kafka.cluster.Partition)
[2019-04-19 12:46:16,939] INFO Replica loaded for partition emails_two-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:16,940] INFO [Partition emails_two-9 broker=1] emails_two-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:16,941] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-9 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:16,948] INFO [Log partition=emails_two-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:16,950] INFO [Log partition=emails_two-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:16,991] INFO Created log for partition emails_two-6 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:16,998] INFO [Partition emails_two-6 broker=1] No checkpointed highwatermark is found for partition emails_two-6 (kafka.cluster.Partition)
[2019-04-19 12:46:16,998] INFO Replica loaded for partition emails_two-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:16,998] INFO [Partition emails_two-6 broker=1] emails_two-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:16,999] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-6 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,013] INFO [Log partition=emails_two-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,015] INFO [Log partition=emails_two-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:17,017] INFO Created log for partition emails_two-3 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,020] INFO [Partition emails_two-3 broker=1] No checkpointed highwatermark is found for partition emails_two-3 (kafka.cluster.Partition)
[2019-04-19 12:46:17,020] INFO Replica loaded for partition emails_two-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,020] INFO [Partition emails_two-3 broker=1] emails_two-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,020] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-3 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,026] INFO [Log partition=emails_two-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,028] INFO [Log partition=emails_two-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:17,038] INFO Created log for partition emails_two-0 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,043] INFO [Partition emails_two-0 broker=1] No checkpointed highwatermark is found for partition emails_two-0 (kafka.cluster.Partition)
[2019-04-19 12:46:17,047] INFO Replica loaded for partition emails_two-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,048] INFO [Partition emails_two-0 broker=1] emails_two-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,050] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-0 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,060] INFO [Log partition=emails_two-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,063] INFO [Log partition=emails_two-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:17,066] INFO Created log for partition emails_two-13 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,068] INFO [Partition emails_two-13 broker=1] No checkpointed highwatermark is found for partition emails_two-13 (kafka.cluster.Partition)
[2019-04-19 12:46:17,069] INFO Replica loaded for partition emails_two-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,070] INFO [Partition emails_two-13 broker=1] emails_two-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,075] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-13 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,097] INFO [Log partition=emails_two-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,109] INFO [Log partition=emails_two-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-04-19 12:46:17,117] INFO Created log for partition emails_two-10 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,120] INFO [Partition emails_two-10 broker=1] No checkpointed highwatermark is found for partition emails_two-10 (kafka.cluster.Partition)
[2019-04-19 12:46:17,121] INFO Replica loaded for partition emails_two-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,122] INFO [Partition emails_two-10 broker=1] emails_two-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,123] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-10 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,141] INFO [Log partition=emails_two-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,145] INFO [Log partition=emails_two-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-04-19 12:46:17,168] INFO Created log for partition emails_two-7 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,205] INFO [Partition emails_two-7 broker=1] No checkpointed highwatermark is found for partition emails_two-7 (kafka.cluster.Partition)
[2019-04-19 12:46:17,206] INFO Replica loaded for partition emails_two-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,206] INFO [Partition emails_two-7 broker=1] emails_two-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,207] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-7 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,221] INFO [Log partition=emails_two-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,224] INFO [Log partition=emails_two-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:17,266] INFO Created log for partition emails_two-4 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,267] INFO [Partition emails_two-4 broker=1] No checkpointed highwatermark is found for partition emails_two-4 (kafka.cluster.Partition)
[2019-04-19 12:46:17,267] INFO Replica loaded for partition emails_two-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,268] INFO [Partition emails_two-4 broker=1] emails_two-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,268] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-4 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,301] INFO [Log partition=emails_two-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,303] INFO [Log partition=emails_two-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:17,318] INFO Created log for partition emails_two-1 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,324] INFO [Partition emails_two-1 broker=1] No checkpointed highwatermark is found for partition emails_two-1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,337] INFO Replica loaded for partition emails_two-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,337] INFO [Partition emails_two-1 broker=1] emails_two-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,338] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-1 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,359] INFO [Log partition=emails_two-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,376] INFO [Log partition=emails_two-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-04-19 12:46:17,380] INFO Created log for partition emails_two-14 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,386] INFO [Partition emails_two-14 broker=1] No checkpointed highwatermark is found for partition emails_two-14 (kafka.cluster.Partition)
[2019-04-19 12:46:17,386] INFO Replica loaded for partition emails_two-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,388] INFO [Partition emails_two-14 broker=1] emails_two-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,389] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-14 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,416] INFO [Log partition=emails_two-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,419] INFO [Log partition=emails_two-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-04-19 12:46:17,423] INFO Created log for partition emails_two-11 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,428] INFO [Partition emails_two-11 broker=1] No checkpointed highwatermark is found for partition emails_two-11 (kafka.cluster.Partition)
[2019-04-19 12:46:17,429] INFO Replica loaded for partition emails_two-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,429] INFO [Partition emails_two-11 broker=1] emails_two-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,499] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-11 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,669] INFO [Log partition=emails_two-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,671] INFO [Log partition=emails_two-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:17,685] INFO Created log for partition emails_two-8 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,687] INFO [Partition emails_two-8 broker=1] No checkpointed highwatermark is found for partition emails_two-8 (kafka.cluster.Partition)
[2019-04-19 12:46:17,687] INFO Replica loaded for partition emails_two-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,689] INFO [Partition emails_two-8 broker=1] emails_two-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,701] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-8 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,709] INFO [Log partition=emails_two-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:17,711] INFO [Log partition=emails_two-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:17,719] INFO Created log for partition emails_two-5 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:17,720] INFO [Partition emails_two-5 broker=1] No checkpointed highwatermark is found for partition emails_two-5 (kafka.cluster.Partition)
[2019-04-19 12:46:17,720] INFO Replica loaded for partition emails_two-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:17,720] INFO [Partition emails_two-5 broker=1] emails_two-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 5 for partition emails_two-5 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-2 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-12 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-9 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-6 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-3 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-0 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-13 (state.change.logger)
[2019-04-19 12:46:17,720] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-10 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-7 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-4 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-1 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-14 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-11 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-8 (state.change.logger)
[2019-04-19 12:46:17,721] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition emails_two-5 (state.change.logger)
[2019-04-19 12:46:17,721] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2019-04-19 12:46:17,834] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partitions=[{topic=emails_two,partition=3,error_code=0},{topic=emails_two,partition=2,error_code=0},{topic=emails_two,partition=5,error_code=0},{topic=emails_two,partition=4,error_code=0},{topic=emails_two,partition=1,error_code=0},{topic=emails_two,partition=0,error_code=0},{topic=emails_two,partition=11,error_code=0},{topic=emails_two,partition=10,error_code=0},{topic=emails_two,partition=13,error_code=0},{topic=emails_two,partition=12,error_code=0},{topic=emails_two,partition=7,error_code=0},{topic=emails_two,partition=6,error_code=0},{topic=emails_two,partition=9,error_code=0},{topic=emails_two,partition=8,error_code=0},{topic=emails_two,partition=14,error_code=0}]} for request LEADER_AND_ISR with correlation id 5 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:17,866] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,867] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,868] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,868] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,870] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,871] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,872] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,873] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,874] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,875] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,876] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,877] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,878] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,879] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,880] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_two-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[2019-04-19 12:46:17,884] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 6 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:18,545] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2019-04-19 12:46:19,024] INFO AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.id = 
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2019-04-19 12:46:19,078] WARN The configuration 'zookeeper.connect' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2019-04-19 12:46:19,078] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2019-04-19 12:46:19,079] INFO Kafka version : 2.0.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:19,080] INFO Kafka commitId : 887f2c180e0a7130 (org.apache.kafka.common.utils.AppInfoParser)
[2019-04-19 12:46:19,244] INFO Topic creation Map(_confluent-metrics-7 -> ArrayBuffer(1), _confluent-metrics-2 -> ArrayBuffer(1), _confluent-metrics-5 -> ArrayBuffer(1), _confluent-metrics-11 -> ArrayBuffer(1), _confluent-metrics-10 -> ArrayBuffer(1), _confluent-metrics-1 -> ArrayBuffer(1), _confluent-metrics-9 -> ArrayBuffer(1), _confluent-metrics-0 -> ArrayBuffer(1), _confluent-metrics-3 -> ArrayBuffer(1), _confluent-metrics-4 -> ArrayBuffer(1), _confluent-metrics-6 -> ArrayBuffer(1), _confluent-metrics-8 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[2019-04-19 12:46:19,277] INFO [Controller id=1] New topics: [Set(_confluent-metrics)], deleted topics: [Set()], new partition replica assignment [Map(_confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), _confluent-metrics-5 -> Vector(1), _confluent-metrics-11 -> Vector(1), _confluent-metrics-10 -> Vector(1), _confluent-metrics-1 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), _confluent-metrics-3 -> Vector(1), _confluent-metrics-4 -> Vector(1), _confluent-metrics-6 -> Vector(1), _confluent-metrics-8 -> Vector(1))] (kafka.controller.KafkaController)
[2019-04-19 12:46:19,280] INFO [Controller id=1] New partition creation callback for _confluent-metrics-7,_confluent-metrics-2,_confluent-metrics-5,_confluent-metrics-11,_confluent-metrics-10,_confluent-metrics-1,_confluent-metrics-9,_confluent-metrics-0,_confluent-metrics-3,_confluent-metrics-4,_confluent-metrics-6,_confluent-metrics-8 (kafka.controller.KafkaController)
[2019-04-19 12:46:19,287] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,288] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,289] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,290] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,291] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,292] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,293] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,306] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,308] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,309] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,309] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,310] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:19,317] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-11 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,317] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-5 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,318] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-6 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,319] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-3 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,319] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-2 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,320] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-4 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,321] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-7 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,321] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-10 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,322] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-9 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,322] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-1 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,323] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-0 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,324] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-8 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:19,420] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,420] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,421] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,421] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,422] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,423] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,423] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,424] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,425] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,425] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,426] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,426] TRACE [Controller id=1 epoch=1] Changed partition _confluent-metrics-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:19,427] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-6 (state.change.logger)
[2019-04-19 12:46:19,428] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-3 (state.change.logger)
[2019-04-19 12:46:19,428] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-8 (state.change.logger)
[2019-04-19 12:46:19,429] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-0 (state.change.logger)
[2019-04-19 12:46:19,430] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-11 (state.change.logger)
[2019-04-19 12:46:19,434] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-5 (state.change.logger)
[2019-04-19 12:46:19,435] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-2 (state.change.logger)
[2019-04-19 12:46:19,435] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-10 (state.change.logger)
[2019-04-19 12:46:19,436] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-4 (state.change.logger)
[2019-04-19 12:46:19,436] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-7 (state.change.logger)
[2019-04-19 12:46:19,437] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-1 (state.change.logger)
[2019-04-19 12:46:19,438] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition _confluent-metrics-9 (state.change.logger)
[2019-04-19 12:46:19,439] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-6 (state.change.logger)
[2019-04-19 12:46:19,440] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-3 (state.change.logger)
[2019-04-19 12:46:19,442] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-11 (state.change.logger)
[2019-04-19 12:46:19,447] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-9 (state.change.logger)
[2019-04-19 12:46:19,448] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-10 (state.change.logger)
[2019-04-19 12:46:19,449] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-7 (state.change.logger)
[2019-04-19 12:46:19,449] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-8 (state.change.logger)
[2019-04-19 12:46:19,450] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-5 (state.change.logger)
[2019-04-19 12:46:19,451] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-6 (state.change.logger)
[2019-04-19 12:46:19,451] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-3 (state.change.logger)
[2019-04-19 12:46:19,452] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-4 (state.change.logger)
[2019-04-19 12:46:19,453] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-1 (state.change.logger)
[2019-04-19 12:46:19,454] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-2 (state.change.logger)
[2019-04-19 12:46:19,455] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 7 from controller 1 epoch 1 for partition _confluent-metrics-0 (state.change.logger)
[2019-04-19 12:46:19,464] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-6 (state.change.logger)
[2019-04-19 12:46:19,465] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-8 (state.change.logger)
[2019-04-19 12:46:19,479] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-0 (state.change.logger)
[2019-04-19 12:46:19,480] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-11 (state.change.logger)
[2019-04-19 12:46:19,480] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-5 (state.change.logger)
[2019-04-19 12:46:19,481] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-2 (state.change.logger)
[2019-04-19 12:46:19,482] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-10 (state.change.logger)
[2019-04-19 12:46:19,483] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-4 (state.change.logger)
[2019-04-19 12:46:19,484] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-7 (state.change.logger)
[2019-04-19 12:46:19,485] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-1 (state.change.logger)
[2019-04-19 12:46:19,485] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition _confluent-metrics-9 (state.change.logger)
[2019-04-19 12:46:19,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-11 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,490] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-5 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,491] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-6 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,492] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-3 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,492] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-3 (state.change.logger)
[2019-04-19 12:46:19,493] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-0 (state.change.logger)
[2019-04-19 12:46:19,494] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-10 (state.change.logger)
[2019-04-19 12:46:19,495] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-7 (state.change.logger)
[2019-04-19 12:46:19,495] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-4 (state.change.logger)
[2019-04-19 12:46:19,496] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-1 (state.change.logger)
[2019-04-19 12:46:19,494] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-2 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,498] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-4 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,498] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-11 (state.change.logger)
[2019-04-19 12:46:19,500] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-8 (state.change.logger)
[2019-04-19 12:46:19,501] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-5 (state.change.logger)
[2019-04-19 12:46:19,499] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-7 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,503] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-10 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-9 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,505] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-1 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,506] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-0 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,504] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-2 (state.change.logger)
[2019-04-19 12:46:19,507] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-metrics-9 (state.change.logger)
[2019-04-19 12:46:19,512] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions _confluent-metrics-7,_confluent-metrics-2,_confluent-metrics-5,_confluent-metrics-11,_confluent-metrics-10,_confluent-metrics-1,_confluent-metrics-9,_confluent-metrics-0,_confluent-metrics-3,_confluent-metrics-4,_confluent-metrics-6,_confluent-metrics-8 (kafka.server.ReplicaFetcherManager)
[2019-04-19 12:46:19,513] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-metrics-8 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:19,520] INFO [Log partition=_confluent-metrics-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,523] INFO [Log partition=_confluent-metrics-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:19,534] INFO Created log for partition _confluent-metrics-6 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,536] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[2019-04-19 12:46:19,536] INFO Replica loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,538] INFO [Partition _confluent-metrics-6 broker=1] _confluent-metrics-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,539] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-6 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,544] INFO [Log partition=_confluent-metrics-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,546] INFO [Log partition=_confluent-metrics-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:19,558] INFO Created log for partition _confluent-metrics-3 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,560] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
[2019-04-19 12:46:19,560] INFO Replica loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,561] INFO [Partition _confluent-metrics-3 broker=1] _confluent-metrics-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,562] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-3 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,569] INFO [Log partition=_confluent-metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,570] INFO [Log partition=_confluent-metrics-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:19,572] INFO Created log for partition _confluent-metrics-0 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,578] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[2019-04-19 12:46:19,578] INFO Replica loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,580] INFO [Partition _confluent-metrics-0 broker=1] _confluent-metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,582] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-0 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,601] INFO [Log partition=_confluent-metrics-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,603] INFO [Log partition=_confluent-metrics-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-04-19 12:46:19,605] INFO Created log for partition _confluent-metrics-10 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,608] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[2019-04-19 12:46:19,608] INFO Replica loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,610] INFO [Partition _confluent-metrics-10 broker=1] _confluent-metrics-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,611] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-10 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,620] INFO [Log partition=_confluent-metrics-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,622] INFO [Log partition=_confluent-metrics-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:19,624] INFO Created log for partition _confluent-metrics-7 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,627] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[2019-04-19 12:46:19,634] INFO Replica loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,636] INFO [Partition _confluent-metrics-7 broker=1] _confluent-metrics-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,644] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-7 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,681] INFO [Log partition=_confluent-metrics-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,694] INFO [Log partition=_confluent-metrics-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-04-19 12:46:19,695] INFO Created log for partition _confluent-metrics-4 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,697] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[2019-04-19 12:46:19,697] INFO Replica loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,698] INFO [Partition _confluent-metrics-4 broker=1] _confluent-metrics-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,699] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-4 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,703] INFO [Log partition=_confluent-metrics-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,705] INFO [Log partition=_confluent-metrics-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:19,710] INFO Created log for partition _confluent-metrics-1 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,711] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,711] INFO Replica loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,713] INFO [Partition _confluent-metrics-1 broker=1] _confluent-metrics-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,722] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-1 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,727] INFO [Log partition=_confluent-metrics-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,728] INFO [Log partition=_confluent-metrics-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:19,730] INFO Created log for partition _confluent-metrics-11 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,732] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[2019-04-19 12:46:19,732] INFO Replica loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,733] INFO [Partition _confluent-metrics-11 broker=1] _confluent-metrics-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,734] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-11 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,756] INFO [Log partition=_confluent-metrics-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,758] INFO [Log partition=_confluent-metrics-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-04-19 12:46:19,762] INFO Created log for partition _confluent-metrics-8 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,763] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
[2019-04-19 12:46:19,764] INFO Replica loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,766] INFO [Partition _confluent-metrics-8 broker=1] _confluent-metrics-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,772] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-8 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,781] INFO [Log partition=_confluent-metrics-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,785] INFO [Log partition=_confluent-metrics-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-04-19 12:46:19,787] INFO Created log for partition _confluent-metrics-5 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,792] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[2019-04-19 12:46:19,792] INFO Replica loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,795] INFO [Partition _confluent-metrics-5 broker=1] _confluent-metrics-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,800] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-5 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,809] INFO [Log partition=_confluent-metrics-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,810] INFO [Log partition=_confluent-metrics-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:19,812] INFO Created log for partition _confluent-metrics-2 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,813] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
[2019-04-19 12:46:19,814] INFO Replica loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,815] INFO [Partition _confluent-metrics-2 broker=1] _confluent-metrics-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,816] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-2 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,830] INFO [Log partition=_confluent-metrics-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:19,832] INFO [Log partition=_confluent-metrics-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:19,835] INFO Created log for partition _confluent-metrics-9 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 14400000, segment.bytes -> 1073741824, retention.ms -> 259200000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:19,843] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
[2019-04-19 12:46:19,847] INFO Replica loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:19,852] INFO [Partition _confluent-metrics-9 broker=1] _confluent-metrics-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:19,861] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 7 for partition _confluent-metrics-9 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:19,861] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-6 (state.change.logger)
[2019-04-19 12:46:19,861] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-3 (state.change.logger)
[2019-04-19 12:46:19,866] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-0 (state.change.logger)
[2019-04-19 12:46:19,867] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-10 (state.change.logger)
[2019-04-19 12:46:19,868] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-7 (state.change.logger)
[2019-04-19 12:46:19,868] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-4 (state.change.logger)
[2019-04-19 12:46:19,869] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-1 (state.change.logger)
[2019-04-19 12:46:19,870] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-11 (state.change.logger)
[2019-04-19 12:46:19,870] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-8 (state.change.logger)
[2019-04-19 12:46:19,872] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-5 (state.change.logger)
[2019-04-19 12:46:19,872] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-2 (state.change.logger)
[2019-04-19 12:46:19,873] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition _confluent-metrics-9 (state.change.logger)
[2019-04-19 12:46:19,874] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2019-04-19 12:46:19,889] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partitions=[{topic=_confluent-metrics,partition=11,error_code=0},{topic=_confluent-metrics,partition=9,error_code=0},{topic=_confluent-metrics,partition=10,error_code=0},{topic=_confluent-metrics,partition=7,error_code=0},{topic=_confluent-metrics,partition=8,error_code=0},{topic=_confluent-metrics,partition=5,error_code=0},{topic=_confluent-metrics,partition=6,error_code=0},{topic=_confluent-metrics,partition=3,error_code=0},{topic=_confluent-metrics,partition=4,error_code=0},{topic=_confluent-metrics,partition=1,error_code=0},{topic=_confluent-metrics,partition=2,error_code=0},{topic=_confluent-metrics,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 7 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:19,892] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,893] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,894] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,895] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,895] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,896] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,897] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,901] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,902] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,903] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,904] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,904] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-metrics-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[2019-04-19 12:46:19,932] INFO Created metrics reporter topic _confluent-metrics (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[2019-04-19 12:46:19,966] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 8 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:21,227] INFO Cluster ID: t8fF06H2TrSdRZJb7qBbkA (org.apache.kafka.clients.Metadata)
[2019-04-19 12:46:21,434] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-8. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:21,445] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:21,496] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-11. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:21,504] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-5. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:21,509] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-4. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:21,516] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-7. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:24,652] INFO [Controller id=1] New topics: [Set(emails_three)], deleted topics: [Set()], new partition replica assignment [Map(emails_three-8 -> Vector(1), emails_three-11 -> Vector(1), emails_three-0 -> Vector(1), emails_three-6 -> Vector(1), emails_three-4 -> Vector(1), emails_three-5 -> Vector(1), emails_three-1 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), emails_three-3 -> Vector(1), emails_three-9 -> Vector(1), emails_three-12 -> Vector(1), emails_three-7 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1))] (kafka.controller.KafkaController)
[2019-04-19 12:46:24,652] INFO [Controller id=1] New partition creation callback for emails_three-8,emails_three-11,emails_three-0,emails_three-6,emails_three-4,emails_three-5,emails_three-1,emails_three-14,emails_three-10,emails_three-3,emails_three-9,emails_three-12,emails_three-7,emails_three-2,emails_three-13 (kafka.controller.KafkaController)
[2019-04-19 12:46:24,653] TRACE [Controller id=1 epoch=1] Changed partition emails_three-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,654] TRACE [Controller id=1 epoch=1] Changed partition emails_three-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,655] TRACE [Controller id=1 epoch=1] Changed partition emails_three-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,655] TRACE [Controller id=1 epoch=1] Changed partition emails_three-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,656] TRACE [Controller id=1 epoch=1] Changed partition emails_three-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,656] TRACE [Controller id=1 epoch=1] Changed partition emails_three-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,657] TRACE [Controller id=1 epoch=1] Changed partition emails_three-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,658] TRACE [Controller id=1 epoch=1] Changed partition emails_three-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,659] TRACE [Controller id=1 epoch=1] Changed partition emails_three-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,660] TRACE [Controller id=1 epoch=1] Changed partition emails_three-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,661] TRACE [Controller id=1 epoch=1] Changed partition emails_three-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,662] TRACE [Controller id=1 epoch=1] Changed partition emails_three-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,663] TRACE [Controller id=1 epoch=1] Changed partition emails_three-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,665] TRACE [Controller id=1 epoch=1] Changed partition emails_three-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,667] TRACE [Controller id=1 epoch=1] Changed partition emails_three-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:24,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-9 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,679] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-4 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,679] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-7 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,680] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-14 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,681] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-12 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,681] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-0 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,682] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-6 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,683] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-10 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,684] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-5 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,685] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-3 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,686] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-11 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,687] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-1 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,688] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-2 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,689] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-13 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,690] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-8 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:24,781] TRACE [Controller id=1 epoch=1] Changed partition emails_three-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,782] TRACE [Controller id=1 epoch=1] Changed partition emails_three-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,782] TRACE [Controller id=1 epoch=1] Changed partition emails_three-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,783] TRACE [Controller id=1 epoch=1] Changed partition emails_three-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,784] TRACE [Controller id=1 epoch=1] Changed partition emails_three-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,785] TRACE [Controller id=1 epoch=1] Changed partition emails_three-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,786] TRACE [Controller id=1 epoch=1] Changed partition emails_three-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,787] TRACE [Controller id=1 epoch=1] Changed partition emails_three-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,787] TRACE [Controller id=1 epoch=1] Changed partition emails_three-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,788] TRACE [Controller id=1 epoch=1] Changed partition emails_three-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,789] TRACE [Controller id=1 epoch=1] Changed partition emails_three-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,790] TRACE [Controller id=1 epoch=1] Changed partition emails_three-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,791] TRACE [Controller id=1 epoch=1] Changed partition emails_three-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,792] TRACE [Controller id=1 epoch=1] Changed partition emails_three-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,793] TRACE [Controller id=1 epoch=1] Changed partition emails_three-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:24,794] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-5 (state.change.logger)
[2019-04-19 12:46:24,795] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-13 (state.change.logger)
[2019-04-19 12:46:24,795] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-2 (state.change.logger)
[2019-04-19 12:46:24,796] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-10 (state.change.logger)
[2019-04-19 12:46:24,797] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-4 (state.change.logger)
[2019-04-19 12:46:24,797] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-7 (state.change.logger)
[2019-04-19 12:46:24,798] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-1 (state.change.logger)
[2019-04-19 12:46:24,799] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-12 (state.change.logger)
[2019-04-19 12:46:24,800] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-9 (state.change.logger)
[2019-04-19 12:46:24,800] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-6 (state.change.logger)
[2019-04-19 12:46:24,801] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-11 (state.change.logger)
[2019-04-19 12:46:24,802] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-3 (state.change.logger)
[2019-04-19 12:46:24,802] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-14 (state.change.logger)
[2019-04-19 12:46:24,803] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-8 (state.change.logger)
[2019-04-19 12:46:24,804] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition emails_three-0 (state.change.logger)
[2019-04-19 12:46:24,805] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-5 (state.change.logger)
[2019-04-19 12:46:24,806] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-13 (state.change.logger)
[2019-04-19 12:46:24,807] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-2 (state.change.logger)
[2019-04-19 12:46:24,808] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-10 (state.change.logger)
[2019-04-19 12:46:24,808] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-4 (state.change.logger)
[2019-04-19 12:46:24,809] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-7 (state.change.logger)
[2019-04-19 12:46:24,810] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-12 (state.change.logger)
[2019-04-19 12:46:24,812] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-0 (state.change.logger)
[2019-04-19 12:46:24,817] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-1 (state.change.logger)
[2019-04-19 12:46:24,818] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-1 (state.change.logger)
[2019-04-19 12:46:24,818] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-2 (state.change.logger)
[2019-04-19 12:46:24,819] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-3 (state.change.logger)
[2019-04-19 12:46:24,820] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-9 (state.change.logger)
[2019-04-19 12:46:24,820] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-6 (state.change.logger)
[2019-04-19 12:46:24,820] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-4 (state.change.logger)
[2019-04-19 12:46:24,821] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-11 (state.change.logger)
[2019-04-19 12:46:24,822] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-3 (state.change.logger)
[2019-04-19 12:46:24,821] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-5 (state.change.logger)
[2019-04-19 12:46:24,828] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-14 (state.change.logger)
[2019-04-19 12:46:24,829] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-6 (state.change.logger)
[2019-04-19 12:46:24,830] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-7 (state.change.logger)
[2019-04-19 12:46:24,831] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-8 (state.change.logger)
[2019-04-19 12:46:24,831] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-9 (state.change.logger)
[2019-04-19 12:46:24,832] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-10 (state.change.logger)
[2019-04-19 12:46:24,833] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-11 (state.change.logger)
[2019-04-19 12:46:24,829] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-8 (state.change.logger)
[2019-04-19 12:46:24,834] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition emails_three-0 (state.change.logger)
[2019-04-19 12:46:24,834] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-12 (state.change.logger)
[2019-04-19 12:46:24,836] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-13 (state.change.logger)
[2019-04-19 12:46:24,836] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 9 from controller 1 epoch 1 for partition emails_three-14 (state.change.logger)
[2019-04-19 12:46:24,838] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-9 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,838] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-4 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,838] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-7 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,839] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-14 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,839] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-12 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,839] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-0 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,840] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-6 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,847] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-10 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,847] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-5 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,848] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-3 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,849] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-11 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,849] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-1 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,850] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-2 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,850] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-13 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,850] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition emails_three-8 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:24,866] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-10 (state.change.logger)
[2019-04-19 12:46:24,866] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-7 (state.change.logger)
[2019-04-19 12:46:24,867] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-4 (state.change.logger)
[2019-04-19 12:46:24,868] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-1 (state.change.logger)
[2019-04-19 12:46:24,869] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-14 (state.change.logger)
[2019-04-19 12:46:24,870] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-2 (state.change.logger)
[2019-04-19 12:46:24,871] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-11 (state.change.logger)
[2019-04-19 12:46:24,872] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-8 (state.change.logger)
[2019-04-19 12:46:24,872] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-5 (state.change.logger)
[2019-04-19 12:46:24,874] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-12 (state.change.logger)
[2019-04-19 12:46:24,874] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-9 (state.change.logger)
[2019-04-19 12:46:24,874] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-6 (state.change.logger)
[2019-04-19 12:46:24,876] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-3 (state.change.logger)
[2019-04-19 12:46:24,876] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-0 (state.change.logger)
[2019-04-19 12:46:24,877] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition emails_three-13 (state.change.logger)
[2019-04-19 12:46:24,879] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions emails_three-8,emails_three-11,emails_three-0,emails_three-6,emails_three-4,emails_three-5,emails_three-1,emails_three-14,emails_three-10,emails_three-3,emails_three-9,emails_three-12,emails_three-7,emails_three-2,emails_three-13 (kafka.server.ReplicaFetcherManager)
[2019-04-19 12:46:24,884] INFO [Log partition=emails_three-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:24,891] INFO [Log partition=emails_three-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-04-19 12:46:24,894] INFO Created log for partition emails_three-10 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:24,897] INFO [Partition emails_three-10 broker=1] No checkpointed highwatermark is found for partition emails_three-10 (kafka.cluster.Partition)
[2019-04-19 12:46:24,898] INFO Replica loaded for partition emails_three-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:24,899] INFO [Partition emails_three-10 broker=1] emails_three-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:24,901] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-10 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:24,925] INFO [Log partition=emails_three-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:24,926] INFO [Log partition=emails_three-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-04-19 12:46:24,927] INFO Created log for partition emails_three-7 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:24,948] INFO [Partition emails_three-7 broker=1] No checkpointed highwatermark is found for partition emails_three-7 (kafka.cluster.Partition)
[2019-04-19 12:46:24,952] INFO Replica loaded for partition emails_three-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:24,955] INFO [Partition emails_three-7 broker=1] emails_three-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:24,963] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-7 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,015] INFO [Log partition=emails_three-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,035] INFO [Log partition=emails_three-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[2019-04-19 12:46:25,050] INFO Created log for partition emails_three-4 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,065] INFO [Partition emails_three-4 broker=1] No checkpointed highwatermark is found for partition emails_three-4 (kafka.cluster.Partition)
[2019-04-19 12:46:25,065] INFO Replica loaded for partition emails_three-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,066] INFO [Partition emails_three-4 broker=1] emails_three-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,068] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-4 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,109] INFO [Log partition=emails_three-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,113] INFO [Log partition=emails_three-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-04-19 12:46:25,115] INFO Created log for partition emails_three-1 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,116] INFO [Partition emails_three-1 broker=1] No checkpointed highwatermark is found for partition emails_three-1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,116] INFO Replica loaded for partition emails_three-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,116] INFO [Partition emails_three-1 broker=1] emails_three-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,117] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-1 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,122] INFO [Log partition=emails_three-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,126] INFO [Log partition=emails_three-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:25,128] INFO Created log for partition emails_three-14 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,129] INFO [Partition emails_three-14 broker=1] No checkpointed highwatermark is found for partition emails_three-14 (kafka.cluster.Partition)
[2019-04-19 12:46:25,129] INFO Replica loaded for partition emails_three-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,129] INFO [Partition emails_three-14 broker=1] emails_three-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,130] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-14 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,135] INFO [Log partition=emails_three-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,136] INFO [Log partition=emails_three-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:25,138] INFO Created log for partition emails_three-2 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,138] INFO [Partition emails_three-2 broker=1] No checkpointed highwatermark is found for partition emails_three-2 (kafka.cluster.Partition)
[2019-04-19 12:46:25,138] INFO Replica loaded for partition emails_three-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,139] INFO [Partition emails_three-2 broker=1] emails_three-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,139] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-2 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,170] INFO [Log partition=emails_three-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,185] INFO [Log partition=emails_three-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[2019-04-19 12:46:25,193] INFO Created log for partition emails_three-11 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,196] INFO [Partition emails_three-11 broker=1] No checkpointed highwatermark is found for partition emails_three-11 (kafka.cluster.Partition)
[2019-04-19 12:46:25,196] INFO Replica loaded for partition emails_three-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,202] INFO [Partition emails_three-11 broker=1] emails_three-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,213] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-11 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,350] INFO [Log partition=emails_three-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,355] INFO [Log partition=emails_three-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-04-19 12:46:25,372] INFO Created log for partition emails_three-8 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,382] INFO [Partition emails_three-8 broker=1] No checkpointed highwatermark is found for partition emails_three-8 (kafka.cluster.Partition)
[2019-04-19 12:46:25,386] INFO Replica loaded for partition emails_three-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,388] INFO [Partition emails_three-8 broker=1] emails_three-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,388] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-8 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,403] INFO [Log partition=emails_three-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,404] INFO [Log partition=emails_three-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:25,405] INFO Created log for partition emails_three-5 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,407] INFO [Partition emails_three-5 broker=1] No checkpointed highwatermark is found for partition emails_three-5 (kafka.cluster.Partition)
[2019-04-19 12:46:25,410] INFO Replica loaded for partition emails_three-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,410] INFO [Partition emails_three-5 broker=1] emails_three-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,411] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-5 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,416] INFO [Log partition=emails_three-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,419] INFO [Log partition=emails_three-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:25,421] INFO Created log for partition emails_three-12 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,430] INFO [Partition emails_three-12 broker=1] No checkpointed highwatermark is found for partition emails_three-12 (kafka.cluster.Partition)
[2019-04-19 12:46:25,433] INFO Replica loaded for partition emails_three-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,434] INFO [Partition emails_three-12 broker=1] emails_three-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,435] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-12 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,489] INFO [Log partition=emails_three-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,508] INFO [Log partition=emails_three-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[2019-04-19 12:46:25,654] INFO Created log for partition emails_three-9 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,664] INFO [Partition emails_three-9 broker=1] No checkpointed highwatermark is found for partition emails_three-9 (kafka.cluster.Partition)
[2019-04-19 12:46:25,664] INFO Replica loaded for partition emails_three-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,665] INFO [Partition emails_three-9 broker=1] emails_three-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,667] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-9 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,912] INFO [Log partition=emails_three-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,917] INFO [Log partition=emails_three-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[2019-04-19 12:46:25,940] INFO Created log for partition emails_three-6 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,942] INFO [Partition emails_three-6 broker=1] No checkpointed highwatermark is found for partition emails_three-6 (kafka.cluster.Partition)
[2019-04-19 12:46:25,942] INFO Replica loaded for partition emails_three-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,943] INFO [Partition emails_three-6 broker=1] emails_three-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,943] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-6 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,960] INFO [Log partition=emails_three-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,962] INFO [Log partition=emails_three-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:25,963] INFO Created log for partition emails_three-3 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,964] INFO [Partition emails_three-3 broker=1] No checkpointed highwatermark is found for partition emails_three-3 (kafka.cluster.Partition)
[2019-04-19 12:46:25,964] INFO Replica loaded for partition emails_three-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,964] INFO [Partition emails_three-3 broker=1] emails_three-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,965] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-3 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:25,981] INFO [Log partition=emails_three-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:25,983] INFO [Log partition=emails_three-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:25,986] INFO Created log for partition emails_three-0 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:25,989] INFO [Partition emails_three-0 broker=1] No checkpointed highwatermark is found for partition emails_three-0 (kafka.cluster.Partition)
[2019-04-19 12:46:25,989] INFO Replica loaded for partition emails_three-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:25,993] INFO [Partition emails_three-0 broker=1] emails_three-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:25,993] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-0 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:26,009] INFO [Log partition=emails_three-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:26,010] INFO [Log partition=emails_three-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:26,012] INFO Created log for partition emails_three-13 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:26,013] INFO [Partition emails_three-13 broker=1] No checkpointed highwatermark is found for partition emails_three-13 (kafka.cluster.Partition)
[2019-04-19 12:46:26,013] INFO Replica loaded for partition emails_three-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:26,084] INFO [Partition emails_three-13 broker=1] emails_three-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 9 for partition emails_three-13 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-10 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-7 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-4 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-1 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-14 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-2 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-11 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-8 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-5 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-12 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-9 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-6 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-3 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-0 (state.change.logger)
[2019-04-19 12:46:26,085] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition emails_three-13 (state.change.logger)
[2019-04-19 12:46:26,091] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2019-04-19 12:46:26,152] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partitions=[{topic=emails_three,partition=0,error_code=0},{topic=emails_three,partition=1,error_code=0},{topic=emails_three,partition=2,error_code=0},{topic=emails_three,partition=3,error_code=0},{topic=emails_three,partition=4,error_code=0},{topic=emails_three,partition=5,error_code=0},{topic=emails_three,partition=6,error_code=0},{topic=emails_three,partition=7,error_code=0},{topic=emails_three,partition=8,error_code=0},{topic=emails_three,partition=9,error_code=0},{topic=emails_three,partition=10,error_code=0},{topic=emails_three,partition=11,error_code=0},{topic=emails_three,partition=12,error_code=0},{topic=emails_three,partition=13,error_code=0},{topic=emails_three,partition=14,error_code=0}]} for request LEADER_AND_ISR with correlation id 9 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:26,241] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,241] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,242] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,242] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,242] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,242] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,253] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,254] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,254] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition emails_three-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[2019-04-19 12:46:26,255] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 10 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:34,036] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:34,091] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-1. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:34,109] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-3. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:34,120] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-9. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:34,156] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-6. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:34,171] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: _confluent-metrics-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:45,869] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-4. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:45,877] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-5. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:45,883] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-8. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:45,893] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:45,961] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_two-7. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:45,968] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_two-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:46,090] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:51,241] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(1), __consumer_offsets-30 -> ArrayBuffer(1), __consumer_offsets-8 -> ArrayBuffer(1), __consumer_offsets-21 -> ArrayBuffer(1), __consumer_offsets-4 -> ArrayBuffer(1), __consumer_offsets-27 -> ArrayBuffer(1), __consumer_offsets-7 -> ArrayBuffer(1), __consumer_offsets-9 -> ArrayBuffer(1), __consumer_offsets-46 -> ArrayBuffer(1), __consumer_offsets-25 -> ArrayBuffer(1), __consumer_offsets-35 -> ArrayBuffer(1), __consumer_offsets-41 -> ArrayBuffer(1), __consumer_offsets-33 -> ArrayBuffer(1), __consumer_offsets-23 -> ArrayBuffer(1), __consumer_offsets-49 -> ArrayBuffer(1), __consumer_offsets-47 -> ArrayBuffer(1), __consumer_offsets-16 -> ArrayBuffer(1), __consumer_offsets-28 -> ArrayBuffer(1), __consumer_offsets-31 -> ArrayBuffer(1), __consumer_offsets-36 -> ArrayBuffer(1), __consumer_offsets-42 -> ArrayBuffer(1), __consumer_offsets-3 -> ArrayBuffer(1), __consumer_offsets-18 -> ArrayBuffer(1), __consumer_offsets-37 -> ArrayBuffer(1), __consumer_offsets-15 -> ArrayBuffer(1), __consumer_offsets-24 -> ArrayBuffer(1), __consumer_offsets-38 -> ArrayBuffer(1), __consumer_offsets-17 -> ArrayBuffer(1), __consumer_offsets-48 -> ArrayBuffer(1), __consumer_offsets-19 -> ArrayBuffer(1), __consumer_offsets-11 -> ArrayBuffer(1), __consumer_offsets-13 -> ArrayBuffer(1), __consumer_offsets-2 -> ArrayBuffer(1), __consumer_offsets-43 -> ArrayBuffer(1), __consumer_offsets-6 -> ArrayBuffer(1), __consumer_offsets-14 -> ArrayBuffer(1), __consumer_offsets-20 -> ArrayBuffer(1), __consumer_offsets-0 -> ArrayBuffer(1), __consumer_offsets-44 -> ArrayBuffer(1), __consumer_offsets-39 -> ArrayBuffer(1), __consumer_offsets-12 -> ArrayBuffer(1), __consumer_offsets-45 -> ArrayBuffer(1), __consumer_offsets-1 -> ArrayBuffer(1), __consumer_offsets-5 -> ArrayBuffer(1), __consumer_offsets-26 -> ArrayBuffer(1), __consumer_offsets-29 -> ArrayBuffer(1), __consumer_offsets-34 -> ArrayBuffer(1), __consumer_offsets-10 -> ArrayBuffer(1), __consumer_offsets-32 -> ArrayBuffer(1), __consumer_offsets-40 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[2019-04-19 12:46:51,250] INFO [KafkaApi-1] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-04-19 12:46:51,255] INFO [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-22 -> Vector(1), __consumer_offsets-30 -> Vector(1), __consumer_offsets-8 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), __consumer_offsets-46 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), __consumer_offsets-48 -> Vector(1), __consumer_offsets-19 -> Vector(1), __consumer_offsets-11 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), __consumer_offsets-39 -> Vector(1), __consumer_offsets-12 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))] (kafka.controller.KafkaController)
[2019-04-19 12:46:51,258] INFO [Controller id=1] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
[2019-04-19 12:46:51,260] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,260] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,265] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,272] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,274] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,275] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,275] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,276] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,276] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,277] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,279] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,279] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,280] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,280] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,281] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,282] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,282] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,283] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,283] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,283] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,283] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,284] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,285] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2019-04-19 12:46:51,295] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,295] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,296] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,296] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,296] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,296] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,296] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,296] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,297] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,298] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,299] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,302] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,303] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
[2019-04-19 12:46:51,564] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,564] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,564] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,564] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,565] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,569] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,569] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,569] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,570] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,571] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,571] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,571] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,571] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,571] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,571] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,573] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,573] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,573] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,573] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,574] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,574] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,574] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,574] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,574] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,574] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,575] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,575] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,575] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,575] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,576] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,576] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,576] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,576] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,576] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,577] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
[2019-04-19 12:46:51,578] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
[2019-04-19 12:46:51,579] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
[2019-04-19 12:46:51,580] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
[2019-04-19 12:46:51,581] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
[2019-04-19 12:46:51,581] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
[2019-04-19 12:46:51,590] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
[2019-04-19 12:46:51,591] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
[2019-04-19 12:46:51,592] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
[2019-04-19 12:46:51,598] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-13 (state.change.logger)
[2019-04-19 12:46:51,599] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-46 (state.change.logger)
[2019-04-19 12:46:51,600] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-9 (state.change.logger)
[2019-04-19 12:46:51,600] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-42 (state.change.logger)
[2019-04-19 12:46:51,600] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-21 (state.change.logger)
[2019-04-19 12:46:51,600] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-17 (state.change.logger)
[2019-04-19 12:46:51,601] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-30 (state.change.logger)
[2019-04-19 12:46:51,601] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-26 (state.change.logger)
[2019-04-19 12:46:51,602] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-5 (state.change.logger)
[2019-04-19 12:46:51,602] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-38 (state.change.logger)
[2019-04-19 12:46:51,602] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-1 (state.change.logger)
[2019-04-19 12:46:51,602] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-34 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-16 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-45 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-12 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-41 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-24 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-20 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-49 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-0 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-29 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-25 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-8 (state.change.logger)
[2019-04-19 12:46:51,603] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-37 (state.change.logger)
[2019-04-19 12:46:51,604] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-4 (state.change.logger)
[2019-04-19 12:46:51,604] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-33 (state.change.logger)
[2019-04-19 12:46:51,604] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-15 (state.change.logger)
[2019-04-19 12:46:51,605] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-48 (state.change.logger)
[2019-04-19 12:46:51,605] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-11 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-44 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-23 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-19 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-32 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-28 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-7 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-40 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-3 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-36 (state.change.logger)
[2019-04-19 12:46:51,606] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-47 (state.change.logger)
[2019-04-19 12:46:51,607] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-14 (state.change.logger)
[2019-04-19 12:46:51,607] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-43 (state.change.logger)
[2019-04-19 12:46:51,610] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-49 (state.change.logger)
[2019-04-19 12:46:51,610] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-38 (state.change.logger)
[2019-04-19 12:46:51,611] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-10 (state.change.logger)
[2019-04-19 12:46:51,611] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-22 (state.change.logger)
[2019-04-19 12:46:51,611] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-18 (state.change.logger)
[2019-04-19 12:46:51,614] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-31 (state.change.logger)
[2019-04-19 12:46:51,612] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-16 (state.change.logger)
[2019-04-19 12:46:51,615] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-27 (state.change.logger)
[2019-04-19 12:46:51,615] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-8 (state.change.logger)
[2019-04-19 12:46:51,616] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-19 (state.change.logger)
[2019-04-19 12:46:51,617] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-13 (state.change.logger)
[2019-04-19 12:46:51,617] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-2 (state.change.logger)
[2019-04-19 12:46:51,618] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-35 (state.change.logger)
[2019-04-19 12:46:51,618] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-46 (state.change.logger)
[2019-04-19 12:46:51,619] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-27 (state.change.logger)
[2019-04-19 12:46:51,619] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-39 (state.change.logger)
[2019-04-19 12:46:51,619] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-6 (state.change.logger)
[2019-04-19 12:46:51,619] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-35 (state.change.logger)
[2019-04-19 12:46:51,621] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-24 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-5 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-43 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-21 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-32 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-10 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-37 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-48 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-40 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-18 (state.change.logger)
[2019-04-19 12:46:51,622] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-29 (state.change.logger)
[2019-04-19 12:46:51,623] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-7 (state.change.logger)
[2019-04-19 12:46:51,623] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-23 (state.change.logger)
[2019-04-19 12:46:51,621] TRACE [Broker id=1] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) correlation id 11 from controller 1 epoch 1 for partition __consumer_offsets-2 (state.change.logger)
[2019-04-19 12:46:51,623] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-45 (state.change.logger)
[2019-04-19 12:46:51,627] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-34 (state.change.logger)
[2019-04-19 12:46:51,627] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-26 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-15 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-4 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-42 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-31 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-9 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-20 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-12 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-1 (state.change.logger)
[2019-04-19 12:46:51,628] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-28 (state.change.logger)
[2019-04-19 12:46:51,629] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-17 (state.change.logger)
[2019-04-19 12:46:51,635] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-6 (state.change.logger)
[2019-04-19 12:46:51,635] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-39 (state.change.logger)
[2019-04-19 12:46:51,636] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-44 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-36 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-47 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-3 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-25 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-14 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-41 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-30 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-33 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-22 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-11 (state.change.logger)
[2019-04-19 12:46:51,637] TRACE [Controller id=1 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __consumer_offsets-0 (state.change.logger)
[2019-04-19 12:46:51,654] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,656] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,657] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,660] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,661] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,662] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,663] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,664] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,667] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,670] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,671] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,673] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,674] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,675] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,676] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,677] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,678] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
[2019-04-19 12:46:51,693] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[2019-04-19 12:46:51,693] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
[2019-04-19 12:46:51,693] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[2019-04-19 12:46:51,693] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
[2019-04-19 12:46:51,694] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
[2019-04-19 12:46:51,695] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[2019-04-19 12:46:51,696] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[2019-04-19 12:46:51,699] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[2019-04-19 12:46:51,709] INFO [Log partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,713] INFO [Log partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:51,715] INFO Created log for partition __consumer_offsets-0 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,717] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-04-19 12:46:51,719] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,720] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,722] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-0 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,728] INFO [Log partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,730] INFO [Log partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:51,733] INFO Created log for partition __consumer_offsets-29 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,735] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-04-19 12:46:51,736] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,737] INFO [Partition __consumer_offsets-29 broker=1] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,738] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-29 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,746] INFO [Log partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,747] INFO [Log partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:51,749] INFO Created log for partition __consumer_offsets-48 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,751] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-04-19 12:46:51,753] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,756] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,757] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-48 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,763] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,765] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:51,767] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,770] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-04-19 12:46:51,771] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,772] INFO [Partition __consumer_offsets-10 broker=1] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,773] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-10 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,779] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,781] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:51,784] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,786] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-04-19 12:46:51,787] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,788] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,790] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-45 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,803] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,804] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-04-19 12:46:51,808] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,811] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-04-19 12:46:51,811] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,812] INFO [Partition __consumer_offsets-26 broker=1] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,813] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-26 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,819] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,820] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2019-04-19 12:46:51,822] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,825] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-04-19 12:46:51,825] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,825] INFO [Partition __consumer_offsets-7 broker=1] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,826] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-7 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,833] INFO [Log partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,834] INFO [Log partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:51,836] INFO Created log for partition __consumer_offsets-42 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,838] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-04-19 12:46:51,838] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,838] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,839] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-42 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,844] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,845] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:51,847] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,849] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-04-19 12:46:51,849] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,850] INFO [Partition __consumer_offsets-4 broker=1] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,851] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-4 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,856] INFO [Log partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,857] INFO [Log partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:51,860] INFO Created log for partition __consumer_offsets-23 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,861] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-04-19 12:46:51,862] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,863] INFO [Partition __consumer_offsets-23 broker=1] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,864] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-23 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,869] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,871] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:51,873] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,874] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,875] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,876] INFO [Partition __consumer_offsets-1 broker=1] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,877] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-1 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,885] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,888] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:51,890] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,892] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-04-19 12:46:51,892] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,892] INFO [Partition __consumer_offsets-20 broker=1] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,894] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-20 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,902] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,903] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:51,905] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,907] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-04-19 12:46:51,907] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,907] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,908] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-39 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,915] INFO [Log partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,916] INFO [Log partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:51,919] INFO Created log for partition __consumer_offsets-17 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,921] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-04-19 12:46:51,921] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,922] INFO [Partition __consumer_offsets-17 broker=1] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,922] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-17 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,932] INFO [Log partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,933] INFO [Log partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:51,935] INFO Created log for partition __consumer_offsets-36 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,937] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-04-19 12:46:51,937] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,937] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,938] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-36 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,943] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,945] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:51,947] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,949] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-04-19 12:46:51,949] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,949] INFO [Partition __consumer_offsets-14 broker=1] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,951] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-14 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,960] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,961] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:51,963] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,971] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-04-19 12:46:51,971] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,971] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,972] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-33 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,979] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,982] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:51,984] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:51,985] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-04-19 12:46:51,986] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:51,987] INFO [Partition __consumer_offsets-49 broker=1] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:51,989] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-49 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:51,997] INFO [Log partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:51,998] INFO [Log partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,000] INFO Created log for partition __consumer_offsets-11 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,003] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-04-19 12:46:52,003] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,003] INFO [Partition __consumer_offsets-11 broker=1] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,004] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-11 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,011] INFO [Log partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,012] INFO [Log partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,014] INFO Created log for partition __consumer_offsets-30 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,018] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-04-19 12:46:52,019] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,019] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,020] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-30 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,025] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,026] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,028] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,030] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-04-19 12:46:52,030] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,030] INFO [Partition __consumer_offsets-46 broker=1] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,031] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-46 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,035] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,037] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,038] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,040] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-04-19 12:46:52,040] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,040] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,041] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-27 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,046] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,047] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,049] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,051] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-04-19 12:46:52,051] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,051] INFO [Partition __consumer_offsets-8 broker=1] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,052] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-8 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,057] INFO [Log partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,060] INFO [Log partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:52,063] INFO Created log for partition __consumer_offsets-24 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,064] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-04-19 12:46:52,064] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,065] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,066] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-24 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,112] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,114] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 42 ms (kafka.log.Log)
[2019-04-19 12:46:52,116] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,117] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-04-19 12:46:52,117] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,117] INFO [Partition __consumer_offsets-43 broker=1] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,118] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-43 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,127] INFO [Log partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,128] INFO [Log partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,130] INFO Created log for partition __consumer_offsets-5 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,132] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-04-19 12:46:52,132] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,133] INFO [Partition __consumer_offsets-5 broker=1] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,134] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-5 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,140] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,141] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:52,143] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,145] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-04-19 12:46:52,145] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,146] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,147] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-21 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,155] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,157] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:52,159] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,161] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-04-19 12:46:52,161] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,162] INFO [Partition __consumer_offsets-2 broker=1] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,163] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-2 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,168] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,169] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,171] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,174] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-04-19 12:46:52,174] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,175] INFO [Partition __consumer_offsets-40 broker=1] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,177] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-40 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,182] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,183] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,185] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,186] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-04-19 12:46:52,187] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,187] INFO [Partition __consumer_offsets-37 broker=1] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,188] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-37 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,198] INFO [Log partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,199] INFO [Log partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:52,201] INFO Created log for partition __consumer_offsets-18 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,203] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-04-19 12:46:52,203] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,204] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,205] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-18 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,212] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,213] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:52,215] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,216] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-04-19 12:46:52,216] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,217] INFO [Partition __consumer_offsets-34 broker=1] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,218] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-34 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,226] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,228] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,229] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,231] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-04-19 12:46:52,231] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,231] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,232] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-15 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,237] INFO [Log partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,238] INFO [Log partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,240] INFO Created log for partition __consumer_offsets-12 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,241] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-04-19 12:46:52,241] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,242] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,242] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-12 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,251] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,252] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,254] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,256] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-04-19 12:46:52,256] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,256] INFO [Partition __consumer_offsets-31 broker=1] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,257] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-31 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,262] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,263] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,265] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,267] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-04-19 12:46:52,267] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,268] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,269] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-9 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,279] INFO [Log partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,283] INFO [Log partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-04-19 12:46:52,286] INFO Created log for partition __consumer_offsets-47 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,288] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-04-19 12:46:52,288] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,289] INFO [Partition __consumer_offsets-47 broker=1] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,290] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-47 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,301] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,302] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-04-19 12:46:52,305] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,306] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-04-19 12:46:52,306] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,307] INFO [Partition __consumer_offsets-19 broker=1] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,308] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-19 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,312] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,314] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,316] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,317] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-04-19 12:46:52,317] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,318] INFO [Partition __consumer_offsets-28 broker=1] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,319] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-28 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,325] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,326] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,328] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,329] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-04-19 12:46:52,330] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,330] INFO [Partition __consumer_offsets-38 broker=1] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,331] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-38 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,336] INFO [Log partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,339] INFO [Log partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-04-19 12:46:52,340] INFO Created log for partition __consumer_offsets-35 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,342] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-04-19 12:46:52,342] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,343] INFO [Partition __consumer_offsets-35 broker=1] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,343] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-35 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,349] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,351] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,352] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,353] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-04-19 12:46:52,354] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,358] INFO [Partition __consumer_offsets-44 broker=1] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,359] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-44 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,365] INFO [Log partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,366] INFO [Log partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,368] INFO Created log for partition __consumer_offsets-6 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,370] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-04-19 12:46:52,370] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,371] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,372] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-6 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,377] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,378] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,379] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,381] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-04-19 12:46:52,381] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,381] INFO [Partition __consumer_offsets-25 broker=1] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,382] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-25 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,390] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,392] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,394] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,396] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-04-19 12:46:52,396] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,397] INFO [Partition __consumer_offsets-16 broker=1] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,398] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-16 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,405] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,407] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-04-19 12:46:52,408] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,410] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-04-19 12:46:52,410] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,411] INFO [Partition __consumer_offsets-22 broker=1] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,413] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-22 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,422] INFO [Log partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,423] INFO [Log partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,425] INFO Created log for partition __consumer_offsets-41 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,427] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-04-19 12:46:52,427] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,428] INFO [Partition __consumer_offsets-41 broker=1] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,429] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-41 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,433] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,438] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-04-19 12:46:52,440] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,441] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-04-19 12:46:52,441] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,442] INFO [Partition __consumer_offsets-32 broker=1] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,443] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-32 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,448] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,449] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,452] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,454] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-04-19 12:46:52,454] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,455] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,456] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-3 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,462] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-04-19 12:46:52,464] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-04-19 12:46:52,466] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-04-19 12:46:52,467] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-04-19 12:46:52,467] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-04-19 12:46:52,468] INFO [Partition __consumer_offsets-13 broker=1] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-04-19 12:46:52,469] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 1 with correlation id 11 for partition __consumer_offsets-13 (last update controller epoch 1) (state.change.logger)
[2019-04-19 12:46:52,469] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[2019-04-19 12:46:52,469] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
[2019-04-19 12:46:52,470] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[2019-04-19 12:46:52,470] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[2019-04-19 12:46:52,471] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[2019-04-19 12:46:52,471] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
[2019-04-19 12:46:52,471] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[2019-04-19 12:46:52,471] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[2019-04-19 12:46:52,472] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[2019-04-19 12:46:52,472] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
[2019-04-19 12:46:52,473] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[2019-04-19 12:46:52,474] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
[2019-04-19 12:46:52,474] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[2019-04-19 12:46:52,474] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
[2019-04-19 12:46:52,475] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[2019-04-19 12:46:52,476] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
[2019-04-19 12:46:52,476] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[2019-04-19 12:46:52,476] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[2019-04-19 12:46:52,477] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
[2019-04-19 12:46:52,477] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[2019-04-19 12:46:52,477] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[2019-04-19 12:46:52,477] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[2019-04-19 12:46:52,479] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
[2019-04-19 12:46:52,479] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[2019-04-19 12:46:52,479] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[2019-04-19 12:46:52,480] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
[2019-04-19 12:46:52,480] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[2019-04-19 12:46:52,480] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
[2019-04-19 12:46:52,480] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[2019-04-19 12:46:52,480] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[2019-04-19 12:46:52,480] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[2019-04-19 12:46:52,481] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[2019-04-19 12:46:52,481] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[2019-04-19 12:46:52,482] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[2019-04-19 12:46:52,482] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[2019-04-19 12:46:52,483] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[2019-04-19 12:46:52,483] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
[2019-04-19 12:46:52,483] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[2019-04-19 12:46:52,484] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[2019-04-19 12:46:52,484] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
[2019-04-19 12:46:52,484] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
[2019-04-19 12:46:52,486] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
[2019-04-19 12:46:52,486] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[2019-04-19 12:46:52,487] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[2019-04-19 12:46:52,488] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[2019-04-19 12:46:52,489] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[2019-04-19 12:46:52,489] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
[2019-04-19 12:46:52,489] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
[2019-04-19 12:46:52,489] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[2019-04-19 12:46:52,490] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[2019-04-19 12:46:52,494] INFO [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2019-04-19 12:46:52,500] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,503] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,503] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,505] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,505] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,506] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,507] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,509] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,510] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,512] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,513] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,514] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,514] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,517] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,517] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,517] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,518] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,520] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,521] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,522] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,522] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,522] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,522] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,522] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,530] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,532] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,532] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,534] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,534] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,535] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,537] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,537] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,541] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,541] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,547] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,548] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,549] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,551] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,552] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,552] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,555] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,556] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,572] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partitions=[{topic=__consumer_offsets,partition=13,error_code=0},{topic=__consumer_offsets,partition=46,error_code=0},{topic=__consumer_offsets,partition=9,error_code=0},{topic=__consumer_offsets,partition=42,error_code=0},{topic=__consumer_offsets,partition=21,error_code=0},{topic=__consumer_offsets,partition=17,error_code=0},{topic=__consumer_offsets,partition=30,error_code=0},{topic=__consumer_offsets,partition=26,error_code=0},{topic=__consumer_offsets,partition=5,error_code=0},{topic=__consumer_offsets,partition=38,error_code=0},{topic=__consumer_offsets,partition=1,error_code=0},{topic=__consumer_offsets,partition=34,error_code=0},{topic=__consumer_offsets,partition=16,error_code=0},{topic=__consumer_offsets,partition=45,error_code=0},{topic=__consumer_offsets,partition=12,error_code=0},{topic=__consumer_offsets,partition=41,error_code=0},{topic=__consumer_offsets,partition=24,error_code=0},{topic=__consumer_offsets,partition=20,error_code=0},{topic=__consumer_offsets,partition=49,error_code=0},{topic=__consumer_offsets,partition=0,error_code=0},{topic=__consumer_offsets,partition=29,error_code=0},{topic=__consumer_offsets,partition=25,error_code=0},{topic=__consumer_offsets,partition=8,error_code=0},{topic=__consumer_offsets,partition=37,error_code=0},{topic=__consumer_offsets,partition=4,error_code=0},{topic=__consumer_offsets,partition=33,error_code=0},{topic=__consumer_offsets,partition=15,error_code=0},{topic=__consumer_offsets,partition=48,error_code=0},{topic=__consumer_offsets,partition=11,error_code=0},{topic=__consumer_offsets,partition=44,error_code=0},{topic=__consumer_offsets,partition=23,error_code=0},{topic=__consumer_offsets,partition=19,error_code=0},{topic=__consumer_offsets,partition=32,error_code=0},{topic=__consumer_offsets,partition=28,error_code=0},{topic=__consumer_offsets,partition=7,error_code=0},{topic=__consumer_offsets,partition=40,error_code=0},{topic=__consumer_offsets,partition=3,error_code=0},{topic=__consumer_offsets,partition=36,error_code=0},{topic=__consumer_offsets,partition=47,error_code=0},{topic=__consumer_offsets,partition=14,error_code=0},{topic=__consumer_offsets,partition=43,error_code=0},{topic=__consumer_offsets,partition=10,error_code=0},{topic=__consumer_offsets,partition=22,error_code=0},{topic=__consumer_offsets,partition=18,error_code=0},{topic=__consumer_offsets,partition=31,error_code=0},{topic=__consumer_offsets,partition=27,error_code=0},{topic=__consumer_offsets,partition=39,error_code=0},{topic=__consumer_offsets,partition=6,error_code=0},{topic=__consumer_offsets,partition=35,error_code=0},{topic=__consumer_offsets,partition=2,error_code=0}]} for request LEADER_AND_ISR with correlation id 11 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:52,590] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 87 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,594] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,607] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,608] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,609] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,610] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,612] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,612] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,613] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,614] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,602] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,618] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,615] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,620] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,621] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,623] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,622] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,626] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,627] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,629] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,630] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,632] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,628] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,636] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,637] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,642] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,643] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,635] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,645] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,646] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,648] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,648] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,650] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,651] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,652] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,652] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,653] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,655] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,655] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,656] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,657] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,659] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,661] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,662] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,663] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,665] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,666] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,667] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,668] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,669] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,670] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,670] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,667] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 9 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,673] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,671] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,676] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,676] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,677] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,678] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,678] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,679] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,680] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,682] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,682] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,683] TRACE [Broker id=1] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[2019-04-19 12:46:52,686] TRACE [Controller id=1 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 12 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[2019-04-19 12:46:52,694] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 13 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,695] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,696] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,697] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,698] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,699] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,700] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,701] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,703] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,704] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,705] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,706] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,707] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,708] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,709] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,710] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,711] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,712] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,713] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,714] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,716] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,718] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,719] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,719] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,720] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,721] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,723] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,723] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,724] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,725] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,726] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,727] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,728] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,729] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:52,730] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:46:53,224] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 0 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:46:53,242] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 1 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:46:53,264] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:46:53,284] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-32. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:46:56,696] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 1 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:47:26,176] INFO [GroupCoordinator 1]: Member rdkafka-599f0676-02d9-4f6a-abb8-9e3d4b1aa0b0 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:47:26,179] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 2 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:47:26,197] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49738-6 (kafka.network.Processor)
[2019-04-19 12:47:26,201] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49734-5 (kafka.network.Processor)
[2019-04-19 12:47:56,151] INFO [GroupCoordinator 1]: Member rdkafka-cf8aa163-6735-4da6-a50b-2ec8ae497e60 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:47:56,153] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 2 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:47:56,154] INFO [GroupCoordinator 1]: Member rdkafka-11b694f7-b8b7-4111-b432-f9f6ed960fb8 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:47:56,156] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 3 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:51:08,572] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 12:51:08,581] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 12:51:08,591] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 12:51:08,594] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 12:52:07,671] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 3 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:07,677] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 4 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:07,690] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 4 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:10,811] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 4 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:10,812] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 5 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:10,842] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49756-8 (kafka.network.Processor)
[2019-04-19 12:52:10,880] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 5 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:10,884] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 6 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:10,890] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 6 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:14,117] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 6 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:14,121] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 7 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:14,132] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49760-9 (kafka.network.Processor)
[2019-04-19 12:52:14,196] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 7 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:14,199] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 8 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:14,209] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 8 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:17,474] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 8 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:17,483] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 9 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:17,496] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49764-10 (kafka.network.Processor)
[2019-04-19 12:52:32,442] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 9 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:32,445] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 10 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:32,452] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 10 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:35,584] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 10 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:35,584] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 11 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:35,609] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49782-12 (kafka.network.Processor)
[2019-04-19 12:52:35,650] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 11 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:35,658] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 12 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:35,664] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 12 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:38,736] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 12 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:38,740] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 13 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:38,743] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49786-13 (kafka.network.Processor)
[2019-04-19 12:52:38,776] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 13 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:38,786] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 14 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:38,804] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 14 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:41,913] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 14 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:41,914] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 15 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:52:41,927] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49790-14 (kafka.network.Processor)
[2019-04-19 12:55:30,694] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 15 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:30,702] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 16 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:30,720] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 16 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:33,820] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 16 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:33,821] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 17 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:33,825] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49808-16 (kafka.network.Processor)
[2019-04-19 12:55:33,866] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 17 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:33,871] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 18 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:33,878] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 18 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:36,960] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 18 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:36,960] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 19 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:36,996] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49812-17 (kafka.network.Processor)
[2019-04-19 12:55:37,014] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 19 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:37,018] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 20 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:37,026] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 20 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:40,098] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 20 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:40,099] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 21 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:40,101] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49816-18 (kafka.network.Processor)
[2019-04-19 12:55:46,210] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 21 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:46,216] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 22 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:46,226] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 22 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:49,305] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 22 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:49,329] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 23 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:49,337] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49834-20 (kafka.network.Processor)
[2019-04-19 12:55:49,394] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 23 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:49,396] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 24 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:49,403] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 24 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:52,454] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 24 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:52,454] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 25 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:52,456] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49838-21 (kafka.network.Processor)
[2019-04-19 12:55:52,504] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 25 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:52,510] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 26 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:52,515] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 26 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:55,595] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 26 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:55,596] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 27 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:55:55,599] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49842-22 (kafka.network.Processor)
[2019-04-19 12:56:01,778] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 27 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:01,779] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 28 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:01,784] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 28 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:02,805] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 10 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 12:56:04,875] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 28 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:04,878] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 29 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:04,881] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49860-24 (kafka.network.Processor)
[2019-04-19 12:56:04,924] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 29 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:04,928] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 30 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:04,941] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 30 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:08,021] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 30 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:08,023] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 31 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:08,027] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49864-25 (kafka.network.Processor)
[2019-04-19 12:56:08,072] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 31 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:08,077] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 32 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:08,084] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 32 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:08,257] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 12:56:08,260] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 12:56:08,263] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 12:56:08,263] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 12:56:11,144] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 32 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:11,149] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 33 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:11,154] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49868-26 (kafka.network.Processor)
[2019-04-19 12:56:17,256] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 33 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:17,262] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 34 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:17,268] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 34 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:20,308] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 34 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:20,312] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 35 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:20,318] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49886-28 (kafka.network.Processor)
[2019-04-19 12:56:20,349] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 35 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:20,352] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 36 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:20,357] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 36 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:23,407] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 36 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:23,407] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 37 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:23,409] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49890-29 (kafka.network.Processor)
[2019-04-19 12:56:23,467] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 37 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:23,475] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 38 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:23,487] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 38 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:26,567] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 38 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:26,568] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 39 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:26,571] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49894-30 (kafka.network.Processor)
[2019-04-19 12:56:32,764] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 39 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:32,766] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 40 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:32,771] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 40 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:35,874] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 40 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:35,878] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 41 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:35,880] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49912-32 (kafka.network.Processor)
[2019-04-19 12:56:35,937] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 41 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:35,945] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 42 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:35,950] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 42 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:38,993] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 42 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:38,994] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 43 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:38,998] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49916-33 (kafka.network.Processor)
[2019-04-19 12:56:39,074] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 43 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:39,077] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 44 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:39,082] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 44 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:42,122] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 44 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:42,122] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 45 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:42,125] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49920-34 (kafka.network.Processor)
[2019-04-19 12:56:48,809] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 45 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:48,811] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 46 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:48,818] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 46 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:51,895] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 46 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:51,898] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 47 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:51,901] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49938-36 (kafka.network.Processor)
[2019-04-19 12:56:51,936] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 47 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:51,941] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 48 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:51,966] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 48 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:55,034] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 48 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:55,034] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 49 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:55,039] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49942-37 (kafka.network.Processor)
[2019-04-19 12:56:55,079] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 49 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:55,087] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 50 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:55,105] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 50 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:58,256] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 50 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:58,258] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 51 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:56:58,261] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49946-38 (kafka.network.Processor)
[2019-04-19 12:57:04,657] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 51 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:04,663] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 52 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:04,681] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 52 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:07,718] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 52 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:07,718] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 53 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:07,744] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 53 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:07,758] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 54 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:07,768] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 54 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:10,914] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 54 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:10,916] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 55 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:10,920] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49968-41 (kafka.network.Processor)
[2019-04-19 12:57:10,953] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 55 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:10,957] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 56 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:10,963] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 56 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:14,055] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 56 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:14,056] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 57 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:14,059] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49972-42 (kafka.network.Processor)
[2019-04-19 12:57:20,263] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 57 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:20,265] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 58 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:20,273] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 58 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:23,428] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 58 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:23,429] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 59 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:23,435] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49990-44 (kafka.network.Processor)
[2019-04-19 12:57:23,476] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 59 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:23,479] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 60 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:23,485] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 60 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:26,585] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 60 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:26,585] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 61 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:26,590] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49994-45 (kafka.network.Processor)
[2019-04-19 12:57:26,627] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 61 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:26,631] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 62 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:26,644] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 62 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:29,730] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 62 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:29,730] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 63 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:29,733] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:49998-46 (kafka.network.Processor)
[2019-04-19 12:57:36,002] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 63 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:36,003] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 64 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:36,011] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 64 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:39,051] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 64 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:39,051] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 65 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:39,054] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50016-48 (kafka.network.Processor)
[2019-04-19 12:57:39,107] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 65 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:39,109] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 66 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:39,123] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 66 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:42,133] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 66 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:42,134] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 67 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:42,183] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 67 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:42,191] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 68 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:42,211] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 68 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:45,312] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 68 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:45,313] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 69 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:45,317] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50024-50 (kafka.network.Processor)
[2019-04-19 12:57:51,639] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 69 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:51,643] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 70 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:51,650] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 70 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:54,716] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 70 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:54,716] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 71 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:54,722] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50042-52 (kafka.network.Processor)
[2019-04-19 12:57:54,754] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 71 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:54,764] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 72 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:54,776] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 72 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:57,834] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 72 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:57,837] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 73 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:57,851] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50046-53 (kafka.network.Processor)
[2019-04-19 12:57:57,900] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 73 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:57,912] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 74 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:57:57,928] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 74 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:00,962] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 74 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:00,962] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 75 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:00,964] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50050-54 (kafka.network.Processor)
[2019-04-19 12:58:07,275] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 75 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:07,280] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 76 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:07,291] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 76 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:10,332] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 76 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:10,334] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 77 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:10,337] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50068-56 (kafka.network.Processor)
[2019-04-19 12:58:10,373] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 77 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:10,375] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 78 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:10,379] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 78 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:13,451] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 78 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:13,458] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 79 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:13,461] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50072-57 (kafka.network.Processor)
[2019-04-19 12:58:13,513] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 79 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:13,527] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 80 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:13,538] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 80 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:16,630] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 80 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:16,630] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 81 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:16,635] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50076-58 (kafka.network.Processor)
[2019-04-19 12:58:22,790] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 81 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:22,792] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 82 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:22,806] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 82 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:25,865] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 82 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:25,870] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 83 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:25,883] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50094-60 (kafka.network.Processor)
[2019-04-19 12:58:25,905] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 83 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:25,913] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 84 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:25,935] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 84 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:29,041] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 84 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:29,041] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 85 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:29,053] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50098-61 (kafka.network.Processor)
[2019-04-19 12:58:29,100] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 85 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:29,103] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 86 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:29,118] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 86 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:32,249] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 86 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:32,251] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 87 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:32,254] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50102-62 (kafka.network.Processor)
[2019-04-19 12:58:38,687] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 87 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:38,689] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 88 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:38,694] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 88 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:41,731] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 88 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:41,732] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 89 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:41,737] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50120-64 (kafka.network.Processor)
[2019-04-19 12:58:41,800] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 89 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:41,802] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 90 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:41,806] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 90 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:44,876] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 90 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:44,876] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 91 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:44,879] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50124-65 (kafka.network.Processor)
[2019-04-19 12:58:44,932] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 91 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:44,935] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 92 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:44,941] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 92 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:48,011] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 92 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:48,012] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 93 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:48,049] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50128-66 (kafka.network.Processor)
[2019-04-19 12:58:54,116] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 93 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:54,119] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 94 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:54,125] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 94 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:57,171] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 94 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:57,172] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 95 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:57,176] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50146-68 (kafka.network.Processor)
[2019-04-19 12:58:57,240] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 95 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:57,242] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 96 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:58:57,247] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 96 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:00,358] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 96 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:00,359] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 97 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:00,362] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50150-69 (kafka.network.Processor)
[2019-04-19 12:59:00,405] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 97 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:00,412] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 98 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:00,418] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 98 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:03,463] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 98 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:03,464] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 99 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:09,951] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 99 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:09,953] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 100 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:09,958] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 100 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:13,075] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 100 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:13,076] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 101 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:13,078] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50172-72 (kafka.network.Processor)
[2019-04-19 12:59:13,116] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 101 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:13,119] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 102 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:13,131] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 102 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:16,189] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 102 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:16,191] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 103 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:16,206] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50176-73 (kafka.network.Processor)
[2019-04-19 12:59:16,258] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 103 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:16,262] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 104 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:16,272] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 104 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:19,371] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 104 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:19,371] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 105 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:19,373] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50180-74 (kafka.network.Processor)
[2019-04-19 12:59:25,463] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 105 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:25,466] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 106 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:25,475] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 106 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:28,550] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 106 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:28,550] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 107 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:28,551] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50198-76 (kafka.network.Processor)
[2019-04-19 12:59:28,611] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 107 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:28,614] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 108 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:28,620] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 108 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:31,727] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 108 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:31,728] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 109 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:31,730] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50202-77 (kafka.network.Processor)
[2019-04-19 12:59:31,748] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 109 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:31,750] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 110 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:31,792] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 110 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:34,883] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 110 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:34,883] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 111 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:34,886] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50206-78 (kafka.network.Processor)
[2019-04-19 12:59:40,939] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 111 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:40,942] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 112 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:40,955] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 112 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:44,046] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 112 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:44,047] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 113 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:44,048] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50224-80 (kafka.network.Processor)
[2019-04-19 12:59:44,113] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 113 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:44,114] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 114 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:44,118] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 114 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:47,145] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 114 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:47,147] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 115 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:47,151] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50228-81 (kafka.network.Processor)
[2019-04-19 12:59:47,185] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 115 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:47,192] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 116 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:47,197] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 116 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:50,249] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 116 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:50,249] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 117 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:51,571] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_two-3. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 12:59:56,837] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 117 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:56,839] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 118 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:56,847] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 118 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:59,925] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 118 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:59,926] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 119 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:59,930] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50250-84 (kafka.network.Processor)
[2019-04-19 12:59:59,955] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 119 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:59,958] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 120 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 12:59:59,963] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 120 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:03,191] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 120 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:03,191] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 121 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:03,199] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50254-85 (kafka.network.Processor)
[2019-04-19 13:00:03,253] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 121 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:03,255] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 122 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:03,261] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 122 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:06,315] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 122 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:06,315] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 123 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:06,319] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50258-86 (kafka.network.Processor)
[2019-04-19 13:00:12,695] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 123 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:12,698] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 124 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:12,707] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 124 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:15,797] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 124 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:15,798] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 125 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:15,805] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50276-88 (kafka.network.Processor)
[2019-04-19 13:00:15,832] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 125 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:15,834] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 126 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:15,839] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 126 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:18,948] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 126 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:18,949] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 127 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:18,952] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50280-89 (kafka.network.Processor)
[2019-04-19 13:00:19,003] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 127 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:19,007] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 128 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:19,012] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 128 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:22,041] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 128 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:22,041] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 129 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:22,047] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50284-90 (kafka.network.Processor)
[2019-04-19 13:00:28,629] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 129 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:28,635] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 130 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:28,645] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 130 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:31,753] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 130 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:31,753] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 131 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:31,754] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50302-92 (kafka.network.Processor)
[2019-04-19 13:00:31,785] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 131 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:31,793] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 132 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:31,816] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 132 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:34,877] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 132 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:34,880] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 133 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:34,883] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50306-93 (kafka.network.Processor)
[2019-04-19 13:00:34,926] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 133 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:34,928] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 134 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:34,942] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 134 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:38,091] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 134 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:38,092] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 135 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:38,094] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50310-94 (kafka.network.Processor)
[2019-04-19 13:00:44,518] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 135 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:44,525] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 136 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:44,535] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 136 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:47,643] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 136 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:47,643] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 137 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:47,647] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50328-96 (kafka.network.Processor)
[2019-04-19 13:00:47,694] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 137 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:47,699] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 138 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:47,709] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 138 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:50,843] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 138 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:50,848] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 139 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:50,850] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50332-97 (kafka.network.Processor)
[2019-04-19 13:00:50,869] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 139 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:50,872] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 140 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:50,880] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 140 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:54,226] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 140 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:54,227] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 141 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:00:54,229] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50336-98 (kafka.network.Processor)
[2019-04-19 13:00:59,484] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-9. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:01:04,977] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 141 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:04,982] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 142 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:05,029] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 142 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:07,927] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:01:07,953] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:01:07,960] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:01:08,018] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:01:09,387] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 142 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:09,389] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 143 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:09,393] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50354-100 (kafka.network.Processor)
[2019-04-19 13:01:09,491] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 143 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:09,496] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 144 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:09,513] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 144 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:12,609] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 144 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:12,612] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 145 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:12,616] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50358-101 (kafka.network.Processor)
[2019-04-19 13:01:12,647] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 145 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:12,650] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 146 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:12,659] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 146 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:24,400] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 146 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:45,762] INFO [GroupCoordinator 1]: Member rdkafka-32f1c64f-4a4f-4fa9-a648-132d0e84c2b1 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:45,767] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 147 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:01:45,797] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50384-105 (kafka.network.Processor)
[2019-04-19 13:01:45,799] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50388-106 (kafka.network.Processor)
[2019-04-19 13:01:45,798] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50380-104 (kafka.network.Processor)
[2019-04-19 13:02:15,746] INFO [GroupCoordinator 1]: Member rdkafka-aef2d51e-7c89-4d04-8f6c-b9b2c310321d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:02:15,753] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 147 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:02:15,764] INFO [GroupCoordinator 1]: Member rdkafka-86bee659-45c4-460d-b19c-e67d380d9f40 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:02:15,770] INFO [GroupCoordinator 1]: Member rdkafka-8fb96b43-a259-4c2c-a4e3-2a21b5fd5264 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:02:15,773] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 148 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:55,042] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 148 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:55,049] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 149 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:55,056] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 149 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:58,194] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 149 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:58,195] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 150 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:58,198] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50420-110 (kafka.network.Processor)
[2019-04-19 13:03:58,269] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 150 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:58,270] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 151 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:03:58,276] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 151 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:01,346] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 151 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:01,347] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 152 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:01,349] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50424-111 (kafka.network.Processor)
[2019-04-19 13:04:01,393] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 152 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:01,405] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 153 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:01,414] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 153 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:04,466] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 153 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:04,466] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 154 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:04,469] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50428-112 (kafka.network.Processor)
[2019-04-19 13:04:10,643] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 154 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:10,652] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 155 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:10,656] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 155 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:13,774] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 155 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:13,775] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 156 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:04:13,777] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50446-114 (kafka.network.Processor)
[2019-04-19 13:05:23,734] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 156 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:23,748] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 157 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:23,755] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 157 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:26,814] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 157 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:26,818] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 158 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:26,823] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50464-117 (kafka.network.Processor)
[2019-04-19 13:05:26,885] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 158 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:26,891] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 159 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:26,895] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 159 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:30,002] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 159 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:30,003] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 160 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:30,007] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50468-118 (kafka.network.Processor)
[2019-04-19 13:05:30,083] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 160 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:30,090] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 161 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:30,094] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 161 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:33,167] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 161 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:33,168] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 162 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:33,180] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50472-118 (kafka.network.Processor)
[2019-04-19 13:05:43,789] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 162 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:43,796] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 163 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:43,802] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 163 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:46,861] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 163 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:46,862] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 164 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:46,866] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50490-121 (kafka.network.Processor)
[2019-04-19 13:05:46,923] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 164 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:46,933] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 165 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:46,949] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 165 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:50,074] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 165 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:50,076] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 166 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:50,078] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50494-122 (kafka.network.Processor)
[2019-04-19 13:05:50,121] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 166 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:50,127] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 167 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:50,149] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 167 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:53,181] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 167 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:53,181] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 168 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:59,535] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 168 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:59,538] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 169 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:05:59,542] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 169 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:02,118] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 13:06:02,660] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 169 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:02,661] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 170 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:02,670] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50516-125 (kafka.network.Processor)
[2019-04-19 13:06:02,735] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 170 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:02,737] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 171 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:02,744] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 171 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:05,835] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 171 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:05,835] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 172 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:05,838] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50520-126 (kafka.network.Processor)
[2019-04-19 13:06:05,895] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 172 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:05,900] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 173 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:05,913] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 173 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:07,682] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:06:07,688] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:06:07,688] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:06:07,688] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:06:09,001] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 173 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:09,005] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 174 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:09,013] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50524-126 (kafka.network.Processor)
[2019-04-19 13:06:15,508] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 174 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:15,513] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 175 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:15,527] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 175 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:18,626] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 175 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:18,626] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 176 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:18,629] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50542-129 (kafka.network.Processor)
[2019-04-19 13:06:18,681] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 176 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:18,687] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 177 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:18,696] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 177 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:21,740] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 177 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:21,741] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 178 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:21,742] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50546-130 (kafka.network.Processor)
[2019-04-19 13:06:21,817] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 178 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:21,826] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 179 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:21,833] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 179 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:24,930] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 179 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:24,931] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 180 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:24,932] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50550-130 (kafka.network.Processor)
[2019-04-19 13:06:31,570] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 180 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:31,575] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 181 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:31,580] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 181 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:34,673] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 181 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:34,674] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 182 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:34,678] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50568-133 (kafka.network.Processor)
[2019-04-19 13:06:34,738] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 182 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:34,740] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 183 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:34,745] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 183 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:37,802] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 183 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:37,803] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 184 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:37,813] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50572-134 (kafka.network.Processor)
[2019-04-19 13:06:37,884] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 184 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:37,885] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 185 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:37,890] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 185 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:40,964] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 185 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:40,965] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 186 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:40,967] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50576-134 (kafka.network.Processor)
[2019-04-19 13:06:42,076] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:06:47,440] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 186 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:47,451] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 187 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:47,461] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 187 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:50,840] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 187 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:50,841] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 188 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:50,842] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50594-137 (kafka.network.Processor)
[2019-04-19 13:06:50,889] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 188 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:50,898] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 189 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:50,912] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 189 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:53,984] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 189 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:53,986] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 190 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:53,990] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50598-138 (kafka.network.Processor)
[2019-04-19 13:06:54,053] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 190 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:54,055] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 191 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:54,066] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 191 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:57,153] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 191 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:57,154] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 192 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:06:57,159] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50602-138 (kafka.network.Processor)
[2019-04-19 13:07:03,500] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 192 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:03,502] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 193 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:03,507] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 193 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:06,645] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 193 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:06,646] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 194 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:06,659] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50620-141 (kafka.network.Processor)
[2019-04-19 13:07:06,705] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 194 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:06,716] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 195 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:06,724] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 195 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:09,921] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 195 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:09,922] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 196 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:09,925] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50624-142 (kafka.network.Processor)
[2019-04-19 13:07:09,959] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 196 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:09,961] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 197 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:09,967] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 197 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:13,058] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 197 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:13,060] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 198 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:13,062] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50628-142 (kafka.network.Processor)
[2019-04-19 13:07:19,558] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 198 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:19,562] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 199 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:19,570] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 199 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:22,600] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 199 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:22,601] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 200 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:22,602] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50646-145 (kafka.network.Processor)
[2019-04-19 13:07:22,659] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 200 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:22,665] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 201 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:22,681] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 201 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:25,732] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 201 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:25,737] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 202 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:25,742] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50650-146 (kafka.network.Processor)
[2019-04-19 13:07:25,791] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 202 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:25,792] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 203 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:25,815] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 203 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:28,919] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 203 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:28,920] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 204 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:28,923] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50654-146 (kafka.network.Processor)
[2019-04-19 13:07:35,737] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 204 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:35,761] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 205 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:35,785] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 205 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:38,987] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 205 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:38,988] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 206 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:38,991] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50672-149 (kafka.network.Processor)
[2019-04-19 13:07:39,025] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 206 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:39,028] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 207 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:39,034] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 207 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:42,036] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 207 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:42,038] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 208 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:42,043] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50676-150 (kafka.network.Processor)
[2019-04-19 13:07:42,071] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 208 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:42,073] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 209 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:42,083] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 209 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:45,155] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 209 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:45,156] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 210 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:45,164] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50680-150 (kafka.network.Processor)
[2019-04-19 13:07:52,273] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 210 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:52,291] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 211 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:52,308] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 211 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:55,445] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 211 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:55,446] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 212 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:55,448] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50698-153 (kafka.network.Processor)
[2019-04-19 13:07:55,496] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 212 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:55,500] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 213 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:55,512] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 213 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:58,595] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 213 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:58,596] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 214 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:58,608] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50702-154 (kafka.network.Processor)
[2019-04-19 13:07:58,673] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 214 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:58,679] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 215 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:07:58,683] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 215 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:01,749] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 215 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:01,750] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 216 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:01,759] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50706-154 (kafka.network.Processor)
[2019-04-19 13:08:02,685] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:02,771] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_two-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:02,803] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_two-4. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:02,893] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-4. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:02,912] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-5. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:02,927] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-7. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:02,939] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-9. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:08,085] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 216 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:08,094] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 217 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:08,103] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 217 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:11,502] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 217 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:11,503] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 218 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:11,548] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 218 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:11,550] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 219 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:11,562] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 219 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:14,851] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 219 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:14,851] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 220 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:14,861] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50728-158 (kafka.network.Processor)
[2019-04-19 13:08:14,935] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 220 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:14,942] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 221 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:14,949] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 221 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:18,257] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 221 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:18,259] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 222 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:18,266] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50732-158 (kafka.network.Processor)
[2019-04-19 13:08:24,497] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 222 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:24,503] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 223 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:24,519] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 223 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:27,550] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 223 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:27,550] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 224 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:27,635] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 224 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:27,636] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 225 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:27,645] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 225 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:30,692] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 225 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:30,692] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 226 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:30,697] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50754-162 (kafka.network.Processor)
[2019-04-19 13:08:30,737] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 226 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:30,740] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 227 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:30,773] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 227 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:33,928] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 227 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:33,930] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 228 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:33,935] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50758-162 (kafka.network.Processor)
[2019-04-19 13:08:40,263] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 228 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:40,268] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 229 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:40,279] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 229 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:43,372] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 229 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:43,375] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 230 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:43,378] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50776-165 (kafka.network.Processor)
[2019-04-19 13:08:43,442] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 230 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:43,447] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 231 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:43,462] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 231 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:46,592] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 231 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:46,608] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 232 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:46,619] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50780-166 (kafka.network.Processor)
[2019-04-19 13:08:46,672] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 232 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:46,675] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 233 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:46,681] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 233 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:49,751] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 233 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:49,753] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 234 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:49,754] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50784-166 (kafka.network.Processor)
[2019-04-19 13:08:50,836] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_two-9. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:50,947] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-2. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:50,959] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-3. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:50,965] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-6. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:08:56,104] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 234 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:56,115] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 235 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:56,124] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 235 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:59,201] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 235 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:59,201] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 236 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:59,227] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50802-169 (kafka.network.Processor)
[2019-04-19 13:08:59,276] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 236 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:59,278] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 237 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:08:59,284] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 237 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:02,761] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 237 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:02,763] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 238 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:02,792] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50806-170 (kafka.network.Processor)
[2019-04-19 13:09:02,806] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 238 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:02,807] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 239 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:02,813] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 239 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:07,780] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails-12. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:09:12,940] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 239 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:35,958] INFO [GroupCoordinator 1]: Member rdkafka-69766084-969b-4b29-8331-77481353c8f6 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:35,961] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 240 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:09:35,971] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50836-174 (kafka.network.Processor)
[2019-04-19 13:09:35,975] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50832-174 (kafka.network.Processor)
[2019-04-19 13:09:35,977] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50828-173 (kafka.network.Processor)
[2019-04-19 13:10:00,345] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 240 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:05,931] INFO [GroupCoordinator 1]: Member rdkafka-6b7a8003-f954-4bef-86c6-37b4b1fdcb35 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:05,932] INFO [GroupCoordinator 1]: Member rdkafka-b1d13626-b064-4681-b8b7-ffc3657880b9 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:05,933] INFO [GroupCoordinator 1]: Member rdkafka-579796c8-9008-4472-9301-4f74f1b7cd2f in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:05,935] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 241 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:05,943] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50854-177 (kafka.network.Processor)
[2019-04-19 13:10:06,339] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 241 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:06,346] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50858-178 (kafka.network.Processor)
[2019-04-19 13:10:35,905] INFO [GroupCoordinator 1]: Member rdkafka-3700e5d4-e98f-4da0-aebf-f8b8316d4318 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:35,908] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 242 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:10:35,919] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50862-178 (kafka.network.Processor)
[2019-04-19 13:10:35,919] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50884-182 (kafka.network.Processor)
[2019-04-19 13:10:35,919] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50880-181 (kafka.network.Processor)
[2019-04-19 13:10:35,925] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50888-182 (kafka.network.Processor)
[2019-04-19 13:10:42,682] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 242 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:05,877] INFO [GroupCoordinator 1]: Member rdkafka-f33cbb94-c1ff-4530-83a6-5f475a62c127 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:05,878] INFO [GroupCoordinator 1]: Member rdkafka-5fd98262-9457-4217-a938-a30c61efad10 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:05,880] INFO [GroupCoordinator 1]: Member rdkafka-78adfda5-0eef-4e66-89a1-91226e7c2d86 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:05,883] INFO [GroupCoordinator 1]: Member rdkafka-2476122e-e083-4453-b84d-39adccd728dc in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:05,885] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 243 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:05,890] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50910-186 (kafka.network.Processor)
[2019-04-19 13:11:05,890] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50914-186 (kafka.network.Processor)
[2019-04-19 13:11:05,891] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50906-185 (kafka.network.Processor)
[2019-04-19 13:11:07,351] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:11:07,354] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:11:07,355] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:11:07,355] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:11:11,949] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 243 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:35,852] INFO [GroupCoordinator 1]: Member rdkafka-0377c683-b4a4-44b8-ae39-c93715371f86 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:35,855] INFO [GroupCoordinator 1]: Member rdkafka-d49b29f9-1601-4861-975b-0a700432f34b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:35,856] INFO [GroupCoordinator 1]: Member rdkafka-fba6cec8-ee91-4124-b6d3-fdd301fdcc71 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:35,857] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 244 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:11:35,868] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50940-190 (kafka.network.Processor)
[2019-04-19 13:11:35,871] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50932-189 (kafka.network.Processor)
[2019-04-19 13:11:35,871] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50936-190 (kafka.network.Processor)
[2019-04-19 13:12:05,833] INFO [GroupCoordinator 1]: Member rdkafka-1339b2bc-be50-44d0-b780-73a184fbdbbb in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:05,834] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 244 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:05,835] INFO [GroupCoordinator 1]: Member rdkafka-b7e2a2e4-c0a2-41ef-af0c-41bafebc5bca in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:05,835] INFO [GroupCoordinator 1]: Member rdkafka-9cc25bf7-96a5-4792-a341-096155e7077d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:05,835] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 245 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:09,503] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 245 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:09,505] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 246 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:09,510] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 246 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:12,822] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 246 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:12,826] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 247 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:12,827] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50958-193 (kafka.network.Processor)
[2019-04-19 13:12:12,841] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 247 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:12,842] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 248 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:12,846] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 248 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:15,886] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 248 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:15,888] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 249 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:15,893] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50962-194 (kafka.network.Processor)
[2019-04-19 13:12:15,944] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 249 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:15,950] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 250 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:15,971] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 250 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:19,074] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 250 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:12:19,075] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 251 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:03,912] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 251 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:03,916] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 252 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:03,942] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 252 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:06,975] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 252 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:06,975] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 253 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:06,980] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50984-197 (kafka.network.Processor)
[2019-04-19 13:13:07,010] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 253 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:07,015] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 254 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:07,025] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 254 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:10,134] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 254 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:10,134] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 255 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:10,135] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50988-198 (kafka.network.Processor)
[2019-04-19 13:13:10,184] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 255 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:10,190] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 256 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:10,196] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 256 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:13,318] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 256 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:13,320] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 257 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:13,324] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:50992-198 (kafka.network.Processor)
[2019-04-19 13:13:19,854] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 257 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:19,855] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 258 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:19,860] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 258 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:22,966] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 258 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:22,967] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 259 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:22,970] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51010-201 (kafka.network.Processor)
[2019-04-19 13:13:23,023] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 259 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:23,026] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 260 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:23,046] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 260 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:26,115] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 260 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:26,115] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 261 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:26,153] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 261 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:26,160] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 262 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:26,178] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 262 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:29,238] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 262 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:29,238] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 263 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:29,241] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51018-202 (kafka.network.Processor)
[2019-04-19 13:13:35,519] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 263 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:35,521] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 264 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:35,536] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 264 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:38,665] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 264 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:38,665] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 265 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:38,668] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51036-205 (kafka.network.Processor)
[2019-04-19 13:13:38,693] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 265 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:38,696] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 266 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:38,701] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 266 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:41,742] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 266 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:41,758] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 267 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:41,777] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 267 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:41,788] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 268 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:41,796] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 268 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:41,798] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51040-206 (kafka.network.Processor)
[2019-04-19 13:13:44,965] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 268 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:44,966] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 269 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:44,967] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51044-206 (kafka.network.Processor)
[2019-04-19 13:13:51,205] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 269 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:51,209] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 270 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:51,217] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 270 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:13:54,289] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 270 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:24,219] INFO [GroupCoordinator 1]: Member rdkafka-7d7ead97-c699-42e9-bab9-6a230ac5e6fc in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:24,220] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 271 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:24,221] INFO [GroupCoordinator 1]: Member rdkafka-7d7ead97-c699-42e9-bab9-6a230ac5e6fc in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:24,221] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 271 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:24,224] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51066-210 (kafka.network.Processor)
[2019-04-19 13:14:24,225] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51070-210 (kafka.network.Processor)
[2019-04-19 13:14:54,189] INFO [GroupCoordinator 1]: Member rdkafka-3d44e6e3-b273-4f8b-8874-b4300a0df7d1 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:54,190] INFO [GroupCoordinator 1]: Member rdkafka-754a650e-2d34-47a7-94ae-98196612e5ef in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:14:54,191] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 272 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:16:01,443] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 13:16:07,020] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:16:07,023] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:16:07,024] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:16:07,025] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:18:50,367] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 272 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:50,370] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 273 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:50,391] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 273 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:53,468] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 273 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:53,473] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 274 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:53,486] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51088-213 (kafka.network.Processor)
[2019-04-19 13:18:53,547] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 274 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:53,550] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 275 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:53,559] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 275 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:56,675] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 275 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:56,675] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 276 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:56,679] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51092-214 (kafka.network.Processor)
[2019-04-19 13:18:56,723] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 276 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:56,734] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 277 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:56,750] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 277 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:59,798] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 277 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:59,798] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 278 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:18:59,800] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51096-214 (kafka.network.Processor)
[2019-04-19 13:19:15,277] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 278 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:15,285] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 279 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:15,297] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 279 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:18,359] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 279 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:18,360] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 280 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:18,361] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51114-217 (kafka.network.Processor)
[2019-04-19 13:19:18,392] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 280 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:18,401] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 281 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:18,412] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 281 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:21,570] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 281 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:21,570] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 282 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:21,576] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51118-218 (kafka.network.Processor)
[2019-04-19 13:19:21,605] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 282 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:21,611] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 283 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:21,635] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 283 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:24,743] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 283 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:24,745] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 284 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:24,747] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51122-218 (kafka.network.Processor)
[2019-04-19 13:19:31,236] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 284 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:31,240] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 285 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:31,259] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 285 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:34,293] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 285 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:34,294] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 286 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:34,297] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51140-221 (kafka.network.Processor)
[2019-04-19 13:19:34,337] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 286 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:34,341] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 287 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:34,357] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 287 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:37,513] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 287 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:37,514] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 288 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:37,516] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51144-222 (kafka.network.Processor)
[2019-04-19 13:19:37,566] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 288 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:37,570] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 289 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:37,583] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 289 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:40,594] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 289 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:40,595] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 290 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:46,932] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 290 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:46,936] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 291 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:46,949] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 291 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:50,058] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 291 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:50,059] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 292 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:50,060] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51166-225 (kafka.network.Processor)
[2019-04-19 13:19:50,115] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 292 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:50,123] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 293 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:50,130] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 293 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:53,265] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 293 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:53,266] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 294 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:53,269] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51170-226 (kafka.network.Processor)
[2019-04-19 13:19:53,299] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 294 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:53,305] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 295 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:53,321] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 295 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:56,408] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 295 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:56,410] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 296 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:19:56,412] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51174-226 (kafka.network.Processor)
[2019-04-19 13:20:02,902] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 296 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:02,905] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 297 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:02,909] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 297 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:06,024] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 297 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:06,024] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 298 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:06,026] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51192-229 (kafka.network.Processor)
[2019-04-19 13:20:06,069] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 298 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:06,073] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 299 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:06,079] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 299 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:09,077] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 299 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:09,078] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 300 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:09,081] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51196-230 (kafka.network.Processor)
[2019-04-19 13:20:09,102] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 300 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:09,109] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 301 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:09,127] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 301 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:12,216] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 301 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:12,216] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 302 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:12,218] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51200-230 (kafka.network.Processor)
[2019-04-19 13:20:18,768] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 302 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:18,775] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 303 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:18,795] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 303 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:21,854] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 303 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:21,854] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 304 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:21,863] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51218-233 (kafka.network.Processor)
[2019-04-19 13:20:21,913] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 304 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:21,917] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 305 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:21,930] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 305 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:25,054] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 305 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:25,054] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 306 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:25,056] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51222-234 (kafka.network.Processor)
[2019-04-19 13:20:25,092] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 306 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:25,095] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 307 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:25,112] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 307 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:28,182] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 307 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:28,182] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 308 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:28,188] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51226-234 (kafka.network.Processor)
[2019-04-19 13:20:34,793] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 308 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:34,797] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 309 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:34,803] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 309 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:37,993] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 309 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:37,994] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 310 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:37,997] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51244-237 (kafka.network.Processor)
[2019-04-19 13:20:38,057] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 310 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:38,060] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 311 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:38,071] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 311 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:41,138] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 311 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:41,138] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 312 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:41,142] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51248-238 (kafka.network.Processor)
[2019-04-19 13:20:41,173] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 312 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:41,181] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 313 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:41,193] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 313 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:44,341] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 313 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:44,342] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 314 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:44,346] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51252-238 (kafka.network.Processor)
[2019-04-19 13:20:50,649] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 314 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:50,653] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 315 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:50,676] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 315 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:53,701] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 315 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:53,702] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 316 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:53,703] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51270-241 (kafka.network.Processor)
[2019-04-19 13:20:53,729] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 316 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:53,730] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 317 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:53,753] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 317 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:56,858] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 317 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:56,858] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 318 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:56,859] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51274-242 (kafka.network.Processor)
[2019-04-19 13:20:56,891] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 318 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:56,895] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 319 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:20:56,909] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 319 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:00,012] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 319 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:00,012] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 320 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:00,017] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51278-242 (kafka.network.Processor)
[2019-04-19 13:21:06,211] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 320 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:06,213] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 321 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:06,226] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 321 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:06,690] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:21:06,692] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:21:06,693] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:21:06,693] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:21:09,216] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 321 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:09,218] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 322 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:09,222] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51296-245 (kafka.network.Processor)
[2019-04-19 13:21:09,236] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 322 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:09,238] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 323 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:09,244] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 323 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:12,371] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 323 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:12,371] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 324 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:12,375] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51300-246 (kafka.network.Processor)
[2019-04-19 13:21:12,413] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 324 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:12,414] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 325 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:12,428] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 325 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:15,551] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 325 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:15,551] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 326 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:15,552] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51304-246 (kafka.network.Processor)
[2019-04-19 13:21:21,874] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 326 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:21,877] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 327 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:21,885] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 327 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:25,026] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 327 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:25,027] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 328 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:25,105] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 328 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:25,109] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 329 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:25,123] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 329 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:28,208] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 329 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:28,208] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 330 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:28,211] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51326-250 (kafka.network.Processor)
[2019-04-19 13:21:28,244] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 330 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:28,253] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 331 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:28,261] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 331 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:31,312] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 331 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:31,312] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 332 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:31,314] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51330-250 (kafka.network.Processor)
[2019-04-19 13:21:37,881] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 332 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:37,886] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 333 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:37,898] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 333 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:40,895] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 333 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:40,896] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 334 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:40,897] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51348-253 (kafka.network.Processor)
[2019-04-19 13:21:40,920] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 334 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:40,927] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 335 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:40,937] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 335 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:44,063] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 335 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:44,063] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 336 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:44,072] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51352-254 (kafka.network.Processor)
[2019-04-19 13:21:44,112] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 336 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:44,117] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 337 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:44,126] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 337 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:47,250] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 337 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:47,251] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 338 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:47,255] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51356-254 (kafka.network.Processor)
[2019-04-19 13:21:53,451] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 338 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:53,458] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 339 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:53,491] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 339 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:56,587] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 339 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:56,587] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 340 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:56,590] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51374-257 (kafka.network.Processor)
[2019-04-19 13:21:56,622] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 340 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:56,628] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 341 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:56,647] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 341 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:59,691] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 341 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:59,692] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 342 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:59,697] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51378-258 (kafka.network.Processor)
[2019-04-19 13:21:59,717] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 342 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:59,722] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 343 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:21:59,731] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 343 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:02,796] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 343 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:02,797] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 344 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:02,799] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51382-258 (kafka.network.Processor)
[2019-04-19 13:22:08,916] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 344 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:08,918] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 345 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:08,924] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 345 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:11,982] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 345 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:11,982] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 346 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:11,984] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51400-261 (kafka.network.Processor)
[2019-04-19 13:22:12,033] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 346 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:12,036] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 347 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:12,052] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 347 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:15,196] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 347 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:15,196] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 348 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:15,200] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51404-262 (kafka.network.Processor)
[2019-04-19 13:22:15,245] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 348 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:15,250] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 349 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:15,261] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 349 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:18,354] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 349 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:18,355] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 350 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:18,359] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51408-262 (kafka.network.Processor)
[2019-04-19 13:22:24,618] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 350 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:24,629] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 351 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:24,641] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 351 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:27,776] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 351 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:57,705] INFO [GroupCoordinator 1]: Member rdkafka-5cc9e467-6a87-4a63-b2a1-ed305172290b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:57,708] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 352 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:22:57,716] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51430-266 (kafka.network.Processor)
[2019-04-19 13:22:57,719] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51456-270 (kafka.network.Processor)
[2019-04-19 13:22:57,722] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51452-269 (kafka.network.Processor)
[2019-04-19 13:22:57,724] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51460-270 (kafka.network.Processor)
[2019-04-19 13:22:57,725] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51434-266 (kafka.network.Processor)
[2019-04-19 13:22:58,393] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 352 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,676] INFO [GroupCoordinator 1]: Member rdkafka-bbed1a82-030f-447a-bcb8-8db014497d55 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,676] INFO [GroupCoordinator 1]: Member rdkafka-21519e82-b869-49ff-82e1-709686159327 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,676] INFO [GroupCoordinator 1]: Member rdkafka-46e81f14-4c01-49a4-995d-096bc38f8b97 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,676] INFO [GroupCoordinator 1]: Member rdkafka-30ebbc6f-a871-42f8-ada9-4b1a6b046aee in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,678] INFO [GroupCoordinator 1]: Member rdkafka-1cf69e9f-67a1-489c-bd89-a58cad7a9f60 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,679] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 353 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:27,684] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51504-277 (kafka.network.Processor)
[2019-04-19 13:23:27,684] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51478-273 (kafka.network.Processor)
[2019-04-19 13:23:27,688] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51486-274 (kafka.network.Processor)
[2019-04-19 13:23:27,688] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51512-278 (kafka.network.Processor)
[2019-04-19 13:23:27,694] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51508-278 (kafka.network.Processor)
[2019-04-19 13:23:27,695] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51482-274 (kafka.network.Processor)
[2019-04-19 13:23:31,230] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 353 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,646] INFO [GroupCoordinator 1]: Member rdkafka-560c460c-6727-4d32-b70f-29b77dd707f6 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,647] INFO [GroupCoordinator 1]: Member rdkafka-472c4c6b-c1e0-40c7-b806-6c98d07105ca in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,648] INFO [GroupCoordinator 1]: Member rdkafka-7261ba4f-c974-4a46-8eee-93c299f7b7a0 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,648] INFO [GroupCoordinator 1]: Member rdkafka-14c844ad-9da2-42b1-91f7-d45f0471fcb9 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,648] INFO [GroupCoordinator 1]: Member rdkafka-012e7a36-919e-4a6c-bb89-5f5a7f622784 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,648] INFO [GroupCoordinator 1]: Member rdkafka-26618606-7d0b-4afd-bbb5-5d0e91120c12 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,649] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 354 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:23:57,658] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51560-286 (kafka.network.Processor)
[2019-04-19 13:23:57,659] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51534-282 (kafka.network.Processor)
[2019-04-19 13:23:57,665] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51538-282 (kafka.network.Processor)
[2019-04-19 13:23:57,666] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51564-286 (kafka.network.Processor)
[2019-04-19 13:23:57,667] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51530-281 (kafka.network.Processor)
[2019-04-19 13:23:57,671] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51556-285 (kafka.network.Processor)
[2019-04-19 13:24:05,188] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 354 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,621] INFO [GroupCoordinator 1]: Member rdkafka-4223710e-e822-472b-85da-a06807d2a7d0 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,621] INFO [GroupCoordinator 1]: Member rdkafka-a8cb143a-2b5a-433b-ba87-f7d8a8ba4f91 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,621] INFO [GroupCoordinator 1]: Member rdkafka-867ce8b7-e6e0-4dd4-b741-c8d2602d801c in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,621] INFO [GroupCoordinator 1]: Member rdkafka-baf8cb1c-2f93-4aaf-909e-ffbae640672e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,621] INFO [GroupCoordinator 1]: Member rdkafka-e099a2f0-c381-4236-add4-83ef12a29790 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,622] INFO [GroupCoordinator 1]: Member rdkafka-31b59e3e-7b4b-4004-b32f-3da436996e4b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,624] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 355 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:27,627] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51590-290 (kafka.network.Processor)
[2019-04-19 13:24:27,630] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51586-290 (kafka.network.Processor)
[2019-04-19 13:24:27,636] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51582-289 (kafka.network.Processor)
[2019-04-19 13:24:29,142] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 355 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:57,594] INFO [GroupCoordinator 1]: Member rdkafka-475d100c-6757-4347-a230-be6aee9cba8e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:57,594] INFO [GroupCoordinator 1]: Member rdkafka-f557559e-ebae-4d2c-a9a8-f4015cd7fa68 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:57,595] INFO [GroupCoordinator 1]: Member rdkafka-0490c733-5cbb-4dc0-b62d-e99cc19a6819 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:57,596] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 356 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:57,603] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51612-294 (kafka.network.Processor)
[2019-04-19 13:24:57,609] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51634-297 (kafka.network.Processor)
[2019-04-19 13:24:57,610] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51616-294 (kafka.network.Processor)
[2019-04-19 13:24:57,610] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 356 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:24:57,612] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51608-293 (kafka.network.Processor)
[2019-04-19 13:25:00,740] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 356 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:00,741] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51638-298 (kafka.network.Processor)
[2019-04-19 13:25:27,566] INFO [GroupCoordinator 1]: Member rdkafka-5d1dc7d3-705b-4f3e-98a0-4314228d3205 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:27,566] INFO [GroupCoordinator 1]: Member rdkafka-b5acfa25-58e3-44b5-8913-9a3c24013572 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:27,566] INFO [GroupCoordinator 1]: Member rdkafka-3e73b3d9-68e2-4f39-8d34-7d7e336c2f45 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:27,566] INFO [GroupCoordinator 1]: Member rdkafka-f62f3fa6-ff18-4b5f-b555-7a3ef37eaf2e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:27,568] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 357 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:27,580] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51668-302 (kafka.network.Processor)
[2019-04-19 13:25:27,581] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51664-302 (kafka.network.Processor)
[2019-04-19 13:25:27,582] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51660-301 (kafka.network.Processor)
[2019-04-19 13:25:27,585] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51642-298 (kafka.network.Processor)
[2019-04-19 13:25:35,385] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 357 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:57,537] INFO [GroupCoordinator 1]: Member rdkafka-8d1c6f16-8ffb-4e78-abed-e6f6c03d3594 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:57,540] INFO [GroupCoordinator 1]: Member rdkafka-eca551a6-78fb-4c49-a70c-7583d2209ed1 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:57,540] INFO [GroupCoordinator 1]: Member rdkafka-761338a9-2cf6-46d4-87a3-dd1667f9613a in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:57,545] INFO [GroupCoordinator 1]: Member rdkafka-998470fc-5f3e-4be2-96b3-3bdbd21af044 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:57,546] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 358 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:57,554] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51694-306 (kafka.network.Processor)
[2019-04-19 13:25:57,555] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51686-305 (kafka.network.Processor)
[2019-04-19 13:25:57,559] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51712-309 (kafka.network.Processor)
[2019-04-19 13:25:57,559] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51690-306 (kafka.network.Processor)
[2019-04-19 13:25:58,750] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 358 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:25:58,752] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51716-310 (kafka.network.Processor)
[2019-04-19 13:26:00,766] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 13:26:06,358] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:26:06,360] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:26:06,361] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:26:06,361] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:26:27,516] INFO [GroupCoordinator 1]: Member rdkafka-b9b600d3-9296-4f3b-91d8-a9051e90a782 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:27,520] INFO [GroupCoordinator 1]: Member rdkafka-8e5964f7-ac08-4215-8a0f-a7d7c8c3a704 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:27,520] INFO [GroupCoordinator 1]: Member rdkafka-1cb3ae4a-d80f-46e8-a5a5-960164d7bad9 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:27,520] INFO [GroupCoordinator 1]: Member rdkafka-a41b691b-4ac3-4674-9875-cc73cebac460 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:27,523] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 359 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:27,527] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51720-310 (kafka.network.Processor)
[2019-04-19 13:26:27,537] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51742-314 (kafka.network.Processor)
[2019-04-19 13:26:27,538] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51746-314 (kafka.network.Processor)
[2019-04-19 13:26:27,542] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51738-313 (kafka.network.Processor)
[2019-04-19 13:26:52,943] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 359 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:57,490] INFO [GroupCoordinator 1]: Member rdkafka-ff2c67ed-5a98-4e08-bbe3-e13e508476eb in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:57,491] INFO [GroupCoordinator 1]: Member rdkafka-58eddb12-0714-4673-acea-a5775c08f213 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:57,492] INFO [GroupCoordinator 1]: Member rdkafka-916bf6df-b40b-4279-af76-fd79162f0679 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:57,493] INFO [GroupCoordinator 1]: Member rdkafka-6fdf52ca-a63d-4dc7-967c-5ab91343d01c in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:57,495] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 360 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:57,499] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51764-317 (kafka.network.Processor)
[2019-04-19 13:26:58,944] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 360 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:26:58,947] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51768-318 (kafka.network.Processor)
[2019-04-19 13:27:27,463] INFO [GroupCoordinator 1]: Member rdkafka-a0485078-2665-42bd-9087-48052a10f05b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:27:27,465] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 361 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:27:27,469] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51772-318 (kafka.network.Processor)
[2019-04-19 13:27:33,094] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 361 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:27:57,435] INFO [GroupCoordinator 1]: Member rdkafka-191d2229-2094-41b1-8d55-b55f063ee9af in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:27:57,437] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 362 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:27:57,442] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51798-322 (kafka.network.Processor)
[2019-04-19 13:27:57,442] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51816-325 (kafka.network.Processor)
[2019-04-19 13:27:57,443] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51790-321 (kafka.network.Processor)
[2019-04-19 13:27:57,454] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51794-322 (kafka.network.Processor)
[2019-04-19 13:27:57,455] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51820-326 (kafka.network.Processor)
[2019-04-19 13:28:03,711] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 362 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:03,715] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51824-326 (kafka.network.Processor)
[2019-04-19 13:28:27,407] INFO [GroupCoordinator 1]: Member rdkafka-5e003dba-0b6e-46bb-80c5-9ad73675707e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:27,408] INFO [GroupCoordinator 1]: Member rdkafka-61798303-fed2-478c-8af2-01a8e39a8ce7 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:27,412] INFO [GroupCoordinator 1]: Member rdkafka-4bb095fc-afb4-4fb1-981b-db8ce23f0af2 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:27,412] INFO [GroupCoordinator 1]: Member rdkafka-b239ad44-67b3-4674-8052-92a1e2e44897 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:27,413] INFO [GroupCoordinator 1]: Member rdkafka-595e4b0c-02f3-4326-a4cb-8add0cd7d765 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:27,415] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 363 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:27,419] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51872-334 (kafka.network.Processor)
[2019-04-19 13:28:27,422] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51846-330 (kafka.network.Processor)
[2019-04-19 13:28:27,426] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51850-330 (kafka.network.Processor)
[2019-04-19 13:28:27,427] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51868-333 (kafka.network.Processor)
[2019-04-19 13:28:27,428] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51842-329 (kafka.network.Processor)
[2019-04-19 13:28:27,430] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 363 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:30,562] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 363 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:30,563] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51876-334 (kafka.network.Processor)
[2019-04-19 13:28:57,383] INFO [GroupCoordinator 1]: Member rdkafka-1170f709-7606-414f-ab5f-d09ec296bce4 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:57,383] INFO [GroupCoordinator 1]: Member rdkafka-16092472-5995-4221-9c11-61aba01e82c6 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:57,385] INFO [GroupCoordinator 1]: Member rdkafka-b71bca58-69b7-47fe-aebb-e765c42e69df in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:57,385] INFO [GroupCoordinator 1]: Member rdkafka-540ba115-dd5c-40d3-9470-3145d6103fb6 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:57,385] INFO [GroupCoordinator 1]: Member rdkafka-e3a22822-188a-46f6-8b8d-70bf4ea27bfa in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:57,386] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 364 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:57,390] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51898-338 (kafka.network.Processor)
[2019-04-19 13:28:57,393] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51902-338 (kafka.network.Processor)
[2019-04-19 13:28:57,394] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51894-337 (kafka.network.Processor)
[2019-04-19 13:28:57,394] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51920-341 (kafka.network.Processor)
[2019-04-19 13:28:58,372] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 364 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:28:58,379] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51924-342 (kafka.network.Processor)
[2019-04-19 13:29:27,355] INFO [GroupCoordinator 1]: Member rdkafka-4f2707d0-57a4-42f2-863b-7d87986a8ae2 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:27,355] INFO [GroupCoordinator 1]: Member rdkafka-b8f1fa95-1bbf-422e-b3cf-a8a6108691bc in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:27,355] INFO [GroupCoordinator 1]: Member rdkafka-fc3d8d69-4ff0-4b75-8901-ced25677e454 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:27,355] INFO [GroupCoordinator 1]: Member rdkafka-12dcb56f-8454-467a-8658-80a8fa962e96 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:27,357] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 365 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:27,360] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51950-346 (kafka.network.Processor)
[2019-04-19 13:29:27,368] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51946-345 (kafka.network.Processor)
[2019-04-19 13:29:27,371] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51972-349 (kafka.network.Processor)
[2019-04-19 13:29:27,374] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51954-346 (kafka.network.Processor)
[2019-04-19 13:29:27,376] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51928-342 (kafka.network.Processor)
[2019-04-19 13:29:27,380] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 365 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:30,470] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 365 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:30,474] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51976-350 (kafka.network.Processor)
[2019-04-19 13:29:57,326] INFO [GroupCoordinator 1]: Member rdkafka-a4fe27e2-cb6d-4284-a7d7-833ca232d38a in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,327] INFO [GroupCoordinator 1]: Member rdkafka-e4815e92-2ecf-4423-bb66-c9249c929ff3 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,328] INFO [GroupCoordinator 1]: Member rdkafka-c60a8523-6d37-4f8a-819a-587726415947 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,330] INFO [GroupCoordinator 1]: Member rdkafka-2b303b94-9f01-4a83-afea-91ac5e33038e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,330] INFO [GroupCoordinator 1]: Member rdkafka-c1c887bb-a5dd-467a-bff3-b981b7849e29 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,335] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 366 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,341] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51998-353 (kafka.network.Processor)
[2019-04-19 13:29:57,342] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:51980-350 (kafka.network.Processor)
[2019-04-19 13:29:57,345] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52002-354 (kafka.network.Processor)
[2019-04-19 13:29:57,926] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 366 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:29:57,940] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52020-356 (kafka.network.Processor)
[2019-04-19 13:30:27,303] INFO [GroupCoordinator 1]: Member rdkafka-b9db39c7-75fc-4b80-80c6-c4f8f4bc1466 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:27,310] INFO [GroupCoordinator 1]: Member rdkafka-e80a72d0-6cee-4c08-9c9e-78ece01b7991 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:27,314] INFO [GroupCoordinator 1]: Member rdkafka-51fa21fd-4bc5-4a42-8259-9013c1310ee9 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:27,316] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 367 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:27,329] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52046-360 (kafka.network.Processor)
[2019-04-19 13:30:27,332] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52054-362 (kafka.network.Processor)
[2019-04-19 13:30:27,333] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52028-358 (kafka.network.Processor)
[2019-04-19 13:30:27,334] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52050-361 (kafka.network.Processor)
[2019-04-19 13:30:27,335] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52024-357 (kafka.network.Processor)
[2019-04-19 13:30:28,167] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 367 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:28,172] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52072-364 (kafka.network.Processor)
[2019-04-19 13:30:57,283] INFO [GroupCoordinator 1]: Member rdkafka-6570e3c8-fa71-4db3-914b-5845c4bf90dd in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:57,291] INFO [GroupCoordinator 1]: Member rdkafka-834c76a1-1bc1-491c-96ea-2bfc520b262e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:57,292] INFO [GroupCoordinator 1]: Member rdkafka-8506589e-66f5-410a-8aed-5d32e9d9f7e8 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:57,293] INFO [GroupCoordinator 1]: Member rdkafka-3b15f076-1aad-4853-9465-4c80b78c506c in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:57,303] INFO [GroupCoordinator 1]: Member rdkafka-00411382-f96e-4ffc-b6cd-29850696fc55 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:57,312] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 368 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:57,359] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52102-369 (kafka.network.Processor)
[2019-04-19 13:30:57,361] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52098-368 (kafka.network.Processor)
[2019-04-19 13:30:57,362] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52076-365 (kafka.network.Processor)
[2019-04-19 13:30:57,387] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52080-366 (kafka.network.Processor)
[2019-04-19 13:30:57,395] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52106-370 (kafka.network.Processor)
[2019-04-19 13:30:58,401] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 368 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:30:58,404] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52124-372 (kafka.network.Processor)
[2019-04-19 13:31:06,024] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:31:06,025] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:31:06,026] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:31:06,026] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:31:27,292] INFO [GroupCoordinator 1]: Member rdkafka-41a2ad2b-d331-4518-b3cc-339ba30e3ad4 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:27,294] INFO [GroupCoordinator 1]: Member rdkafka-1cd7b4f5-1795-4d30-a951-7e31a28cff65 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:27,296] INFO [GroupCoordinator 1]: Member rdkafka-5f4a0303-5d61-4493-8af5-744524a4257a in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:27,310] INFO [GroupCoordinator 1]: Member rdkafka-44e70bc4-73dc-48b4-b954-3d861fff6fa8 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:27,325] INFO [GroupCoordinator 1]: Member rdkafka-e3763be9-18a4-49c3-a96e-8c8e7ea254b7 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:27,327] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 369 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:27,331] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52132-374 (kafka.network.Processor)
[2019-04-19 13:31:27,332] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52158-378 (kafka.network.Processor)
[2019-04-19 13:31:27,332] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52128-373 (kafka.network.Processor)
[2019-04-19 13:31:27,332] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52154-377 (kafka.network.Processor)
[2019-04-19 13:31:27,334] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52150-376 (kafka.network.Processor)
[2019-04-19 13:31:27,339] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 369 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:30,473] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 369 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:30,475] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52176-380 (kafka.network.Processor)
[2019-04-19 13:31:57,296] INFO [GroupCoordinator 1]: Member rdkafka-42c14702-a938-4fbf-ace7-d3734cad522b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:57,296] INFO [GroupCoordinator 1]: Member rdkafka-bb61dcea-1757-4441-b4db-ec3c4ba8a630 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:57,296] INFO [GroupCoordinator 1]: Member rdkafka-27d249d1-f28e-47b2-af5e-f1075476ac0b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:57,296] INFO [GroupCoordinator 1]: Member rdkafka-cddd8774-55d3-42f5-b2b8-1fa28a0e0990 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:57,297] INFO [GroupCoordinator 1]: Member rdkafka-ced10140-e611-4db4-90be-56b17dfc25dd in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:57,299] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 370 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:31:57,303] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52180-381 (kafka.network.Processor)
[2019-04-19 13:31:57,304] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52202-384 (kafka.network.Processor)
[2019-04-19 13:31:57,308] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52210-386 (kafka.network.Processor)
[2019-04-19 13:31:57,310] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52184-382 (kafka.network.Processor)
[2019-04-19 13:31:57,311] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52206-385 (kafka.network.Processor)
[2019-04-19 13:32:02,338] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 370 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,268] INFO [GroupCoordinator 1]: Member rdkafka-5ab88006-ec4f-40be-b6dc-0c6277ce1a91 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,271] INFO [GroupCoordinator 1]: Member rdkafka-d1cbb792-9607-42ac-888b-8ebdff1c489d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,274] INFO [GroupCoordinator 1]: Member rdkafka-1cfd548e-5c71-4454-aeac-d8a84a71a985 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,275] INFO [GroupCoordinator 1]: Member rdkafka-b4339f50-48a2-4236-b22d-1ad50e3ab59f in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,275] INFO [GroupCoordinator 1]: Member rdkafka-d43d9241-b8b6-4571-938e-d83639a47ef1 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,287] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 371 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:27,317] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52236-390 (kafka.network.Processor)
[2019-04-19 13:32:27,317] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52254-392 (kafka.network.Processor)
[2019-04-19 13:32:27,320] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52258-393 (kafka.network.Processor)
[2019-04-19 13:32:27,320] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52228-388 (kafka.network.Processor)
[2019-04-19 13:32:27,323] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52232-389 (kafka.network.Processor)
[2019-04-19 13:32:38,689] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 371 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:38,693] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52262-394 (kafka.network.Processor)
[2019-04-19 13:32:57,256] INFO [GroupCoordinator 1]: Member rdkafka-add7f241-5d73-48a8-9a58-53e5cb5da14a in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:57,272] INFO [GroupCoordinator 1]: Member rdkafka-99cb3cbe-b3b8-4d86-a83e-519224b96010 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:57,276] INFO [GroupCoordinator 1]: Member rdkafka-02a4b9db-eb7f-4b88-b6da-b263253f1917 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:57,282] INFO [GroupCoordinator 1]: Member rdkafka-b9633534-c092-4b87-a7a7-7b47f856ad9d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:57,282] INFO [GroupCoordinator 1]: Member rdkafka-3b02a999-8401-408c-a63e-7bf5ca5e6456 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:57,283] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 372 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:57,287] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52294-398 (kafka.network.Processor)
[2019-04-19 13:32:57,292] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52320-402 (kafka.network.Processor)
[2019-04-19 13:32:57,293] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52302-400 (kafka.network.Processor)
[2019-04-19 13:32:57,293] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52298-399 (kafka.network.Processor)
[2019-04-19 13:32:59,941] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 372 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:32:59,943] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52324-403 (kafka.network.Processor)
[2019-04-19 13:33:27,251] INFO [GroupCoordinator 1]: Member rdkafka-7da91170-72de-41e0-aead-99a5b3cbd665 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:27,251] INFO [GroupCoordinator 1]: Member rdkafka-a280bbcf-6a39-42bf-9f72-ca90b922e5a0 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:27,252] INFO [GroupCoordinator 1]: Member rdkafka-65d4e200-76eb-463c-be13-39291dc3c855 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:27,252] INFO [GroupCoordinator 1]: Member rdkafka-f9d0a0e4-2383-4d0c-b1dc-46008115136b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:27,254] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 373 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:27,260] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52372-410 (kafka.network.Processor)
[2019-04-19 13:33:27,261] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52346-406 (kafka.network.Processor)
[2019-04-19 13:33:27,265] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52354-408 (kafka.network.Processor)
[2019-04-19 13:33:27,266] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52328-404 (kafka.network.Processor)
[2019-04-19 13:33:27,268] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52350-407 (kafka.network.Processor)
[2019-04-19 13:33:27,281] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 373 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:30,416] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 373 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:30,420] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52376-411 (kafka.network.Processor)
[2019-04-19 13:33:57,222] INFO [GroupCoordinator 1]: Member rdkafka-afa2838c-2bcf-4138-a74b-7b0f65e9dbbb in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,223] INFO [GroupCoordinator 1]: Member rdkafka-4d58210a-6bd5-4faf-9b4f-33254c838938 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,223] INFO [GroupCoordinator 1]: Member rdkafka-18a01afa-4aae-4086-bb33-bb82ee699429 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,224] INFO [GroupCoordinator 1]: Member rdkafka-1d0d9e9e-6374-4c95-be58-a0c806c99a26 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,224] INFO [GroupCoordinator 1]: Member rdkafka-d7f50bb5-ef13-4f00-a22d-5a6936bbc30b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,228] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 374 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,242] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52402-415 (kafka.network.Processor)
[2019-04-19 13:33:57,242] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52380-412 (kafka.network.Processor)
[2019-04-19 13:33:57,242] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52406-416 (kafka.network.Processor)
[2019-04-19 13:33:57,244] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52398-414 (kafka.network.Processor)
[2019-04-19 13:33:57,417] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 374 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:33:57,419] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52424-418 (kafka.network.Processor)
[2019-04-19 13:34:09,831] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 0 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:09,833] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 1 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:09,838] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:09,839] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-15. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:34:13,171] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 1 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:13,172] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 2 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:13,178] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52450-422 (kafka.network.Processor)
[2019-04-19 13:34:13,266] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 2 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:13,272] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 3 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:13,282] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:16,893] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 3 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:16,894] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 4 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:16,898] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52454-423 (kafka.network.Processor)
[2019-04-19 13:34:17,039] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 4 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:17,056] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 5 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:17,116] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:20,613] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 5 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:20,613] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 6 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:20,618] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52458-424 (kafka.network.Processor)
[2019-04-19 13:34:27,200] INFO [GroupCoordinator 1]: Member rdkafka-06cd47b0-2d4c-4f36-b45b-51af35f1f6c6 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,200] INFO [GroupCoordinator 1]: Member rdkafka-7982ae2a-fba8-4dc4-90e6-3dbd0da384ab in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,205] INFO [GroupCoordinator 1]: Member rdkafka-175c8632-87a0-4b0d-9d57-b9c6e6e32a2c in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,209] INFO [GroupCoordinator 1]: Member rdkafka-2e2ff23c-a08f-447e-8e37-046c5240e171 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,211] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 375 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,213] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52432-420 (kafka.network.Processor)
[2019-04-19 13:34:27,213] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52428-419 (kafka.network.Processor)
[2019-04-19 13:34:27,236] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 6 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,239] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 7 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:27,243] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 7 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:30,341] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 7 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:30,341] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 8 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:30,363] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 8 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:30,373] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 9 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:30,409] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 9 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:33,527] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 9 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:33,529] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 10 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:33,530] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52480-427 (kafka.network.Processor)
[2019-04-19 13:34:33,566] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 10 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:33,577] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 11 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:33,594] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 11 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:36,633] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 11 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:36,633] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 12 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:36,634] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52484-428 (kafka.network.Processor)
[2019-04-19 13:34:42,785] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 12 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:42,786] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 13 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:42,790] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 13 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:45,878] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 13 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:45,880] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 14 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:45,882] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52502-430 (kafka.network.Processor)
[2019-04-19 13:34:45,926] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 14 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:45,930] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 15 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:45,938] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 15 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:48,969] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 15 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:48,972] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 16 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:48,975] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52506-431 (kafka.network.Processor)
[2019-04-19 13:34:49,014] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 16 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:49,019] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 17 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:49,027] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 17 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:52,196] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 17 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:52,197] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 18 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:52,201] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52510-432 (kafka.network.Processor)
[2019-04-19 13:34:57,177] INFO [GroupCoordinator 1]: Member rdkafka-aefbdf58-6ef7-44e2-873a-38a41037b96c in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:57,178] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 375 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:57,180] INFO [GroupCoordinator 1]: Member rdkafka-7ae71c10-2314-41f4-850e-7135fa9b4141 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:34:57,180] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 376 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:00,357] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 18 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:00,361] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 19 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:00,381] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 19 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:03,491] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 19 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:03,492] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 20 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:03,528] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52528-434 (kafka.network.Processor)
[2019-04-19 13:35:03,597] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 20 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:03,603] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 21 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:03,628] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 21 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:06,755] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 21 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:06,755] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 22 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:06,757] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52532-435 (kafka.network.Processor)
[2019-04-19 13:35:06,782] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 22 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:06,785] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 23 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:06,792] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 23 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:09,850] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 23 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:09,884] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 24 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:09,895] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52536-436 (kafka.network.Processor)
[2019-04-19 13:35:18,187] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 24 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:18,191] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 25 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:18,198] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 25 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:21,283] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 25 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:21,283] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 26 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:21,288] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52554-438 (kafka.network.Processor)
[2019-04-19 13:35:21,314] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 26 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:21,322] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 27 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:21,330] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 27 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:24,518] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 27 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:24,518] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 28 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:24,521] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52558-439 (kafka.network.Processor)
[2019-04-19 13:35:24,544] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 28 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:24,546] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 29 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:24,561] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 29 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:27,720] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 29 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:27,720] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 30 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:27,747] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52562-440 (kafka.network.Processor)
[2019-04-19 13:35:38,207] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 30 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:38,210] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 31 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:38,218] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 31 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:41,409] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 31 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:41,409] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 32 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:41,413] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52580-442 (kafka.network.Processor)
[2019-04-19 13:35:41,451] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 32 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:41,457] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 33 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:41,472] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 33 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:44,613] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 33 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:44,624] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 34 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:44,646] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 34 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:44,652] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 35 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:44,659] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52584-443 (kafka.network.Processor)
[2019-04-19 13:35:44,666] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 35 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:47,792] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 35 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:47,794] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 36 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:47,797] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52588-444 (kafka.network.Processor)
[2019-04-19 13:35:54,145] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 36 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:54,150] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 37 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:54,156] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 37 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:57,186] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 37 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:57,189] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 38 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:57,193] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52606-446 (kafka.network.Processor)
[2019-04-19 13:35:57,227] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 38 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:57,228] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 39 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:35:57,233] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 39 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:00,090] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 13:36:00,340] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 39 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:00,340] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 40 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:00,343] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52610-447 (kafka.network.Processor)
[2019-04-19 13:36:00,386] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 40 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:00,389] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 41 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:00,407] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 41 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:03,478] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 41 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:03,479] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 42 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:03,480] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52614-448 (kafka.network.Processor)
[2019-04-19 13:36:05,692] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:36:05,694] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:36:05,695] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:36:05,696] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:36:09,963] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 42 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:09,964] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 43 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:09,968] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 43 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:13,073] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 43 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:13,074] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 44 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:13,086] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52632-450 (kafka.network.Processor)
[2019-04-19 13:36:13,094] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 44 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:13,099] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 45 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:13,105] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 45 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:16,182] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 45 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:16,185] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 46 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:16,187] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52636-451 (kafka.network.Processor)
[2019-04-19 13:36:16,202] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 46 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:16,204] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 47 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:16,209] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 47 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:19,297] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 47 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:19,298] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 48 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:19,300] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52640-452 (kafka.network.Processor)
[2019-04-19 13:36:25,628] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 48 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:25,629] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 49 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:25,634] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 49 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:28,751] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 49 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:28,752] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 50 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:28,753] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52658-454 (kafka.network.Processor)
[2019-04-19 13:36:28,776] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 50 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:28,778] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 51 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:28,782] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 51 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:31,901] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 51 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:31,902] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 52 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:31,904] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52662-455 (kafka.network.Processor)
[2019-04-19 13:36:31,922] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 52 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:31,923] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 53 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:31,927] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 53 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:34,994] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 53 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:34,994] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 54 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:34,995] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52666-456 (kafka.network.Processor)
[2019-04-19 13:36:41,296] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 54 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:41,297] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 55 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:41,302] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 55 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:44,380] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 55 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:44,381] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 56 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:44,383] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52684-458 (kafka.network.Processor)
[2019-04-19 13:36:44,405] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 56 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:44,407] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 57 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:44,415] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 57 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:47,452] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 57 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:47,453] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 58 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:47,459] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52688-459 (kafka.network.Processor)
[2019-04-19 13:36:47,476] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 58 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:47,477] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 59 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:47,482] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 59 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:50,589] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 59 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:50,589] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 60 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:50,592] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52692-460 (kafka.network.Processor)
[2019-04-19 13:36:56,803] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 60 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:56,805] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 61 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:56,809] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 61 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:59,845] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 61 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:59,847] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 62 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:59,850] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52710-462 (kafka.network.Processor)
[2019-04-19 13:36:59,871] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 62 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:59,881] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 63 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:36:59,900] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 63 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:02,992] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 63 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:02,993] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 64 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:02,997] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52714-463 (kafka.network.Processor)
[2019-04-19 13:37:03,029] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 64 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:03,037] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 65 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:03,048] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 65 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:06,141] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 65 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:06,141] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 66 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:12,365] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 66 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:12,369] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 67 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:12,373] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 67 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:15,580] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 67 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:15,582] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 68 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:15,585] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52736-466 (kafka.network.Processor)
[2019-04-19 13:37:15,629] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 68 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:15,631] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 69 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:15,640] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 69 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:18,731] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 69 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:18,731] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 70 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:18,732] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52740-467 (kafka.network.Processor)
[2019-04-19 13:37:18,755] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 70 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:18,757] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 71 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:18,763] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 71 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:21,803] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 71 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:21,803] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 72 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:21,816] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52744-468 (kafka.network.Processor)
[2019-04-19 13:37:28,292] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 72 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:28,297] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 73 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:28,302] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 73 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:31,386] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 73 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:31,388] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 74 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:31,392] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52762-470 (kafka.network.Processor)
[2019-04-19 13:37:31,406] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 74 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:31,408] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 75 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:31,412] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 75 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:34,546] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 75 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:34,546] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 76 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:34,549] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52766-471 (kafka.network.Processor)
[2019-04-19 13:37:34,568] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 76 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:34,571] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 77 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:34,575] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 77 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:37,608] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 77 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:37,608] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 78 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:37,609] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52770-472 (kafka.network.Processor)
[2019-04-19 13:37:44,017] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 78 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:44,018] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 79 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:44,022] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 79 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:47,061] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 79 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:47,065] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 80 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:47,066] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52788-474 (kafka.network.Processor)
[2019-04-19 13:37:47,088] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 80 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:47,090] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 81 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:47,095] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 81 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:50,198] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 81 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:50,198] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 82 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:50,202] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52792-475 (kafka.network.Processor)
[2019-04-19 13:37:50,256] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 82 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:50,267] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 83 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:50,290] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 83 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:53,446] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 83 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:53,446] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 84 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:37:53,449] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52796-476 (kafka.network.Processor)
[2019-04-19 13:38:02,132] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 84 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:02,135] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 85 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:02,145] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 85 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:05,252] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 85 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:05,254] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 86 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:05,258] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52814-478 (kafka.network.Processor)
[2019-04-19 13:38:05,291] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 86 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:05,293] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 87 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:05,300] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 87 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:08,381] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 87 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:08,385] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 88 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:08,388] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52818-479 (kafka.network.Processor)
[2019-04-19 13:38:08,446] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 88 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:08,450] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 89 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:08,460] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 89 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:11,582] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 89 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:11,582] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 90 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:11,584] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52822-480 (kafka.network.Processor)
[2019-04-19 13:38:18,228] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 90 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:18,231] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 91 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:18,237] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 91 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:21,371] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 91 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:21,372] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 92 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:21,376] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52840-482 (kafka.network.Processor)
[2019-04-19 13:38:21,423] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 92 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:21,427] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 93 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:21,440] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 93 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:24,473] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 93 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:24,474] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 94 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:24,476] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52844-483 (kafka.network.Processor)
[2019-04-19 13:38:24,521] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 94 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:24,522] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 95 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:24,530] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 95 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:27,697] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 95 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:27,697] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 96 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:27,698] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52848-484 (kafka.network.Processor)
[2019-04-19 13:38:33,862] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 96 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:33,866] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 97 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:33,874] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 97 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:37,049] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 97 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:37,051] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 98 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:37,057] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52866-486 (kafka.network.Processor)
[2019-04-19 13:38:37,107] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 98 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:37,109] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 99 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:37,118] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 99 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:40,203] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 99 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:40,204] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 100 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:40,207] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52870-487 (kafka.network.Processor)
[2019-04-19 13:38:40,267] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 100 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:40,273] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 101 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:40,284] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 101 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:43,389] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 101 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:43,390] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 102 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:43,393] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52874-488 (kafka.network.Processor)
[2019-04-19 13:38:49,659] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 102 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:49,664] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 103 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:49,683] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 103 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:52,759] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 103 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:52,765] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 104 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:52,769] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52892-490 (kafka.network.Processor)
[2019-04-19 13:38:52,787] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 104 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:52,792] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 105 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:52,811] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 105 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:55,951] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 105 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:55,951] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 106 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:55,952] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52896-491 (kafka.network.Processor)
[2019-04-19 13:38:55,994] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 106 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:56,001] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 107 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:56,013] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 107 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:59,115] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 107 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:59,115] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 108 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:38:59,118] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52900-492 (kafka.network.Processor)
[2019-04-19 13:39:05,315] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 108 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:05,323] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 109 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:05,329] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 109 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:08,419] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 109 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:08,421] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 110 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:08,432] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52918-494 (kafka.network.Processor)
[2019-04-19 13:39:08,476] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 110 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:08,478] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 111 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:08,483] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 111 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:11,583] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 111 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:11,584] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 112 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:11,588] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52922-495 (kafka.network.Processor)
[2019-04-19 13:39:11,624] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 112 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:11,629] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 113 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:11,642] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 113 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:14,754] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 113 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:14,755] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 114 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:14,756] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52926-496 (kafka.network.Processor)
[2019-04-19 13:39:21,028] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 114 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:21,033] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 115 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:21,040] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 115 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:24,130] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 115 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:24,131] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 116 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:24,133] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52944-498 (kafka.network.Processor)
[2019-04-19 13:39:24,191] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 116 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:24,200] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 117 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:24,213] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 117 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:27,411] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 117 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:27,411] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 118 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:27,460] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 118 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:27,464] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 119 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:27,480] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 119 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:30,593] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 119 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:30,594] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 120 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:30,595] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52952-500 (kafka.network.Processor)
[2019-04-19 13:39:36,958] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 120 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:36,965] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 121 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:36,975] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 121 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:40,050] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 121 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:40,053] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 122 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:40,057] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52970-502 (kafka.network.Processor)
[2019-04-19 13:39:40,115] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 122 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:40,117] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 123 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:40,131] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 123 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:43,214] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 123 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:43,215] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 124 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:43,250] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 124 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:43,260] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 125 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:43,275] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 125 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:46,372] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 125 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:46,373] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 126 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:46,374] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52978-504 (kafka.network.Processor)
[2019-04-19 13:39:52,609] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 126 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:52,613] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 127 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:52,620] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 127 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:55,718] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 127 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:55,718] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 128 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:55,719] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:52996-506 (kafka.network.Processor)
[2019-04-19 13:39:55,770] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 128 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:55,778] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 129 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:55,792] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 129 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:58,816] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 129 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:58,817] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 130 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:58,818] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53000-507 (kafka.network.Processor)
[2019-04-19 13:39:58,849] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 130 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:58,854] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 131 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:39:58,864] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 131 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:01,911] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 131 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:01,911] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 132 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:01,921] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53004-508 (kafka.network.Processor)
[2019-04-19 13:40:08,136] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 132 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:08,145] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 133 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:08,159] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 133 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:11,206] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 133 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:11,207] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 134 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:11,210] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53022-510 (kafka.network.Processor)
[2019-04-19 13:40:11,248] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 134 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:11,251] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 135 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:11,261] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 135 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:14,330] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 135 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:14,331] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 136 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:14,332] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53026-511 (kafka.network.Processor)
[2019-04-19 13:40:14,398] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 136 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:14,400] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 137 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:14,404] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 137 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:17,506] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 137 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:17,507] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 138 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:17,508] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53030-512 (kafka.network.Processor)
[2019-04-19 13:40:23,727] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 138 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:23,731] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 139 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:23,736] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 139 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:26,819] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 139 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:26,819] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 140 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:26,821] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53048-514 (kafka.network.Processor)
[2019-04-19 13:40:26,875] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 140 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:26,884] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 141 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:26,896] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 141 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:29,992] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 141 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:29,993] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 142 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:29,995] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53052-515 (kafka.network.Processor)
[2019-04-19 13:40:30,028] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 142 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:30,039] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 143 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:30,052] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 143 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:33,142] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 143 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:33,142] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 144 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:33,146] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53056-516 (kafka.network.Processor)
[2019-04-19 13:40:39,322] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 144 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:39,328] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 145 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:39,334] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 145 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:42,472] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 145 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:42,477] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 146 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:42,482] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53074-518 (kafka.network.Processor)
[2019-04-19 13:40:42,508] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 146 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:42,513] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 147 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:42,525] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 147 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:45,680] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 147 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:45,680] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 148 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:45,682] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53078-519 (kafka.network.Processor)
[2019-04-19 13:40:45,742] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 148 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:45,749] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 149 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:45,771] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 149 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:48,853] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 149 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:48,853] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 150 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:48,854] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53082-520 (kafka.network.Processor)
[2019-04-19 13:40:54,876] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 150 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:54,881] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 151 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:54,886] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 151 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:58,005] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 151 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:58,006] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 152 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:58,008] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53100-522 (kafka.network.Processor)
[2019-04-19 13:40:58,025] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 152 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:58,026] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 153 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:40:58,031] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 153 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:01,107] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 153 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:01,107] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 154 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:01,111] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53104-523 (kafka.network.Processor)
[2019-04-19 13:41:01,131] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 154 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:01,132] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 155 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:01,136] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 155 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:04,218] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 155 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:04,221] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 156 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:04,224] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53108-524 (kafka.network.Processor)
[2019-04-19 13:41:05,360] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:41:05,363] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:41:05,363] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:41:05,363] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:41:10,097] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 156 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:10,099] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 157 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:10,103] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 157 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:13,209] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 157 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:13,210] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 158 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:13,223] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53126-526 (kafka.network.Processor)
[2019-04-19 13:41:13,247] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 158 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:13,248] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 159 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:13,253] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 159 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:16,415] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 159 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:16,415] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 160 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:16,419] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53130-527 (kafka.network.Processor)
[2019-04-19 13:41:16,436] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 160 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:16,438] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 161 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:16,443] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 161 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:19,540] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 161 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:19,540] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 162 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:19,548] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53134-528 (kafka.network.Processor)
[2019-04-19 13:41:25,484] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 162 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:25,486] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 163 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:25,491] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 163 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:28,604] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 163 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:28,605] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 164 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:28,610] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53152-530 (kafka.network.Processor)
[2019-04-19 13:41:28,623] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 164 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:28,624] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 165 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:28,634] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 165 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:31,700] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 165 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:31,700] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 166 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:31,705] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53156-531 (kafka.network.Processor)
[2019-04-19 13:41:31,722] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 166 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:31,726] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 167 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:31,732] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 167 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:34,851] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 167 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:34,853] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 168 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:34,855] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53160-532 (kafka.network.Processor)
[2019-04-19 13:41:40,872] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 168 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:40,873] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 169 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:40,877] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 169 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:43,937] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 169 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:43,939] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 170 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:43,959] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53178-534 (kafka.network.Processor)
[2019-04-19 13:41:43,979] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 170 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:43,981] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 171 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:43,987] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 171 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:47,107] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 171 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:47,109] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 172 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:47,116] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53182-535 (kafka.network.Processor)
[2019-04-19 13:41:47,146] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 172 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:47,150] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 173 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:47,155] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 173 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:50,296] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 173 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:50,298] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 174 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:50,302] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53186-536 (kafka.network.Processor)
[2019-04-19 13:41:56,464] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 174 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:56,467] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 175 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:56,472] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 175 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:59,511] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 175 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:59,513] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 176 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:59,515] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53204-538 (kafka.network.Processor)
[2019-04-19 13:41:59,531] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 176 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:59,536] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 177 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:41:59,543] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 177 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:02,602] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 177 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:02,604] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 178 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:02,608] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53208-539 (kafka.network.Processor)
[2019-04-19 13:42:02,638] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 178 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:02,639] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 179 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:02,643] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 179 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:05,777] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 179 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:05,778] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 180 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:05,780] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53212-540 (kafka.network.Processor)
[2019-04-19 13:42:11,872] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 180 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:11,873] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 181 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:11,878] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 181 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:15,020] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 181 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:15,022] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 182 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:15,038] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53230-542 (kafka.network.Processor)
[2019-04-19 13:42:15,055] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 182 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:15,057] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 183 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:15,079] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 183 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:18,165] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 183 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:18,165] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 184 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:18,185] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 184 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:18,187] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-2 generation 185 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:18,191] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-2 for generation 185 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:21,292] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-2 with old generation 185 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:21,292] INFO [GroupCoordinator 1]: Group pyrandall-test-2 with generation 186 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:21,294] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53240-544 (kafka.network.Processor)
[2019-04-19 13:42:27,670] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 376 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:27,677] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 377 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:27,694] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 377 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:30,937] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 377 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:30,938] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 378 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:30,939] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53258-547 (kafka.network.Processor)
[2019-04-19 13:42:30,994] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 378 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:31,001] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 379 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:31,012] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 379 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:34,302] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 379 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:34,302] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 380 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:34,314] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53262-547 (kafka.network.Processor)
[2019-04-19 13:42:34,413] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 380 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:34,448] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 381 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:34,475] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 381 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:37,967] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 381 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:37,972] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 382 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:37,980] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53266-548 (kafka.network.Processor)
[2019-04-19 13:42:44,332] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 382 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:44,339] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 383 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:44,345] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 383 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:47,424] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 383 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:47,434] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 384 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:47,475] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53284-551 (kafka.network.Processor)
[2019-04-19 13:42:47,514] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 384 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:47,519] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 385 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:47,525] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 385 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:50,649] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 385 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:50,651] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 386 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:50,703] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 386 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:50,710] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 387 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:50,723] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 387 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:53,844] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 387 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:53,846] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 388 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:42:53,848] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53292-552 (kafka.network.Processor)
[2019-04-19 13:43:00,090] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 388 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:00,096] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 389 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:00,111] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 389 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:03,223] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 389 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:03,224] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 390 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:03,228] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53310-555 (kafka.network.Processor)
[2019-04-19 13:43:03,290] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 390 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:03,294] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 391 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:03,305] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 391 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:06,354] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 391 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:06,355] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 392 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:06,372] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53314-555 (kafka.network.Processor)
[2019-04-19 13:43:06,428] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 392 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:06,434] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 393 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:06,439] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 393 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:09,536] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 393 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:09,540] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 394 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:09,547] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53318-556 (kafka.network.Processor)
[2019-04-19 13:43:15,794] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 394 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:15,798] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 395 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:15,806] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 395 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:18,944] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 395 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:18,950] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 396 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:18,997] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53336-559 (kafka.network.Processor)
[2019-04-19 13:43:19,018] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 396 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:19,038] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 397 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:19,092] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 397 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:22,419] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 397 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:22,439] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 398 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:22,597] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53340-559 (kafka.network.Processor)
[2019-04-19 13:43:22,757] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 398 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:22,819] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 399 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:22,873] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test for generation 399 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:33,825] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 399 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:56,041] INFO [GroupCoordinator 1]: Member rdkafka-aae7e387-d395-42f5-bd85-0958dc2fb47e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:56,043] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 400 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:43:56,045] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53388-567 (kafka.network.Processor)
[2019-04-19 13:43:56,051] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53366-563 (kafka.network.Processor)
[2019-04-19 13:43:56,053] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53392-567 (kafka.network.Processor)
[2019-04-19 13:43:56,055] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53370-564 (kafka.network.Processor)
[2019-04-19 13:43:56,055] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53362-563 (kafka.network.Processor)
[2019-04-19 13:44:04,940] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 400 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:04,945] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53396-568 (kafka.network.Processor)
[2019-04-19 13:44:26,012] INFO [GroupCoordinator 1]: Member rdkafka-95f15bc0-588b-4b01-9e33-7c2a3629545e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:26,012] INFO [GroupCoordinator 1]: Member rdkafka-ddb399ee-acb2-4f57-863c-d58bbf130749 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:26,012] INFO [GroupCoordinator 1]: Member rdkafka-481581e4-0a8e-4c31-8d64-fa48cc6ec9da in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:26,012] INFO [GroupCoordinator 1]: Member rdkafka-35e837dd-bcda-4c39-9fbe-ebe3337ccfd5 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:26,013] INFO [GroupCoordinator 1]: Member rdkafka-61608e9a-438a-4d21-bede-0c48d67a12e0 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:26,014] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 401 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:26,024] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53422-572 (kafka.network.Processor)
[2019-04-19 13:44:26,029] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53440-575 (kafka.network.Processor)
[2019-04-19 13:44:26,031] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53418-571 (kafka.network.Processor)
[2019-04-19 13:44:27,905] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 401 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:27,909] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53444-575 (kafka.network.Processor)
[2019-04-19 13:44:55,982] INFO [GroupCoordinator 1]: Member rdkafka-8eba7f10-87b8-408f-b09d-795d3c830fd8 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:55,985] INFO [GroupCoordinator 1]: Member rdkafka-2b5bd2c2-cc0c-4493-a631-989bb9912dd3 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:55,987] INFO [GroupCoordinator 1]: Member rdkafka-09ac07ba-1b9a-4d2b-b4d6-252e63597079 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:55,987] INFO [GroupCoordinator 1]: Member rdkafka-e6ac5673-40d2-4965-b984-ee97cdec9995 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:55,989] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 402 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:56,004] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53448-576 (kafka.network.Processor)
[2019-04-19 13:44:56,005] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53474-580 (kafka.network.Processor)
[2019-04-19 13:44:56,021] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53470-579 (kafka.network.Processor)
[2019-04-19 13:44:56,022] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53466-579 (kafka.network.Processor)
[2019-04-19 13:44:56,304] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 402 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:44:56,317] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53492-583 (kafka.network.Processor)
[2019-04-19 13:45:25,960] INFO [GroupCoordinator 1]: Member rdkafka-d6f16cd5-f6f1-4538-9a6b-2ba087f31b7f in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:25,960] INFO [GroupCoordinator 1]: Member rdkafka-a866f3e1-894c-4a64-8a6d-9e0b8d274f97 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:25,960] INFO [GroupCoordinator 1]: Member rdkafka-1757597b-3a59-4fc4-9f05-3261e91c0833 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:25,960] INFO [GroupCoordinator 1]: Member rdkafka-b6766b7c-2cf5-442b-b3b9-4da49ec5d94d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:25,963] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 403 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:25,968] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53522-587 (kafka.network.Processor)
[2019-04-19 13:45:25,970] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53526-588 (kafka.network.Processor)
[2019-04-19 13:45:25,970] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53500-584 (kafka.network.Processor)
[2019-04-19 13:45:25,975] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53496-583 (kafka.network.Processor)
[2019-04-19 13:45:25,976] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53518-587 (kafka.network.Processor)
[2019-04-19 13:45:27,234] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 403 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:27,237] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53544-591 (kafka.network.Processor)
[2019-04-19 13:45:55,933] INFO [GroupCoordinator 1]: Member rdkafka-789fed47-7943-4d1f-82e7-20c101d27868 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:55,933] INFO [GroupCoordinator 1]: Member rdkafka-cff0ca20-882a-47f3-8d89-2ff6b8f0ba2a in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:55,933] INFO [GroupCoordinator 1]: Member rdkafka-49b96cde-67c0-4307-8c8f-a862a0af6f7d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:55,935] INFO [GroupCoordinator 1]: Member rdkafka-5dd0e4cb-c4a2-4e9d-90a3-acb875c36279 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:55,935] INFO [GroupCoordinator 1]: Member rdkafka-23ec5d15-e623-493b-a0f0-7544de6acb13 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:55,938] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 404 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:55,960] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53552-592 (kafka.network.Processor)
[2019-04-19 13:45:55,969] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53578-596 (kafka.network.Processor)
[2019-04-19 13:45:55,973] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53570-595 (kafka.network.Processor)
[2019-04-19 13:45:55,994] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53548-591 (kafka.network.Processor)
[2019-04-19 13:45:57,384] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 404 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:45:57,388] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53596-599 (kafka.network.Processor)
[2019-04-19 13:45:59,413] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 13:46:05,028] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:46:05,030] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:46:05,031] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:46:05,031] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:46:25,909] INFO [GroupCoordinator 1]: Member rdkafka-9b0e8ab8-2376-4673-9bc8-bc2650a39021 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:25,910] INFO [GroupCoordinator 1]: Member rdkafka-87f1880c-a3b5-41bb-90e9-4c8f3a5ac91d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:25,910] INFO [GroupCoordinator 1]: Member rdkafka-c91a0bc5-99e9-48ba-94f6-2482a5a7b3db in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:25,910] INFO [GroupCoordinator 1]: Member rdkafka-a573f0c6-c2ea-4f9d-99d7-a7892ccde28b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:25,912] INFO [GroupCoordinator 1]: Member rdkafka-5a44c488-d227-44d9-8357-1db4b64a9f00 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:25,914] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 405 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:25,932] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53604-600 (kafka.network.Processor)
[2019-04-19 13:46:25,940] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53630-604 (kafka.network.Processor)
[2019-04-19 13:46:25,948] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53626-603 (kafka.network.Processor)
[2019-04-19 13:46:25,949] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53600-599 (kafka.network.Processor)
[2019-04-19 13:46:25,957] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53622-603 (kafka.network.Processor)
[2019-04-19 13:46:27,930] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 405 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:27,969] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53648-607 (kafka.network.Processor)
[2019-04-19 13:46:55,881] INFO [GroupCoordinator 1]: Member rdkafka-54fc0aa5-6fb4-43d0-aa79-7c544e1e383f in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:55,881] INFO [GroupCoordinator 1]: Member rdkafka-ed38471d-4110-4cfc-a656-6d00e6420dde in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:55,890] INFO [GroupCoordinator 1]: Member rdkafka-22184558-f577-4119-8b52-601d74daf2e1 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:55,892] INFO [GroupCoordinator 1]: Member rdkafka-68e67d41-330a-407f-a3b7-a3286b6a8e12 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:55,893] INFO [GroupCoordinator 1]: Member rdkafka-247c1cc7-bed3-4fbf-afac-d672b1eb1cb8 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:55,896] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 406 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:46:55,913] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53678-611 (kafka.network.Processor)
[2019-04-19 13:46:55,916] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53674-611 (kafka.network.Processor)
[2019-04-19 13:46:55,918] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53652-607 (kafka.network.Processor)
[2019-04-19 13:46:55,919] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53682-612 (kafka.network.Processor)
[2019-04-19 13:46:55,921] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53656-608 (kafka.network.Processor)
[2019-04-19 13:46:55,973] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 406 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,865] INFO [GroupCoordinator 1]: Member rdkafka-4bb336b2-9bdb-403c-b447-0d8094ad5bbb in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,874] INFO [GroupCoordinator 1]: Member rdkafka-8edbb1c8-db5b-4267-9bd7-51a927626d29 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,878] INFO [GroupCoordinator 1]: Member rdkafka-7f2645db-689e-4f78-9f32-9e6f4c4a06da in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,880] INFO [GroupCoordinator 1]: Member rdkafka-03104acf-2881-4867-86a1-0d832ce6c0a9 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,880] INFO [GroupCoordinator 1]: Member rdkafka-1cd12aad-90fe-4948-8095-3e5d2c49defc in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,883] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 407 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:25,932] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53704-615 (kafka.network.Processor)
[2019-04-19 13:47:25,937] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53730-619 (kafka.network.Processor)
[2019-04-19 13:47:25,947] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53708-616 (kafka.network.Processor)
[2019-04-19 13:47:25,951] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53700-615 (kafka.network.Processor)
[2019-04-19 13:47:25,958] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53726-619 (kafka.network.Processor)
[2019-04-19 13:47:25,986] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53734-620 (kafka.network.Processor)
[2019-04-19 13:47:26,364] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 407 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,857] INFO [GroupCoordinator 1]: Member rdkafka-516bd3ca-1590-4991-a1f2-9271502c9b32 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,886] INFO [GroupCoordinator 1]: Member rdkafka-8c063730-9145-48da-bd44-035f312080e3 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,888] INFO [GroupCoordinator 1]: Member rdkafka-bdd03844-d6f9-44c4-a717-dc19285b35ea in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,889] INFO [GroupCoordinator 1]: Member rdkafka-36bc8108-a89a-424b-9391-bafa15b6c06d in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,890] INFO [GroupCoordinator 1]: Member rdkafka-24e61d1d-9d36-4f7c-b03d-7d5e1560e9f5 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,890] INFO [GroupCoordinator 1]: Member rdkafka-b3840a33-dda6-4675-9080-a771dc9a865e in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,891] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 408 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:47:55,909] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53782-627 (kafka.network.Processor)
[2019-04-19 13:47:55,909] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53778-627 (kafka.network.Processor)
[2019-04-19 13:47:55,910] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53760-624 (kafka.network.Processor)
[2019-04-19 13:47:55,910] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53752-623 (kafka.network.Processor)
[2019-04-19 13:47:55,913] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53756-623 (kafka.network.Processor)
[2019-04-19 13:47:55,914] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53786-628 (kafka.network.Processor)
[2019-04-19 13:47:56,811] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 408 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,859] INFO [GroupCoordinator 1]: Member rdkafka-79be4777-faa6-4128-acd6-65b53ff3b594 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,859] INFO [GroupCoordinator 1]: Member rdkafka-c8bfc533-fbf4-4873-82b2-7e5bdef85fad in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,859] INFO [GroupCoordinator 1]: Member rdkafka-4ace0869-9179-4099-8769-5d32a1caff63 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,860] INFO [GroupCoordinator 1]: Member rdkafka-a11a6569-bf82-473b-9763-0f86cb56ac71 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,876] INFO [GroupCoordinator 1]: Member rdkafka-004162cb-e115-47f7-b355-e1b6ace46fa8 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,879] INFO [GroupCoordinator 1]: Member rdkafka-b928cd85-248e-4cd4-826e-6e82b2692a03 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,881] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test generation 409 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:25,891] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53812-632 (kafka.network.Processor)
[2019-04-19 13:48:25,892] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53808-631 (kafka.network.Processor)
[2019-04-19 13:48:25,894] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53834-635 (kafka.network.Processor)
[2019-04-19 13:48:25,895] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53830-635 (kafka.network.Processor)
[2019-04-19 13:48:25,896] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53804-631 (kafka.network.Processor)
[2019-04-19 13:48:25,896] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53838-636 (kafka.network.Processor)
[2019-04-19 13:48:28,877] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 0 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:28,884] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 1 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:28,894] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:28,895] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-11. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:48:32,810] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 1 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:32,812] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 2 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:32,814] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53856-639 (kafka.network.Processor)
[2019-04-19 13:48:32,936] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 2 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:32,947] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 3 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:32,969] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:37,014] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 3 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:37,016] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 4 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:37,017] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53860-639 (kafka.network.Processor)
[2019-04-19 13:48:37,114] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 4 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:37,126] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 5 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:37,166] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:41,117] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 5 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:41,117] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 6 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:41,119] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53864-640 (kafka.network.Processor)
[2019-04-19 13:48:47,671] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 6 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:47,679] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 7 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:47,693] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 7 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:50,809] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 7 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:50,809] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 8 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:50,811] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53882-643 (kafka.network.Processor)
[2019-04-19 13:48:50,852] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 8 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:50,859] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 9 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:50,869] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 9 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:53,991] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 9 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:53,991] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 10 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:53,992] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53886-643 (kafka.network.Processor)
[2019-04-19 13:48:54,068] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 10 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:54,073] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 11 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:54,089] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 11 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,851] INFO [GroupCoordinator 1]: Member rdkafka-3d1be9d8-01bd-4ed7-af94-7e10fbfbaa0b in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test with old generation 409 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Member rdkafka-61e4fdcc-fad5-42a8-aa9f-4379d025ff22 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Member rdkafka-310d49bd-b304-4571-8937-3175eda6088c in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Member rdkafka-580066e5-6fd8-4b2c-b979-e6f5cea7ed09 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Member rdkafka-6df4d49a-ca99-4ae3-9ede-19a83ede5692 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Member rdkafka-bb8ac739-93c9-49aa-b0cd-85723605c936 in group pyrandall-test has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:55,852] INFO [GroupCoordinator 1]: Group pyrandall-test with generation 410 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:57,194] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 11 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:57,194] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 12 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:48:57,196] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53890-644 (kafka.network.Processor)
[2019-04-19 13:49:03,320] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 12 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:03,321] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 13 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:03,325] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 13 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:06,406] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 13 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:06,406] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 14 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:06,407] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53908-647 (kafka.network.Processor)
[2019-04-19 13:49:06,424] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 14 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:06,425] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 15 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:06,430] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 15 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:09,505] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 15 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:09,505] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 16 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:09,507] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53912-647 (kafka.network.Processor)
[2019-04-19 13:49:09,525] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 16 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:09,527] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 17 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:09,541] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 17 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:12,619] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 17 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:12,620] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 18 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:12,623] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53916-648 (kafka.network.Processor)
[2019-04-19 13:49:18,539] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 18 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:18,540] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 19 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:18,545] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 19 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:21,585] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 19 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:21,586] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 20 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:21,589] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53934-651 (kafka.network.Processor)
[2019-04-19 13:49:21,611] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 20 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:21,613] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 21 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:21,618] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 21 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:24,715] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 21 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:24,716] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 22 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:24,718] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53938-651 (kafka.network.Processor)
[2019-04-19 13:49:24,734] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 22 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:24,735] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 23 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:24,740] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 23 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:27,792] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 23 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:27,793] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 24 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:27,793] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53942-652 (kafka.network.Processor)
[2019-04-19 13:49:33,757] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 24 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:33,759] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 25 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:33,764] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 25 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:36,812] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 25 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:36,812] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 26 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:36,835] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 26 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:36,836] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 27 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:36,842] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 27 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:39,941] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 27 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:39,942] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 28 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:39,943] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53964-655 (kafka.network.Processor)
[2019-04-19 13:49:39,970] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 28 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:39,972] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 29 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:39,976] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 29 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:43,041] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 29 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:43,042] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 30 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:43,043] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53968-656 (kafka.network.Processor)
[2019-04-19 13:49:49,007] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 30 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:49,009] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 31 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:49,014] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 31 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:52,069] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 31 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:52,076] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 32 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:52,084] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53986-659 (kafka.network.Processor)
[2019-04-19 13:49:52,093] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 32 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:52,095] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 33 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:52,100] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 33 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:55,201] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 33 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:55,202] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 34 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:55,204] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53990-659 (kafka.network.Processor)
[2019-04-19 13:49:55,224] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 34 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:55,228] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 35 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:55,251] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 35 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:58,384] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 35 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:58,384] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 36 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:49:58,385] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:53994-660 (kafka.network.Processor)
[2019-04-19 13:49:59,730] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: emails_three-11. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:51:04,694] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:51:04,696] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:51:04,696] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:51:04,696] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:51:19,043] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 36 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:19,062] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 37 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:19,090] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 37 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:22,223] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 37 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:22,224] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 38 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:22,376] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 38 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:22,383] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 39 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:22,410] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 39 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:25,977] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 39 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:25,977] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 40 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:32,902] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 40 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:32,914] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 41 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:32,946] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 41 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:51:36,540] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 41 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:06,260] INFO [GroupCoordinator 1]: Member rdkafka-2b1a5bd8-e25b-44f7-9317-40645c13e837 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:06,263] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 42 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:06,265] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54072-672 (kafka.network.Processor)
[2019-04-19 13:52:06,271] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54068-671 (kafka.network.Processor)
[2019-04-19 13:52:36,232] INFO [GroupCoordinator 1]: Member rdkafka-a746a978-e5ca-4c67-b654-64153f04994d in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:36,232] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 42 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:36,233] INFO [GroupCoordinator 1]: Member rdkafka-1b2a2c9f-6853-4b93-8fd5-0866d0498b39 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:36,233] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 43 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:54,590] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 43 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:54,604] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 44 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:54,630] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 44 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:57,849] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 44 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:57,849] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 45 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:57,958] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 45 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:57,960] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 46 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:52:57,989] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 46 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:53:01,164] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 46 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:53:01,165] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 47 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:37,999] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 47 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:38,009] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 48 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:38,039] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 48 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:41,238] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 48 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:41,240] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 49 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:41,245] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54112-678 (kafka.network.Processor)
[2019-04-19 13:54:41,333] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 49 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:41,348] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 50 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:41,383] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-666 for generation 50 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:54:44,624] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 50 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:14,439] INFO [GroupCoordinator 1]: Member rdkafka-0890bde6-7b2a-4cba-bf86-d6e132cf0f40 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:14,442] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 51 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:14,447] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54142-682 (kafka.network.Processor)
[2019-04-19 13:55:14,454] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54164-686 (kafka.network.Processor)
[2019-04-19 13:55:14,455] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54146-683 (kafka.network.Processor)
[2019-04-19 13:55:14,456] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54138-682 (kafka.network.Processor)
[2019-04-19 13:55:14,456] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54120-679 (kafka.network.Processor)
[2019-04-19 13:55:16,025] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 51 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:16,032] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54168-686 (kafka.network.Processor)
[2019-04-19 13:55:44,416] INFO [GroupCoordinator 1]: Member rdkafka-e030f76b-15f0-4116-a965-b33949d1ff88 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,418] INFO [GroupCoordinator 1]: Member rdkafka-33dc2210-1093-4ceb-8184-415cf71f0262 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,418] INFO [GroupCoordinator 1]: Member rdkafka-4be89540-5fa1-4d5b-962b-4a71f9e33406 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,419] INFO [GroupCoordinator 1]: Member rdkafka-fe792479-ca18-4311-8e9a-70de50968887 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,420] INFO [GroupCoordinator 1]: Member rdkafka-c2d7b660-9605-40be-b7f0-9f9b8f86b23f in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,422] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 52 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,429] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54194-690 (kafka.network.Processor)
[2019-04-19 13:55:44,435] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54172-687 (kafka.network.Processor)
[2019-04-19 13:55:44,436] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54198-691 (kafka.network.Processor)
[2019-04-19 13:55:44,438] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54190-690 (kafka.network.Processor)
[2019-04-19 13:55:44,705] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 52 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:44,707] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54216-694 (kafka.network.Processor)
[2019-04-19 13:55:57,370] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 0 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:57,388] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 1 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:57,431] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:55:57,441] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-20. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-04-19 13:55:58,745] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-04-19 13:56:01,650] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 1 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:01,687] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 2 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:01,716] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54242-698 (kafka.network.Processor)
[2019-04-19 13:56:01,889] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 2 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:01,912] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 3 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:01,942] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:04,366] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2019-04-19 13:56:04,370] DEBUG [Controller id=1] Preferred replicas by broker Map(1 -> Map(__consumer_offsets-22 -> Vector(1), emails_two-4 -> Vector(1), _confluent-metrics-7 -> Vector(1), _confluent-metrics-2 -> Vector(1), __consumer_offsets-30 -> Vector(1), emails_three-8 -> Vector(1), emails-0 -> Vector(1), __consumer_offsets-8 -> Vector(1), emails_three-11 -> Vector(1), emails-3 -> Vector(1), __consumer_offsets-21 -> Vector(1), __consumer_offsets-4 -> Vector(1), emails_two-10 -> Vector(1), _confluent-metrics-5 -> Vector(1), __consumer_offsets-27 -> Vector(1), __consumer_offsets-7 -> Vector(1), __consumer_offsets-9 -> Vector(1), _confluent-metrics-11 -> Vector(1), emails_two-0 -> Vector(1), __consumer_offsets-46 -> Vector(1), emails_two-5 -> Vector(1), emails_two-13 -> Vector(1), __consumer_offsets-25 -> Vector(1), __consumer_offsets-35 -> Vector(1), emails-10 -> Vector(1), __consumer_offsets-41 -> Vector(1), __consumer_offsets-33 -> Vector(1), __consumer_offsets-23 -> Vector(1), __consumer_offsets-49 -> Vector(1), emails-1 -> Vector(1), emails_two-3 -> Vector(1), _confluent-metrics-10 -> Vector(1), __consumer_offsets-47 -> Vector(1), __consumer_offsets-16 -> Vector(1), __consumer_offsets-28 -> Vector(1), emails_three-0 -> Vector(1), emails-7 -> Vector(1), emails_two-9 -> Vector(1), __consumer_offsets-31 -> Vector(1), __consumer_offsets-36 -> Vector(1), emails_three-6 -> Vector(1), __consumer_offsets-42 -> Vector(1), __consumer_offsets-3 -> Vector(1), __consumer_offsets-18 -> Vector(1), __consumer_offsets-37 -> Vector(1), emails-11 -> Vector(1), __consumer_offsets-15 -> Vector(1), __consumer_offsets-24 -> Vector(1), emails-4 -> Vector(1), emails_two-14 -> Vector(1), _confluent-metrics-1 -> Vector(1), emails_two-6 -> Vector(1), emails-14 -> Vector(1), emails-6 -> Vector(1), emails_three-4 -> Vector(1), emails_two-12 -> Vector(1), __consumer_offsets-38 -> Vector(1), __consumer_offsets-17 -> Vector(1), emails_three-5 -> Vector(1), emails_two-2 -> Vector(1), __consumer_offsets-48 -> Vector(1), emails_three-1 -> Vector(1), __confluent.support.metrics-0 -> Vector(1), emails_three-14 -> Vector(1), emails_three-10 -> Vector(1), __consumer_offsets-19 -> Vector(1), emails_two-7 -> Vector(1), _confluent-metrics-9 -> Vector(1), _confluent-metrics-0 -> Vector(1), __consumer_offsets-11 -> Vector(1), emails_three-3 -> Vector(1), emails_two-8 -> Vector(1), emails_three-9 -> Vector(1), emails-8 -> Vector(1), __consumer_offsets-13 -> Vector(1), __consumer_offsets-2 -> Vector(1), __consumer_offsets-43 -> Vector(1), __consumer_offsets-6 -> Vector(1), __consumer_offsets-14 -> Vector(1), _confluent-metrics-3 -> Vector(1), emails-13 -> Vector(1), emails-2 -> Vector(1), emails_two-11 -> Vector(1), _confluent-metrics-4 -> Vector(1), emails-5 -> Vector(1), emails_three-12 -> Vector(1), __consumer_offsets-20 -> Vector(1), __consumer_offsets-0 -> Vector(1), __consumer_offsets-44 -> Vector(1), emails_three-7 -> Vector(1), emails-9 -> Vector(1), __consumer_offsets-39 -> Vector(1), emails_three-2 -> Vector(1), emails_three-13 -> Vector(1), __consumer_offsets-12 -> Vector(1), emails_two-1 -> Vector(1), _confluent-metrics-6 -> Vector(1), __consumer_offsets-45 -> Vector(1), __consumer_offsets-1 -> Vector(1), __consumer_offsets-5 -> Vector(1), __consumer_offsets-26 -> Vector(1), emails-12 -> Vector(1), _confluent-metrics-8 -> Vector(1), __consumer_offsets-29 -> Vector(1), __consumer_offsets-34 -> Vector(1), __consumer_offsets-10 -> Vector(1), __consumer_offsets-32 -> Vector(1), __consumer_offsets-40 -> Vector(1))) (kafka.controller.KafkaController)
[2019-04-19 13:56:04,370] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2019-04-19 13:56:04,370] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2019-04-19 13:56:06,082] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 3 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:06,095] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 4 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:06,360] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 4 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:06,372] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 5 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:06,434] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:10,522] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 5 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:10,522] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 6 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:14,385] INFO [GroupCoordinator 1]: Member rdkafka-b3686e50-ea65-4083-8c38-481cf53c6b53 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:14,386] INFO [GroupCoordinator 1]: Member rdkafka-120c4637-a30d-40c6-9b15-6e2c3d395478 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:14,387] INFO [GroupCoordinator 1]: Member rdkafka-271e1248-84ba-4f2d-9823-293d753de2a6 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:14,388] INFO [GroupCoordinator 1]: Member rdkafka-95c45b65-e9f5-4e94-8f07-7d9fcaa3b5d9 in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:14,391] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-666 generation 53 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:14,396] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54220-694 (kafka.network.Processor)
[2019-04-19 13:56:14,397] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54224-695 (kafka.network.Processor)
[2019-04-19 13:56:17,578] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 6 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:17,603] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 7 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:17,619] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 7 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:20,779] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 7 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:20,779] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 8 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:20,911] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 8 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:20,914] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 9 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:20,956] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 9 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:24,120] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 9 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:24,120] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 10 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:24,245] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 10 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:24,249] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 11 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:24,286] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 11 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:27,562] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 11 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:27,563] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 12 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:34,450] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 12 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:34,472] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 13 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:34,489] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 13 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:37,681] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 13 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:37,683] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 14 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:37,819] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 14 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:37,830] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 15 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:37,871] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 15 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:41,090] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 15 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:41,090] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 16 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:41,167] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 16 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:41,200] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 17 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:41,210] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 17 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,365] INFO [GroupCoordinator 1]: Member rdkafka-312ea725-9670-47bb-86e8-78d15fabfaed in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,371] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-666 with old generation 53 (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,415] INFO [GroupCoordinator 1]: Member rdkafka-172c6b91-45d1-4d76-abd4-982d24bc8f0e in group pyrandall-test-666 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,428] INFO [GroupCoordinator 1]: Group pyrandall-test-666 with generation 54 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,492] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 17 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,492] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 18 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:44,496] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54302-707 (kafka.network.Processor)
[2019-04-19 13:56:51,584] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 18 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:51,591] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 19 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:51,615] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 19 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:54,846] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 19 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:54,849] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 20 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:54,954] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 20 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:54,967] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 21 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:54,997] INFO [GroupCoordinator 1]: Assignment received from leader for group pyrandall-test-12345 for generation 21 (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:56:58,266] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 21 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:28,057] INFO [GroupCoordinator 1]: Member rdkafka-101f540a-4089-4fea-a415-c409bd0ab80c in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:28,060] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 22 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:28,065] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54328-711 (kafka.network.Processor)
[2019-04-19 13:57:28,066] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54354-715 (kafka.network.Processor)
[2019-04-19 13:57:28,072] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54350-714 (kafka.network.Processor)
[2019-04-19 13:57:28,073] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54346-714 (kafka.network.Processor)
[2019-04-19 13:57:30,185] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 22 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:30,189] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54372-718 (kafka.network.Processor)
[2019-04-19 13:57:58,030] INFO [GroupCoordinator 1]: Member rdkafka-369297d6-e994-49ca-a8d1-f976659498b2 in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:58,032] INFO [GroupCoordinator 1]: Member rdkafka-67c20a3d-b0ac-400f-9c9a-93c476006469 in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:58,032] INFO [GroupCoordinator 1]: Member rdkafka-0f38e673-01e6-4030-bd9b-ecffc8d5590c in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:58,032] INFO [GroupCoordinator 1]: Member rdkafka-01a14878-8a71-4333-849a-fc976843e75f in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:58,034] INFO [GroupCoordinator 1]: Stabilized group pyrandall-test-12345 generation 23 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:57:58,037] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54376-718 (kafka.network.Processor)
[2019-04-19 13:57:58,038] WARN Attempting to send response via channel for which there is no open connection, connection id 172.24.0.4:9092-172.24.0.1:54380-719 (kafka.network.Processor)
[2019-04-19 13:58:28,000] INFO [GroupCoordinator 1]: Member rdkafka-e9bbe297-47fb-41cf-972e-02a053520258 in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:58:28,002] INFO [GroupCoordinator 1]: Preparing to rebalance group pyrandall-test-12345 with old generation 23 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:58:28,002] INFO [GroupCoordinator 1]: Member rdkafka-3e008349-40b9-4a89-a4b8-01ac418b2cad in group pyrandall-test-12345 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-04-19 13:58:28,004] INFO [GroupCoordinator 1]: Group pyrandall-test-12345 with generation 24 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
